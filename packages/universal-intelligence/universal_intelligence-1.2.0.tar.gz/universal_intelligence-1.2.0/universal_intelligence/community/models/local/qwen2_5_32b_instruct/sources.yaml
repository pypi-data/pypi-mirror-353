model_info:
  name: Qwen2.5-32B-Instruct
  default_quantization:
    cuda: BNB_4
    mps: MLX_4
    cpu: Q4_K_M

quantizations:
  bfloat16:
    engine: transformers
    supported_devices: [cuda, cpu]
    model_id: Qwen/Qwen2.5-32B-Instruct
    model_file: null
    model_size: 70.0

  BNB_4:
    engine: transformers
    supported_devices: [cuda]
    model_id: unsloth/Qwen2.5-32B-Instruct-bnb-4bit
    model_file: null
    model_size: 20.0

  AWQ_4:
    engine: transformers
    supported_devices: [cuda]
    model_id: Qwen/Qwen2.5-32B-Instruct-AWQ
    model_file: null
    model_size: 21.0

  GPTQ_4:
    engine: transformers
    supported_devices: [cuda]
    model_id: Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4
    model_file: null
    model_size: 21.0

  GPTQ_8:
    engine: transformers
    supported_devices: [cuda]
    model_id: Qwen/Qwen2.5-32B-Instruct-GPTQ-Int8
    model_file: null
    model_size: 36.0

  MLX_8:
    engine: mlx-lm
    supported_devices: [mps]
    model_id: lmstudio-community/Qwen2.5-32B-Instruct-MLX-8bit
    model_file: null
    model_size: 36.0

  MLX_4:
    engine: mlx-lm
    supported_devices: [mps]
    model_id: lmstudio-community/Qwen2.5-32B-Instruct-MLX-4bit
    model_file: null
    model_size: 19.0

  F16:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Qwen2.5-32B-Instruct-GGUF
    model_file: Qwen2.5-32B-Instruct-f16-00001-of-00002.gguf
    model_size: 70.0

  Q8_0:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Qwen2.5-32B-Instruct-GGUF
    model_file: Qwen2.5-32B-Instruct-Q8_0.gguf
    model_size: 34.8

  Q6_K_L:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Qwen2.5-32B-Instruct-GGUF
    model_file: Qwen2.5-32B-Instruct-Q6_K_L.gguf
    model_size: 27.3

  Q6_K:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Qwen2.5-32B-Instruct-GGUF
    model_file: Qwen2.5-32B-Instruct-Q6_K.gguf
    model_size: 26.9

  Q5_K_L:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Qwen2.5-32B-Instruct-GGUF
    model_file: Qwen2.5-32B-Instruct-Q5_K_L.gguf
    model_size: 23.7

  Q5_K_M:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Qwen2.5-32B-Instruct-GGUF
    model_file: Qwen2.5-32B-Instruct-Q5_K_M.gguf
    model_size: 23.3

  Q5_K_S:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Qwen2.5-32B-Instruct-GGUF
    model_file: Qwen2.5-32B-Instruct-Q5_K_S.gguf
    model_size: 22.6

  Q4_K_L:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Qwen2.5-32B-Instruct-GGUF
    model_file: Qwen2.5-32B-Instruct-Q4_K_L.gguf
    model_size: 20.4

  Q4_K_M:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Qwen2.5-32B-Instruct-GGUF
    model_file: Qwen2.5-32B-Instruct-Q4_K_M.gguf
    model_size: 19.9

  Q4_K_S:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Qwen2.5-32B-Instruct-GGUF
    model_file: Qwen2.5-32B-Instruct-Q4_K_S.gguf
    model_size: 18.8

  Q4_0:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Qwen2.5-32B-Instruct-GGUF
    model_file: Qwen2.5-32B-Instruct-Q4_0.gguf
    model_size: 18.7

  Q3_K_XL:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Qwen2.5-32B-Instruct-GGUF
    model_file: Qwen2.5-32B-Instruct-Q3_K_XL.gguf
    model_size: 17.9

  Q3_K_L:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Qwen2.5-32B-Instruct-GGUF
    model_file: Qwen2.5-32B-Instruct-Q3_K_L.gguf
    model_size: 17.2

  Q3_K_M:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Qwen2.5-32B-Instruct-GGUF
    model_file: Qwen2.5-32B-Instruct-Q3_K_M.gguf
    model_size: 15.9

  Q3_K_S:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Qwen2.5-32B-Instruct-GGUF
    model_file: Qwen2.5-32B-Instruct-Q3_K_S.gguf
    model_size: 14.4

  Q2_K_L:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Qwen2.5-32B-Instruct-GGUF
    model_file: Qwen2.5-32B-Instruct-Q2_K_L.gguf
    model_size: 13.1

  Q2_K:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Qwen2.5-32B-Instruct-GGUF
    model_file: Qwen2.5-32B-Instruct-Q2_K.gguf
    model_size: 12.3

  IQ4_XS:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Qwen2.5-32B-Instruct-GGUF
    model_file: Qwen2.5-32B-Instruct-IQ4_XS.gguf
    model_size: 17.7

  IQ3_XS:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Qwen2.5-32B-Instruct-GGUF
    model_file: Qwen2.5-32B-Instruct-IQ3_XS.gguf
    model_size: 13.7

  IQ3_M:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Qwen2.5-32B-Instruct-GGUF
    model_file: Qwen2.5-32B-Instruct-IQ3_M.gguf
    model_size: 14.8

  IQ2_M:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Qwen2.5-32B-Instruct-GGUF
    model_file: Qwen2.5-32B-Instruct-IQ2_M.gguf
    model_size: 11.3

  IQ2_S:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Qwen2.5-32B-Instruct-GGUF
    model_file: Qwen2.5-32B-Instruct-IQ2_S.gguf
    model_size: 10.4

  IQ2_XS:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Qwen2.5-32B-Instruct-GGUF
    model_file: Qwen2.5-32B-Instruct-IQ2_XS.gguf
    model_size: 9.96

  IQ2_XXS:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Qwen2.5-32B-Instruct-GGUF
    model_file: Qwen2.5-32B-Instruct-IQ2_XXS.gguf
    model_size: 9.03
