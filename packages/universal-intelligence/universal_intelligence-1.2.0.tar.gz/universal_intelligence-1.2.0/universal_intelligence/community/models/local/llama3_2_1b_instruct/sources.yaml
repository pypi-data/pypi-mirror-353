model_info:
  name: Llama-3.2-1B-Instruct
  default_quantization:
    cuda: bfloat16
    mps: F16
    cpu: Q4_K_M

quantizations:
  bfloat16:
    engine: transformers
    supported_devices: [cuda, cpu]
    model_id: meta-llama/Llama-3.2-1B-Instruct
    model_file: null
    model_size: 3.5

  BNB_4:
    engine: transformers
    supported_devices: [cuda]
    model_id: unsloth/Llama-3.2-1B-Instruct-unsloth-bnb-4bit
    model_file: null
    model_size: 2.0

  GPTQ_4:
    engine: transformers
    supported_devices: [cuda]
    model_id: Almheiri/Llama-3.2-1B-Instruct-GPTQ-INT4
    model_file: null
    model_size: 2.5

  AWQ_4:
    engine: transformers
    supported_devices: [cuda]
    model_id: ciCic/llama-3.2-1B-Instruct-AWQ
    model_file: null
    model_size: 2.5

  MLX_4:
    engine: mlx-lm
    supported_devices: [mps]
    model_id: schaye/llama-3.2-1b-instruct-mlx-quantized
    model_file: null
    model_size: 1.5

  F16:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Llama-3.2-1B-Instruct-GGUF
    model_file: Llama-3.2-1B-Instruct-f16.gguf
    model_size: 2.48

  Q8_0:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Llama-3.2-1B-Instruct-GGUF
    model_file: Llama-3.2-1B-Instruct-Q8_0.gguf
    model_size: 1.32

  Q6_K_L:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Llama-3.2-1B-Instruct-GGUF
    model_file: Llama-3.2-1B-Instruct-Q6_K_L.gguf
    model_size: 1.09

  Q6_K:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Llama-3.2-1B-Instruct-GGUF
    model_file: Llama-3.2-1B-Instruct-Q6_K.gguf
    model_size: 1.02

  Q5_K_L:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Llama-3.2-1B-Instruct-GGUF
    model_file: Llama-3.2-1B-Instruct-Q5_K_L.gguf
    model_size: 0.98

  Q5_K_M:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Llama-3.2-1B-Instruct-GGUF
    model_file: Llama-3.2-1B-Instruct-Q5_K_M.gguf
    model_size: 0.91

  Q5_K_S:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Llama-3.2-1B-Instruct-GGUF
    model_file: Llama-3.2-1B-Instruct-Q5_K_S.gguf
    model_size: 0.89

  Q4_K_L:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Llama-3.2-1B-Instruct-GGUF
    model_file: Llama-3.2-1B-Instruct-Q4_K_L.gguf
    model_size: 0.87

  Q4_K_M:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Llama-3.2-1B-Instruct-GGUF
    model_file: Llama-3.2-1B-Instruct-Q4_K_M.gguf
    model_size: 0.81

  Q4_K_S:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Llama-3.2-1B-Instruct-GGUF
    model_file: Llama-3.2-1B-Instruct-Q4_K_S.gguf
    model_size: 0.78

  Q4_0:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Llama-3.2-1B-Instruct-GGUF
    model_file: Llama-3.2-1B-Instruct-Q4_0.gguf
    model_size: 0.77

  Q3_K_XL:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Llama-3.2-1B-Instruct-GGUF
    model_file: Llama-3.2-1B-Instruct-Q3_K_XL.gguf
    model_size: 0.80

  Q3_K_L:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Llama-3.2-1B-Instruct-GGUF
    model_file: Llama-3.2-1B-Instruct-Q3_K_L.gguf
    model_size: 0.73

  IQ4_XS:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Llama-3.2-1B-Instruct-GGUF
    model_file: Llama-3.2-1B-Instruct-IQ4_XS.gguf
    model_size: 0.74

  IQ3_M:
    engine: llama.cpp
    supported_devices: [cuda, mps, cpu]
    model_id: bartowski/Llama-3.2-1B-Instruct-GGUF
    model_file: Llama-3.2-1B-Instruct-IQ3_M.gguf
    model_size: 0.66
