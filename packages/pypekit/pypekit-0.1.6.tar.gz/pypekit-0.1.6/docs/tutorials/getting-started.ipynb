{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67596aca",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "`pypekit` is a Python library for designing and running data-processing pipelines.  \n",
    "Describe each task once and `pypekit` will automatically construct every valid pipeline for you and execute them with caching.\n",
    "\n",
    "\n",
    "## Installation\n",
    "\n",
    "```\n",
    "pip install pypekit\n",
    "```\n",
    "\n",
    "\n",
    "## Defining Tasks\n",
    "\n",
    "Create a task by subclassing `Task`, overriding the `input_types` and `output_types` properties, and implementing the `run()` method.\n",
    "\n",
    "| Property           | Purpose                                                              |\n",
    "| ------------------ | -------------------------------------------------------------------- |\n",
    "| **`input_types`**  | A list of strings representing input types that the task can accept. |\n",
    "| **`output_types`** | A list of strings representing output types that the task produces.  |\n",
    "\n",
    "`run()`: This method is where the task's logic is implemented. It processes the input and produces the output.\n",
    "\n",
    "### Pipeline Construction Rules\n",
    "\n",
    "* Pipelines start with tasks that have `\"source\"` in **`input_types`** and end with tasks that have `\"sink\"` in **`output_types`**.\n",
    "* All other types are intermediate and dictate how tasks can be chained.\n",
    "* Two tasks connect when **at least one** output type of the upstream task matches an input type of the downstream task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13a0f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypekit import Task\n",
    "\n",
    "class Source(Task):\n",
    "    input_types = [\"source\"]\n",
    "    output_types = [\"a\"]\n",
    "\n",
    "    def run(self, _):\n",
    "        print(\"Running Source\")\n",
    "        return \"source\"\n",
    "\n",
    "class Transform1(Task):\n",
    "    input_types = [\"a\"]\n",
    "    output_types = [\"b\"]\n",
    "\n",
    "    def run(self, x):\n",
    "        print(\"Running Transform1\")\n",
    "        return x + \"_transformed-1\"\n",
    "    \n",
    "class Transform2(Task):\n",
    "    input_types = [\"a\", \"b\"]\n",
    "    output_types = [\"b\"]\n",
    "\n",
    "    def run(self, x):\n",
    "        print(\"Running Transform2\")\n",
    "        return x + \"_transformed-2\"\n",
    "    \n",
    "class Sink1(Task):\n",
    "    input_types = [\"b\"]\n",
    "    output_types = [\"sink\"]\n",
    "\n",
    "    def run(self, x):\n",
    "        print(\"Running Sink1\")\n",
    "        return x + \"_sink-1\"\n",
    "    \n",
    "class Sink2(Task):\n",
    "    input_types = [\"b\"]\n",
    "    output_types = [\"sink\"]\n",
    "\n",
    "    def run(self, x):\n",
    "        print(\"Running Sink2\")\n",
    "        return x + \"_sink-2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb67b003",
   "metadata": {},
   "source": [
    "## Building a Repository\n",
    "\n",
    "Collect your task classes in a `Repository`, then call `build_tree()` to let `pypekit` construct a tree of tasks based on the defined `input_types` and `output_types`.\n",
    "The resulting tree represents every source-to-sink pathway implied by your task definitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f54657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypekit import Repository\n",
    "\n",
    "repository = Repository([\n",
    "    Source,\n",
    "    Transform1,\n",
    "    Transform2,\n",
    "    Sink1,\n",
    "    Sink2\n",
    "])\n",
    "\n",
    "root = repository.build_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b8f2f9",
   "metadata": {},
   "source": [
    "## Inspect Tree\n",
    "\n",
    "To inspect the tree structure, call `build_tree_string()`, which will return a string representation of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89bade5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "└── Root()\n",
      "    └── Source()\n",
      "        ├── Transform1()\n",
      "        │   ├── Transform2()\n",
      "        │   │   ├── Sink1()\n",
      "        │   │   └── Sink2()\n",
      "        │   ├── Sink1()\n",
      "        │   └── Sink2()\n",
      "        └── Transform2()\n",
      "            ├── Sink1()\n",
      "            └── Sink2()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree_representation = repository.build_tree_string()\n",
    "print(tree_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3194265d",
   "metadata": {},
   "source": [
    "## Build Pipelines\n",
    "\n",
    "To get all viable pipelines from the tree, call `build_pipelines()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "122b038d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(tasks=[Source(), Transform1(), Transform2(), Sink1()])\n",
      "Pipeline(tasks=[Source(), Transform1(), Transform2(), Sink2()])\n",
      "Pipeline(tasks=[Source(), Transform1(), Sink1()])\n",
      "Pipeline(tasks=[Source(), Transform1(), Sink2()])\n",
      "Pipeline(tasks=[Source(), Transform2(), Sink1()])\n",
      "Pipeline(tasks=[Source(), Transform2(), Sink2()])\n"
     ]
    }
   ],
   "source": [
    "pipelines = repository.build_pipelines()\n",
    "for p in pipelines:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b058404",
   "metadata": {},
   "source": [
    "## Execute Pipelines with Caching\n",
    "\n",
    "Running many similar pipelines can be wasteful if they share sub-chains. \n",
    "To cache intermediate results, pass a list of pipelines to the `CachedExecutor` and call the `run()` method.\n",
    "\n",
    "The executor only runs tasks that have not been executed with the same input before, and reuses cached results for the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "860359c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Source\n",
      "Running Transform1\n",
      "Running Transform2\n",
      "Running Sink1\n",
      "Pipeline 1/6 completed. Runtime: 0.00s.\n",
      "Running Sink2\n",
      "Pipeline 2/6 completed. Runtime: 0.00s.\n",
      "Running Sink1\n",
      "Pipeline 3/6 completed. Runtime: 0.00s.\n",
      "Running Sink2\n",
      "Pipeline 4/6 completed. Runtime: 0.00s.\n",
      "Running Transform2\n",
      "Running Sink1\n",
      "Pipeline 5/6 completed. Runtime: 0.00s.\n",
      "Running Sink2\n",
      "Pipeline 6/6 completed. Runtime: 0.00s.\n"
     ]
    }
   ],
   "source": [
    "from pypekit import CachedExecutor\n",
    "\n",
    "executor = CachedExecutor(pipelines, verbose=True)\n",
    "results = executor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617012a7",
   "metadata": {},
   "source": [
    "## Inspect Results\n",
    "\n",
    "After `run()` finishes, `executor.results` is a nested dict whose keys are pipeline IDs.\n",
    "Each entry records:\n",
    "\n",
    "* **`output`** – the output of the pipeline,\n",
    "* **`runtime`** – cumulative seconds spent (for cached tasks, the runtime is also taken from the cache),\n",
    "* **`tasks`** – the task list that formed the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb407c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"output\": \"source_transformed-1_transformed-2_sink-1\",\n",
      "  \"runtime\": 7.836699933250202e-05,\n",
      "  \"tasks\": [\n",
      "    \"Source()\",\n",
      "    \"Transform1()\",\n",
      "    \"Transform2()\",\n",
      "    \"Sink1()\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"output\": \"source_transformed-1_transformed-2_sink-2\",\n",
      "  \"runtime\": 8.094199893093901e-05,\n",
      "  \"tasks\": [\n",
      "    \"Source()\",\n",
      "    \"Transform1()\",\n",
      "    \"Transform2()\",\n",
      "    \"Sink2()\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"output\": \"source_transformed-1_sink-1\",\n",
      "  \"runtime\": 7.130399899324402e-05,\n",
      "  \"tasks\": [\n",
      "    \"Source()\",\n",
      "    \"Transform1()\",\n",
      "    \"Sink1()\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"output\": \"source_transformed-1_sink-2\",\n",
      "  \"runtime\": 7.185499907791382e-05,\n",
      "  \"tasks\": [\n",
      "    \"Source()\",\n",
      "    \"Transform1()\",\n",
      "    \"Sink2()\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"output\": \"source_transformed-2_sink-1\",\n",
      "  \"runtime\": 6.648499856964918e-05,\n",
      "  \"tasks\": [\n",
      "    \"Source()\",\n",
      "    \"Transform2()\",\n",
      "    \"Sink1()\"\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"output\": \"source_transformed-2_sink-2\",\n",
      "  \"runtime\": 6.663499880232848e-05,\n",
      "  \"tasks\": [\n",
      "    \"Source()\",\n",
      "    \"Transform2()\",\n",
      "    \"Sink2()\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for r in results.values():\n",
    "    print(json.dumps(r, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df8b226",
   "metadata": {},
   "source": [
    "# Reusing Cache\n",
    "\n",
    "If you already have a cache from a previous run, you can reuse it by passing the `cache` argument to the `CachedExecutor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c4ced3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 1/6 completed. Runtime: 0.00s.\n",
      "Pipeline 2/6 completed. Runtime: 0.00s.\n",
      "Pipeline 3/6 completed. Runtime: 0.00s.\n",
      "Pipeline 4/6 completed. Runtime: 0.00s.\n",
      "Pipeline 5/6 completed. Runtime: 0.00s.\n",
      "Pipeline 6/6 completed. Runtime: 0.00s.\n"
     ]
    }
   ],
   "source": [
    "new_executor = CachedExecutor(pipelines, cache=executor.cache, verbose=True)\n",
    "new_executor.run();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b276873a",
   "metadata": {},
   "source": [
    "# Instances, Parameters and Pipelines\n",
    "\n",
    "A repository can mix and match several flavours of “tasks”:\n",
    "\n",
    "| What you pass                                | How the repository treats it                          |\n",
    "| -------------------------------------------- | ----------------------------------------------------- |\n",
    "| A **class**                                  | Instantiated at every node of the tree.               |\n",
    "| An **instance** (`Task()`)                   | Re-use the same instance everywhere it’s needed.      |\n",
    "| A **tuple** (`Task`, `kwargs`)               | Task class with kwargs to use on instantiation.       |\n",
    "| An existing **Pipeline**                     | Used as an instance of a task.                        |\n",
    "\n",
    "This flexibility lets you\n",
    "\n",
    "* reuse heavyweight objects (e.g. a loaded ML model),\n",
    "* scan hyper-parameters by specifying multiple (class, kwargs) tuples,\n",
    "* embed pre-fabricated sub-pipelines inside larger graphs,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a32e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "└── Root()\n",
      "    └── Source()\n",
      "        ├── Transform1()\n",
      "        │   └── Sink1()\n",
      "        ├── Transform3(test=True)\n",
      "        │   └── Sink1()\n",
      "        └── Pipeline(tasks=[Transform1(), Sink2()])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pypekit import Pipeline\n",
    "\n",
    "# New task that takes arguments\n",
    "class Transform3(Task):\n",
    "    input_types = [\"a\"]\n",
    "    output_types = [\"b\"]\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.test = kwargs.get(\"test\", False)\n",
    "\n",
    "    def run(self, x):\n",
    "        print(\"Running Transform3 with test =\", self.test)\n",
    "        return x + \"_transformed-3\" + (\"-test\" if self.test else \"\")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    Transform1(),\n",
    "    Sink2()\n",
    "])\n",
    "\n",
    "repository = Repository([\n",
    "    Source,\n",
    "    Transform1,\n",
    "    (Transform3, {\"test\": True}),   # Transform3 will be instantiated with the argument test=True every time it is used\n",
    "    Sink1(),                        # Every node with the task Sink1 will have the same instance\n",
    "    pipeline                        # Pipeline instance as task\n",
    "])\n",
    "\n",
    "repository.build_tree()\n",
    "print(repository.build_tree_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pypekit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
