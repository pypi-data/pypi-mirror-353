"""å¸‚åœºæ•°æ®æœåŠ¡æ¨¡å—ã€‚

æä¾›åŠ å¯†è´§å¸å¸‚åœºæ•°æ®è·å–ã€å¤„ç†å’Œå­˜å‚¨åŠŸèƒ½ã€‚
"""

import logging
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from contextlib import nullcontext
from datetime import datetime, timedelta
from decimal import Decimal
from pathlib import Path
from threading import Lock
from typing import Any

import pandas as pd
from rich.logging import RichHandler
from rich.progress import (
    BarColumn,
    Progress,
    SpinnerColumn,
    TextColumn,
    TimeElapsedColumn,
)

from cryptoservice.client import BinanceClientFactory
from cryptoservice.config import settings
from cryptoservice.data import MarketDB
from cryptoservice.exceptions import (
    InvalidSymbolError,
    MarketDataFetchError,
    RateLimitError,
)
from cryptoservice.interfaces import IMarketDataService
from cryptoservice.models import (
    DailyMarketTicker,
    Freq,
    HistoricalKlinesType,
    KlineMarketTicker,
    PerpetualMarketTicker,
    SortBy,
    SymbolTicker,
    UniverseConfig,
    UniverseDefinition,
    UniverseSnapshot,
)
from cryptoservice.utils import DataConverter

# é…ç½® rich logger
logging.basicConfig(
    level=logging.INFO,
    format="%(message)s",
    handlers=[RichHandler(rich_tracebacks=True)],
)
logger = logging.getLogger(__name__)

cache_lock = Lock()


class MarketDataService(IMarketDataService):
    """å¸‚åœºæ•°æ®æœåŠ¡å®ç°ç±»ã€‚"""

    def __init__(self, api_key: str, api_secret: str) -> None:
        """åˆå§‹åŒ–å¸‚åœºæ•°æ®æœåŠ¡ã€‚

        Args:
            api_key: ç”¨æˆ·APIå¯†é’¥
            api_secret: ç”¨æˆ·APIå¯†é’¥
        """
        self.client = BinanceClientFactory.create_client(api_key, api_secret)
        self.converter = DataConverter()
        self.db: MarketDB | None = None

    def _validate_and_prepare_path(
        self, path: Path | str, is_file: bool = False, file_name: str | None = None
    ) -> Path:
        """éªŒè¯å¹¶å‡†å¤‡è·¯å¾„ã€‚

        Args:
            path: è·¯å¾„å­—ç¬¦ä¸²æˆ–Pathå¯¹è±¡
            is_file: æ˜¯å¦ä¸ºæ–‡ä»¶è·¯å¾„
            file_name: æ–‡ä»¶å
        Returns:
            Path: éªŒè¯åçš„Pathå¯¹è±¡

        Raises:
            ValueError: è·¯å¾„ä¸ºç©ºæˆ–æ— æ•ˆæ—¶
        """
        if not path:
            raise ValueError("è·¯å¾„ä¸èƒ½ä¸ºç©ºï¼Œå¿…é¡»æ‰‹åŠ¨æŒ‡å®š")

        path_obj = Path(path)

        # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„ï¼Œç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
        if is_file:
            if path_obj.is_dir():
                path_obj = path_obj.joinpath(file_name) if file_name else path_obj
            else:
                path_obj.parent.mkdir(parents=True, exist_ok=True)
        else:
            # å¦‚æœæ˜¯ç›®å½•è·¯å¾„ï¼Œç¡®ä¿ç›®å½•å­˜åœ¨
            path_obj.mkdir(parents=True, exist_ok=True)

        return path_obj

    def get_symbol_ticker(self, symbol: str | None = None) -> SymbolTicker | list[SymbolTicker]:
        """è·å–å•ä¸ªæˆ–æ‰€æœ‰äº¤æ˜“å¯¹çš„è¡Œæƒ…æ•°æ®ã€‚

        Args:
            symbol: äº¤æ˜“å¯¹åç§°

        Returns:
            SymbolTicker | list[SymbolTicker]: å•ä¸ªäº¤æ˜“å¯¹çš„è¡Œæƒ…æ•°æ®æˆ–æ‰€æœ‰äº¤æ˜“å¯¹çš„è¡Œæƒ…æ•°æ®
        """
        try:
            ticker = self.client.get_symbol_ticker(symbol=symbol)
            if not ticker:
                raise InvalidSymbolError(f"Invalid symbol: {symbol}")

            if isinstance(ticker, list):
                return [SymbolTicker.from_binance_ticker(t) for t in ticker]
            return SymbolTicker.from_binance_ticker(ticker)

        except Exception as e:
            logger.error(f"[red]Error fetching ticker for {symbol}: {e}[/red]")
            raise MarketDataFetchError(f"Failed to fetch ticker: {e}") from e

    def get_perpetual_symbols(
        self, only_trading: bool = True, quote_asset: str = "USDT"
    ) -> list[str]:
        """è·å–å½“å‰å¸‚åœºä¸Šæ‰€æœ‰æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹ã€‚

        Args:
            only_trading: æ˜¯å¦åªè¿”å›å½“å‰å¯äº¤æ˜“çš„äº¤æ˜“å¯¹
            quote_asset: åŸºå‡†èµ„äº§ï¼Œé»˜è®¤ä¸ºUSDTï¼Œåªè¿”å›ä»¥è¯¥èµ„äº§ç»“å°¾çš„äº¤æ˜“å¯¹

        Returns:
            list[str]: æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹åˆ—è¡¨
        """
        try:
            logger.info(f"è·å–å½“å‰æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹åˆ—è¡¨ï¼ˆç­›é€‰æ¡ä»¶ï¼š{quote_asset}ç»“å°¾ï¼‰")
            futures_info = self.client.futures_exchange_info()
            perpetual_symbols = [
                symbol["symbol"]
                for symbol in futures_info["symbols"]
                if symbol["contractType"] == "PERPETUAL"
                and (not only_trading or symbol["status"] == "TRADING")
                and symbol["symbol"].endswith(quote_asset)
            ]

            logger.info(f"æ‰¾åˆ° {len(perpetual_symbols)} ä¸ª{quote_asset}æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹")
            return perpetual_symbols

        except Exception as e:
            logger.error(f"[red]è·å–æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹å¤±è´¥: {e}[/red]")
            raise MarketDataFetchError(f"è·å–æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹å¤±è´¥: {e}") from e

    def _date_to_timestamp_range(self, date: str) -> tuple[str, str]:
        """å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ—¶é—´æˆ³èŒƒå›´ï¼ˆå¼€å§‹å’Œç»“æŸï¼‰ã€‚

        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'

        Returns:
            tuple[str, str]: (å¼€å§‹æ—¶é—´æˆ³, ç»“æŸæ—¶é—´æˆ³)ï¼Œéƒ½æ˜¯æ¯«ç§’çº§æ—¶é—´æˆ³å­—ç¬¦ä¸²
            - å¼€å§‹æ—¶é—´æˆ³: å½“å¤©çš„ 00:00:00
            - ç»“æŸæ—¶é—´æˆ³: å½“å¤©çš„ 23:59:59
        """
        start_time = int(
            datetime.strptime(f"{date} 00:00:00", "%Y-%m-%d %H:%M:%S").timestamp() * 1000
        )
        end_time = int(
            datetime.strptime(f"{date} 23:59:59", "%Y-%m-%d %H:%M:%S").timestamp() * 1000
        )
        return str(start_time), str(end_time)

    def _date_to_timestamp_start(self, date: str) -> str:
        """å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºå½“å¤©å¼€å§‹çš„æ—¶é—´æˆ³ã€‚

        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'

        Returns:
            str: å½“å¤© 00:00:00 çš„æ¯«ç§’çº§æ—¶é—´æˆ³å­—ç¬¦ä¸²
        """
        timestamp = int(
            datetime.strptime(f"{date} 00:00:00", "%Y-%m-%d %H:%M:%S").timestamp() * 1000
        )
        return str(timestamp)

    def _date_to_timestamp_end(self, date: str) -> str:
        """å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºå½“å¤©ç»“æŸçš„æ—¶é—´æˆ³ã€‚

        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'

        Returns:
            str: å½“å¤© 23:59:59 çš„æ¯«ç§’çº§æ—¶é—´æˆ³å­—ç¬¦ä¸²
        """
        timestamp = int(
            datetime.strptime(f"{date} 23:59:59", "%Y-%m-%d %H:%M:%S").timestamp() * 1000
        )
        return str(timestamp)

    def check_symbol_exists_on_date(self, symbol: str, date: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šæ—¥æœŸæ˜¯å¦å­˜åœ¨è¯¥äº¤æ˜“å¯¹ã€‚

        Args:
            symbol: äº¤æ˜“å¯¹åç§°
            date: æ—¥æœŸï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'

        Returns:
            bool: æ˜¯å¦å­˜åœ¨è¯¥äº¤æ˜“å¯¹
        """
        try:
            # å°†æ—¥æœŸè½¬æ¢ä¸ºæ—¶é—´æˆ³èŒƒå›´
            start_time, end_time = self._date_to_timestamp_range(date)

            # å°è¯•è·å–è¯¥æ—¶é—´èŒƒå›´å†…çš„Kçº¿æ•°æ®
            klines = self.client.futures_klines(
                symbol=symbol,
                interval="1d",
                startTime=start_time,
                endTime=end_time,
                limit=1,
            )

            # å¦‚æœæœ‰æ•°æ®ï¼Œè¯´æ˜è¯¥æ—¥æœŸå­˜åœ¨è¯¥äº¤æ˜“å¯¹
            return bool(klines and len(klines) > 0)

        except Exception as e:
            logger.debug(f"æ£€æŸ¥äº¤æ˜“å¯¹ {symbol} åœ¨ {date} æ˜¯å¦å­˜åœ¨æ—¶å‡ºé”™: {e}")
            return False

    def get_top_coins(
        self,
        limit: int = settings.DEFAULT_LIMIT,
        sort_by: SortBy = SortBy.QUOTE_VOLUME,
        quote_asset: str | None = None,
    ) -> list[DailyMarketTicker]:
        """è·å–å‰Nä¸ªäº¤æ˜“å¯¹ã€‚

        Args:
            limit: æ•°é‡
            sort_by: æ’åºæ–¹å¼
            quote_asset: åŸºå‡†èµ„äº§

        Returns:
            list[DailyMarketTicker]: å‰Nä¸ªäº¤æ˜“å¯¹
        """
        try:
            tickers = self.client.get_ticker()
            market_tickers = [DailyMarketTicker.from_binance_ticker(t) for t in tickers]

            if quote_asset:
                market_tickers = [t for t in market_tickers if t.symbol.endswith(quote_asset)]

            return sorted(
                market_tickers,
                key=lambda x: getattr(x, sort_by.value),
                reverse=True,
            )[:limit]

        except Exception as e:
            logger.error(f"[red]Error getting top coins: {e}[/red]")
            raise MarketDataFetchError(f"Failed to get top coins: {e}") from e

    def get_market_summary(self, interval: Freq = Freq.d1) -> dict[str, Any]:
        """è·å–å¸‚åœºæ¦‚è§ˆã€‚

        Args:
            interval: æ—¶é—´é—´éš”

        Returns:
            dict[str, Any]: å¸‚åœºæ¦‚è§ˆ
        """
        try:
            summary: dict[str, Any] = {"snapshot_time": datetime.now(), "data": {}}
            tickers_result = self.get_symbol_ticker()
            if isinstance(tickers_result, list):
                tickers = [ticker.to_dict() for ticker in tickers_result]
            else:
                tickers = [tickers_result.to_dict()]
            summary["data"] = tickers

            return summary

        except Exception as e:
            logger.error(f"[red]Error getting market summary: {e}[/red]")
            raise MarketDataFetchError(f"Failed to get market summary: {e}") from e

    def get_historical_klines(
        self,
        symbol: str,
        start_time: str | datetime,
        end_time: str | datetime | None = None,
        interval: Freq = Freq.h1,
        klines_type: HistoricalKlinesType = HistoricalKlinesType.SPOT,
    ) -> list[KlineMarketTicker]:
        """è·å–å†å²è¡Œæƒ…æ•°æ®ã€‚

        Args:
            symbol: äº¤æ˜“å¯¹åç§°
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´ï¼Œå¦‚æœä¸ºNoneåˆ™ä¸ºå½“å‰æ—¶é—´
            interval: æ—¶é—´é—´éš”
            klines_type: Kçº¿ç±»å‹ï¼ˆç°è´§æˆ–æœŸè´§ï¼‰

        Returns:
            list[KlineMarketTicker]: å†å²è¡Œæƒ…æ•°æ®
        """
        try:
            # å¤„ç†æ—¶é—´æ ¼å¼
            if isinstance(start_time, str):
                start_time = datetime.fromisoformat(start_time)
            if end_time is None:
                end_time = datetime.now()
            elif isinstance(end_time, str):
                end_time = datetime.fromisoformat(end_time)

            # è½¬æ¢ä¸ºæ—¶é—´æˆ³
            start_ts = self._date_to_timestamp_start(start_time.strftime("%Y-%m-%d"))
            end_ts = self._date_to_timestamp_end(end_time.strftime("%Y-%m-%d"))

            logger.info(f"è·å– {symbol} çš„å†å²æ•°æ® ({interval.value})")

            # æ ¹æ®klines_typeé€‰æ‹©API
            if klines_type == HistoricalKlinesType.FUTURES:
                klines = self.client.futures_klines(
                    symbol=symbol,
                    interval=interval.value,
                    startTime=start_ts,
                    endTime=end_ts,
                    limit=1500,
                )
            else:  # SPOT
                klines = self.client.get_klines(
                    symbol=symbol,
                    interval=interval.value,
                    startTime=start_ts,
                    endTime=end_ts,
                    limit=1500,
                )

            data = list(klines)
            if not data:
                logger.warning(f"æœªæ‰¾åˆ°äº¤æ˜“å¯¹ {symbol} åœ¨æŒ‡å®šæ—¶é—´æ®µå†…çš„æ•°æ®")
                return []

            # è½¬æ¢ä¸ºKlineMarketTickerå¯¹è±¡
            return [
                KlineMarketTicker(
                    symbol=symbol,
                    last_price=Decimal(str(kline[4])),  # æ”¶ç›˜ä»·ä½œä¸ºæœ€æ–°ä»·æ ¼
                    open_price=Decimal(str(kline[1])),
                    high_price=Decimal(str(kline[2])),
                    low_price=Decimal(str(kline[3])),
                    volume=Decimal(str(kline[5])),
                    close_time=kline[6],
                )
                for kline in data
            ]

        except Exception as e:
            logger.error(f"[red]Error getting historical data for {symbol}: {e}[/red]")
            raise MarketDataFetchError(f"Failed to get historical data: {e}") from e

    def _fetch_symbol_data(
        self,
        symbol: str,
        start_ts: str,
        end_ts: str,
        interval: Freq,
        klines_type: HistoricalKlinesType = HistoricalKlinesType.FUTURES,
    ) -> list[PerpetualMarketTicker]:
        """è·å–å•ä¸ªäº¤æ˜“å¯¹çš„æ•°æ®.

        Args:
            symbol: äº¤æ˜“å¯¹åç§°
            start_ts: å¼€å§‹æ—¶é—´æˆ³ (æ¯«ç§’)
            end_ts: ç»“æŸæ—¶é—´æˆ³ (æ¯«ç§’)
            interval: æ—¶é—´é—´éš”
            klines_type: è¡Œæƒ…ç±»å‹
        """
        try:
            # æ£€æŸ¥äº¤æ˜“å¯¹æ˜¯å¦åœ¨æŒ‡å®šæ—¥æœŸå­˜åœ¨
            if start_ts and end_ts:
                # å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºæ—¥æœŸå­—ç¬¦ä¸²è¿›è¡ŒéªŒè¯
                start_date = datetime.fromtimestamp(int(start_ts) / 1000).strftime("%Y-%m-%d")
                if not self.check_symbol_exists_on_date(symbol, start_date):
                    logger.warning(
                        f"äº¤æ˜“å¯¹ {symbol} åœ¨å¼€å§‹æ—¥æœŸ {start_date} ä¸å­˜åœ¨æˆ–æ²¡æœ‰äº¤æ˜“æ•°æ®ï¼Œ"
                        "å°è¯•è·å–å¯ç”¨æ•°æ®"
                    )
                    # ä¸å†æŠ›å‡ºå¼‚å¸¸ï¼Œè€Œæ˜¯ç»§ç»­æ‰§è¡Œï¼Œè®©APIè¿”å›æœ‰æ•ˆçš„æ•°æ®èŒƒå›´

            klines = self.client.futures_klines(
                symbol=symbol,
                interval=interval.value,
                startTime=start_ts,
                endTime=end_ts,
                limit=1500,
            )

            data = list(klines)
            if not data:
                logger.warning(f"æœªæ‰¾åˆ°äº¤æ˜“å¯¹ {symbol} åœ¨ {start_ts} åˆ° {end_ts} ä¹‹é—´çš„æ•°æ®")
                return []  # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸

            # å¤„ç†æœ‰æ•°æ®çš„æƒ…å†µ
            return [
                PerpetualMarketTicker(
                    symbol=symbol,
                    open_time=kline[0],
                    raw_data=kline,  # ä¿å­˜åŸå§‹æ•°æ®
                )
                for kline in data
            ]

        except InvalidSymbolError:
            # äº¤æ˜“å¯¹ä¸å­˜åœ¨çš„æƒ…å†µç›´æ¥é‡æ–°æŠ›å‡º
            raise
        except Exception as e:
            logger.warning(f"è·å–äº¤æ˜“å¯¹ {symbol} æ•°æ®æ—¶å‡ºé”™: {e}")
            if "Invalid symbol" in str(e):
                raise InvalidSymbolError(f"æ— æ•ˆçš„äº¤æ˜“å¯¹: {symbol}") from e
            else:
                # å¯¹äºå…¶ä»–å¼‚å¸¸ï¼Œä»ç„¶æŠ›å‡ºä»¥ä¾¿ä¸Šå±‚å¤„ç†
                raise MarketDataFetchError(f"è·å–äº¤æ˜“å¯¹ {symbol} æ•°æ®å¤±è´¥: {e}") from e

    def get_perpetual_data(
        self,
        symbols: list[str],
        start_time: str,
        db_path: Path | str,
        end_time: str | None = None,
        interval: Freq = Freq.m1,
        max_workers: int = 1,
        max_retries: int = 3,
        progress: Progress | None = None,
    ) -> None:
        """è·å–æ°¸ç»­åˆçº¦æ•°æ®å¹¶å­˜å‚¨.

        Args:
            symbols: äº¤æ˜“å¯¹åˆ—è¡¨
            start_time: å¼€å§‹æ—¶é—´ (YYYY-MM-DD)
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„ (å¿…é¡»æŒ‡å®šï¼Œå¦‚: /path/to/market.db)
            end_time: ç»“æŸæ—¶é—´ (YYYY-MM-DD)
            interval: æ—¶é—´é—´éš”
            max_workers: æœ€å¤§çº¿ç¨‹æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            progress: è¿›åº¦æ˜¾ç¤ºå™¨
        """
        try:
            if not symbols:
                raise ValueError("Symbols list cannot be empty")

            # éªŒè¯å¹¶å‡†å¤‡æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            db_file_path = self._validate_and_prepare_path(db_path, is_file=True)
            end_time = end_time or datetime.now().strftime("%Y-%m-%d")

            # å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ—¶é—´æˆ³
            start_ts = self._date_to_timestamp_start(start_time)
            end_ts = self._date_to_timestamp_end(end_time)

            # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥ - ç›´æ¥ä½¿ç”¨æŒ‡å®šçš„æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            if self.db is None:
                self.db = MarketDB(str(db_file_path))

            # å¦‚æœæ²¡æœ‰ä¼ å…¥progressï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„
            if progress is None:
                progress = Progress(
                    SpinnerColumn(),
                    TextColumn("[progress.description]{task.description}"),
                    BarColumn(),
                    TimeElapsedColumn(),
                )

            def process_symbol(symbol: str) -> None:
                """å¤„ç†å•ä¸ªäº¤æ˜“å¯¹çš„æ•°æ®è·å–ã€‚"""
                retry_count = 0
                while retry_count < max_retries:
                    try:
                        data = self._fetch_symbol_data(
                            symbol=symbol,
                            start_ts=start_ts,
                            end_ts=end_ts,
                            interval=interval,
                        )

                        if data:
                            # ç¡®ä¿ db ä¸ä¸º None
                            if self.db is None:
                                raise MarketDataFetchError("Database pool is not initialized")
                            self.db.store_data(data, interval)  # ç›´æ¥ä¼ é€’ dataï¼Œä¸éœ€è¦åŒ…è£…æˆåˆ—è¡¨
                            return
                        else:
                            logger.info(f"äº¤æ˜“å¯¹ {symbol} åœ¨æŒ‡å®šæ—¶é—´æ®µå†…æ— æ•°æ®")
                            return

                    except InvalidSymbolError as e:
                        # å¯¹äºäº¤æ˜“å¯¹ä¸å­˜åœ¨çš„æƒ…å†µï¼Œè®°å½•ä¿¡æ¯åç›´æ¥è¿”å›ï¼Œä¸éœ€è¦é‡è¯•
                        logger.warning(f"è·³è¿‡äº¤æ˜“å¯¹ {symbol}: {e}")
                        return
                    except RateLimitError:
                        wait_time = min(2**retry_count + 1, 30)
                        logger.warning(f"é¢‘ç‡é™åˆ¶ - {symbol}: ç­‰å¾… {wait_time} ç§’")
                        time.sleep(wait_time)
                        retry_count += 1
                    except Exception as e:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯APIé¢‘ç‡é™åˆ¶é”™è¯¯
                        if "Too many requests" in str(e) or "APIError" in str(e):
                            wait_time = min(2**retry_count + 10, 60)
                            logger.warning(f"APIé”™è¯¯ - {symbol}: ç­‰å¾… {wait_time} ç§’åé‡è¯•")
                            time.sleep(wait_time)
                            retry_count += 1
                        elif retry_count < max_retries - 1:
                            retry_count += 1
                            logger.warning(f"é‡è¯• {retry_count}/{max_retries} - {symbol}: {str(e)}")
                            time.sleep(2)  # å¢åŠ åŸºç¡€å»¶è¿Ÿ
                        else:
                            logger.error(f"å¤„ç†å¤±è´¥ - {symbol}: {str(e)}")
                            break

            with progress if progress is not None else nullcontext():
                overall_task = (
                    progress.add_task("[cyan]å¤„ç†æ‰€æœ‰äº¤æ˜“å¯¹", total=len(symbols))
                    if progress
                    else None
                )

                # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    futures = [executor.submit(process_symbol, symbol) for symbol in symbols]

                    # è·Ÿè¸ªå®Œæˆè¿›åº¦
                    for future in as_completed(futures):
                        try:
                            future.result()
                            if progress and overall_task is not None:
                                progress.update(overall_task, advance=1)
                        except Exception as e:
                            logger.error(f"å¤„ç†å¤±è´¥: {e}")

            logger.info("âœ… Universeæ•°æ®ä¸‹è½½å®Œæˆ!")
            logger.info(f"ğŸ“ æ•°æ®å·²ä¿å­˜åˆ°: {db_file_path}")

        except Exception as e:
            logger.error(f"Failed to fetch perpetual data: {e}")
            raise MarketDataFetchError(f"Failed to fetch perpetual data: {e}") from e

    def define_universe(
        self,
        start_date: str,
        end_date: str,
        t1_months: int,
        t2_months: int,
        t3_months: int,
        top_k: int,
        output_path: Path | str,
        description: str | None = None,
        delay_days: int = 7,
        api_delay_seconds: float = 1.0,
        batch_delay_seconds: float = 3.0,
        batch_size: int = 5,
        quote_asset: str = "USDT",
    ) -> UniverseDefinition:
        """å®šä¹‰universeå¹¶ä¿å­˜åˆ°æ–‡ä»¶.

        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD æˆ– YYYYMMDD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD æˆ– YYYYMMDD)
            t1_months: T1æ—¶é—´çª—å£ï¼ˆæœˆï¼‰ï¼Œç”¨äºè®¡ç®—mean daily amount
            t2_months: T2æ»šåŠ¨é¢‘ç‡ï¼ˆæœˆï¼‰ï¼Œuniverseé‡æ–°é€‰æ‹©çš„é¢‘ç‡
            t3_months: T3åˆçº¦æœ€å°åˆ›å»ºæ—¶é—´ï¼ˆæœˆï¼‰ï¼Œç”¨äºç­›é™¤æ–°åˆçº¦
            top_k: é€‰å–çš„topåˆçº¦æ•°é‡
            output_path: universeè¾“å‡ºæ–‡ä»¶è·¯å¾„ (å¿…é¡»æŒ‡å®š)
            description: æè¿°ä¿¡æ¯
            delay_days: åœ¨é‡æ–°å¹³è¡¡æ—¥æœŸå‰é¢å¤–å¾€å‰æ¨çš„å¤©æ•°ï¼Œé»˜è®¤7å¤©
            api_delay_seconds: æ¯ä¸ªAPIè¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿç§’æ•°ï¼Œé»˜è®¤1.0ç§’
            batch_delay_seconds: æ¯æ‰¹æ¬¡è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿç§’æ•°ï¼Œé»˜è®¤3.0ç§’
            batch_size: æ¯æ‰¹æ¬¡çš„è¯·æ±‚æ•°é‡ï¼Œé»˜è®¤5ä¸ª
            quote_asset: åŸºå‡†èµ„äº§ï¼Œé»˜è®¤ä¸ºUSDTï¼Œåªç­›é€‰ä»¥è¯¥èµ„äº§ç»“å°¾çš„äº¤æ˜“å¯¹

        Returns:
            UniverseDefinition: å®šä¹‰çš„universe
        """
        try:
            # éªŒè¯å¹¶å‡†å¤‡è¾“å‡ºè·¯å¾„
            output_path_obj = self._validate_and_prepare_path(
                output_path,
                is_file=True,
                file_name=(
                    f"universe_{start_date}_{end_date}_{t1_months}_"
                    f"{t2_months}_{t3_months}_{top_k}.json"
                ),
            )

            # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
            start_date = self._standardize_date_format(start_date)
            end_date = self._standardize_date_format(end_date)

            # åˆ›å»ºé…ç½®
            config = UniverseConfig(
                start_date=start_date,
                end_date=end_date,
                t1_months=t1_months,
                t2_months=t2_months,
                t3_months=t3_months,
                top_k=top_k,
            )

            logger.info(f"å¼€å§‹å®šä¹‰universe: {start_date} åˆ° {end_date}")
            logger.info(
                f"å‚æ•°: T1={t1_months}æœˆ, T2={t2_months}æœˆ, T3={t3_months}æœˆ, Top-K={top_k}"
            )

            # ç”Ÿæˆé‡æ–°é€‰æ‹©æ—¥æœŸåºåˆ— (æ¯T2ä¸ªæœˆ)
            # ä»èµ·å§‹æ—¥æœŸå¼€å§‹ï¼Œæ¯éš”T2ä¸ªæœˆç”Ÿæˆé‡å¹³è¡¡æ—¥æœŸï¼Œè¡¨ç¤ºuniverseé‡æ–°é€‰æ‹©çš„æ—¶é—´ç‚¹
            rebalance_dates = self._generate_rebalance_dates(start_date, end_date, t2_months)

            logger.info("é‡å¹³è¡¡è®¡åˆ’:")
            logger.info(f"  - æ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}")
            logger.info(f"  - é‡å¹³è¡¡é—´éš”: æ¯{t2_months}ä¸ªæœˆ")
            logger.info(f"  - æ•°æ®å»¶è¿Ÿ: {delay_days}å¤©")
            logger.info(f"  - T1æ•°æ®çª—å£: {t1_months}ä¸ªæœˆ")
            logger.info(f"  - é‡å¹³è¡¡æ—¥æœŸ: {rebalance_dates}")

            if not rebalance_dates:
                raise ValueError("æ— æ³•ç”Ÿæˆé‡å¹³è¡¡æ—¥æœŸï¼Œè¯·æ£€æŸ¥æ—¶é—´èŒƒå›´å’ŒT2å‚æ•°")

            # æ”¶é›†æ‰€æœ‰å‘¨æœŸçš„snapshots
            all_snapshots = []

            # åœ¨æ¯ä¸ªé‡æ–°é€‰æ‹©æ—¥æœŸè®¡ç®—universe
            for i, rebalance_date in enumerate(rebalance_dates):
                logger.info(f"å¤„ç†æ—¥æœŸ {i + 1}/{len(rebalance_dates)}: {rebalance_date}")

                # è®¡ç®—åŸºå‡†æ—¥æœŸï¼ˆé‡æ–°å¹³è¡¡æ—¥æœŸå‰delay_dayså¤©ï¼‰
                base_date = pd.to_datetime(rebalance_date) - timedelta(days=delay_days)
                base_date_str = base_date.strftime("%Y-%m-%d")

                # è®¡ç®—T1å›çœ‹æœŸé—´çš„å¼€å§‹æ—¥æœŸï¼ˆä»base_dateå¾€å‰æ¨T1ä¸ªæœˆï¼‰
                calculated_t1_start = self._subtract_months(base_date_str, t1_months)

                logger.info(
                    f"å‘¨æœŸ {i + 1}: åŸºå‡†æ—¥æœŸ={base_date_str} (é‡æ–°å¹³è¡¡æ—¥æœŸå‰{delay_days}å¤©), "
                    f"T1æ•°æ®æœŸé—´={calculated_t1_start} åˆ° {base_date_str}"
                )

                # è·å–ç¬¦åˆæ¡ä»¶çš„äº¤æ˜“å¯¹å’Œå®ƒä»¬çš„mean daily amount
                universe_symbols, mean_amounts = self._calculate_universe_for_date(
                    rebalance_date=base_date_str,  # ä½¿ç”¨åŸºå‡†æ—¥æœŸä½œä¸ºè®¡ç®—ç»ˆç‚¹
                    t1_start_date=calculated_t1_start,
                    t3_months=t3_months,
                    top_k=top_k,
                    api_delay_seconds=api_delay_seconds,
                    batch_delay_seconds=batch_delay_seconds,
                    batch_size=batch_size,
                    quote_asset=quote_asset,
                )

                # åˆ›å»ºè¯¥å‘¨æœŸçš„snapshot
                # è®¡ç®—æ—¶é—´æˆ³
                start_ts = self._date_to_timestamp_start(calculated_t1_start)
                end_ts = self._date_to_timestamp_end(base_date_str)

                snapshot = UniverseSnapshot.create_with_dates_and_timestamps(
                    effective_date=rebalance_date,  # ä½¿ç”¨åŸå§‹é‡æ–°å¹³è¡¡æ—¥æœŸ
                    period_start_date=calculated_t1_start,
                    period_end_date=base_date_str,  # ä½¿ç”¨åŸºå‡†æ—¥æœŸä½œä¸ºæ•°æ®ç»“æŸæ—¥æœŸ
                    period_start_ts=start_ts,
                    period_end_ts=end_ts,
                    symbols=universe_symbols,
                    mean_daily_amounts=mean_amounts,
                    metadata={
                        "t1_start_date": calculated_t1_start,
                        "base_date": base_date_str,
                        "delay_days": delay_days,
                        "quote_asset": quote_asset,
                        "selected_symbols_count": len(universe_symbols),
                    },
                )

                all_snapshots.append(snapshot)

                logger.info(f"âœ… æ—¥æœŸ {rebalance_date}: é€‰æ‹©äº† {len(universe_symbols)} ä¸ªäº¤æ˜“å¯¹")

            # åˆ›å»ºå®Œæ•´çš„universeå®šä¹‰
            universe_def = UniverseDefinition(
                config=config,
                snapshots=all_snapshots,
                creation_time=datetime.now(),
                description=description,
            )

            # ä¿å­˜æ±‡æ€»çš„universeå®šä¹‰
            universe_def.save_to_file(output_path_obj)

            logger.info("ğŸ‰ Universeå®šä¹‰å®Œæˆï¼")
            logger.info(f"ğŸ“ åŒ…å« {len(all_snapshots)} ä¸ªé‡æ–°å¹³è¡¡å‘¨æœŸ")
            logger.info(f"ğŸ“‹ æ±‡æ€»æ–‡ä»¶: {output_path_obj}")

            return universe_def

        except Exception as e:
            logger.error(f"[red]å®šä¹‰universeå¤±è´¥: {e}[/red]")
            raise MarketDataFetchError(f"å®šä¹‰universeå¤±è´¥: {e}") from e

    def _standardize_date_format(self, date_str: str) -> str:
        """æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼ä¸º YYYY-MM-DDã€‚"""
        if len(date_str) == 8:  # YYYYMMDD
            return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
        return date_str

    def _generate_rebalance_dates(
        self, start_date: str, end_date: str, t2_months: int
    ) -> list[str]:
        """ç”Ÿæˆé‡æ–°é€‰æ‹©universeçš„æ—¥æœŸåºåˆ—ã€‚

        ä»èµ·å§‹æ—¥æœŸå¼€å§‹ï¼Œæ¯éš”T2ä¸ªæœˆç”Ÿæˆé‡å¹³è¡¡æ—¥æœŸï¼Œè¿™äº›æ—¥æœŸè¡¨ç¤ºuniverseé‡æ–°é€‰æ‹©çš„æ—¶é—´ç‚¹ã€‚

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            t2_months: é‡æ–°å¹³è¡¡é—´éš”ï¼ˆæœˆï¼‰

        Returns:
            list[str]: é‡å¹³è¡¡æ—¥æœŸåˆ—è¡¨
        """
        dates = []
        start_date_obj = pd.to_datetime(start_date)
        end_date_obj = pd.to_datetime(end_date)

        # ä»èµ·å§‹æ—¥æœŸå¼€å§‹ï¼Œæ¯éš”T2ä¸ªæœˆç”Ÿæˆé‡å¹³è¡¡æ—¥æœŸ
        current_date = start_date_obj

        while current_date <= end_date_obj:
            dates.append(current_date.strftime("%Y-%m-%d"))
            current_date = current_date + pd.DateOffset(months=t2_months)

        return dates

    def _subtract_months(self, date_str: str, months: int) -> str:
        """ä»æ—¥æœŸå‡å»æŒ‡å®šæœˆæ•°ã€‚"""
        date_obj = pd.to_datetime(date_str)
        # ä½¿ç”¨pandasçš„DateOffsetæ¥æ­£ç¡®å¤„ç†æœˆä»½è¾¹ç•Œé—®é¢˜
        result_date = date_obj - pd.DateOffset(months=months)
        return str(result_date.strftime("%Y-%m-%d"))

    def _get_available_symbols_for_period(
        self, start_date: str, end_date: str, quote_asset: str = "USDT"
    ) -> list[str]:
        """è·å–æŒ‡å®šæ—¶é—´æ®µå†…å®é™…å¯ç”¨çš„æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹ã€‚

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            quote_asset: åŸºå‡†èµ„äº§ï¼Œç”¨äºç­›é€‰äº¤æ˜“å¯¹

        Returns:
            list[str]: åœ¨è¯¥æ—¶é—´æ®µå†…æœ‰æ•°æ®çš„äº¤æ˜“å¯¹åˆ—è¡¨
        """
        try:
            # å…ˆè·å–å½“å‰æ‰€æœ‰æ°¸ç»­åˆçº¦ä½œä¸ºå€™é€‰ï¼ˆç­›é€‰æŒ‡å®šçš„åŸºå‡†èµ„äº§ï¼‰
            candidate_symbols = self.get_perpetual_symbols(
                only_trading=True, quote_asset=quote_asset
            )
            logger.info(
                f"æ£€æŸ¥ {len(candidate_symbols)} ä¸ª{quote_asset}å€™é€‰äº¤æ˜“å¯¹åœ¨ {start_date} åˆ° "
                f"{end_date} æœŸé—´çš„å¯ç”¨æ€§..."
            )

            available_symbols = []
            batch_size = 50
            for i in range(0, len(candidate_symbols), batch_size):
                batch = candidate_symbols[i : i + batch_size]
                for symbol in batch:
                    # æ£€æŸ¥åœ¨èµ·å§‹æ—¥æœŸæ˜¯å¦æœ‰æ•°æ®
                    if self.check_symbol_exists_on_date(symbol, start_date):
                        available_symbols.append(symbol)

                # æ˜¾ç¤ºè¿›åº¦
                processed = min(i + batch_size, len(candidate_symbols))
                logger.info(
                    f"å·²æ£€æŸ¥ {processed}/{len(candidate_symbols)} ä¸ªäº¤æ˜“å¯¹ï¼Œ"
                    f"æ‰¾åˆ° {len(available_symbols)} ä¸ªå¯ç”¨äº¤æ˜“å¯¹"
                )

                # é¿å…APIé¢‘ç‡é™åˆ¶
                time.sleep(0.1)

            logger.info(
                f"åœ¨ {start_date} åˆ° {end_date} æœŸé—´æ‰¾åˆ° {len(available_symbols)} "
                f"ä¸ªå¯ç”¨çš„{quote_asset}æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹"
            )
            return available_symbols

        except Exception as e:
            logger.warning(f"è·å–å¯ç”¨äº¤æ˜“å¯¹æ—¶å‡ºé”™: {e}")
            # å¦‚æœAPIæ£€æŸ¥å¤±è´¥ï¼Œè¿”å›å½“å‰æ‰€æœ‰æ°¸ç»­åˆçº¦
            return self.get_perpetual_symbols(only_trading=True, quote_asset=quote_asset)

    def _calculate_universe_for_date(
        self,
        rebalance_date: str,
        t1_start_date: str,
        t3_months: int,
        top_k: int,
        api_delay_seconds: float = 1.0,
        batch_delay_seconds: float = 3.0,
        batch_size: int = 5,
        quote_asset: str = "USDT",
    ) -> tuple[list[str], dict[str, float]]:
        """è®¡ç®—æŒ‡å®šæ—¥æœŸçš„universeã€‚

        Args:
            rebalance_date: é‡å¹³è¡¡æ—¥æœŸ
            t1_start_date: T1å¼€å§‹æ—¥æœŸ
            t3_months: T3æœˆæ•°
            top_k: é€‰æ‹©çš„topæ•°é‡
            api_delay_seconds: æ¯ä¸ªAPIè¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿç§’æ•°
            batch_delay_seconds: æ¯æ‰¹æ¬¡è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿç§’æ•°
            batch_size: æ¯æ‰¹æ¬¡çš„è¯·æ±‚æ•°é‡
            quote_asset: åŸºå‡†èµ„äº§ï¼Œç”¨äºç­›é€‰äº¤æ˜“å¯¹
        """
        try:
            # è·å–åœ¨è¯¥æ—¶é—´æ®µå†…å®é™…å­˜åœ¨çš„æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹
            actual_symbols = self._get_available_symbols_for_period(
                t1_start_date, rebalance_date, quote_asset
            )

            # ç­›é™¤æ–°åˆçº¦ (åˆ›å»ºæ—¶é—´ä¸è¶³T3ä¸ªæœˆçš„)
            cutoff_date = self._subtract_months(rebalance_date, t3_months)
            eligible_symbols = [
                symbol
                for symbol in actual_symbols
                if self._symbol_exists_before_date(symbol, cutoff_date)
            ]

            if not eligible_symbols:
                logger.warning(f"æ—¥æœŸ {rebalance_date}: æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„äº¤æ˜“å¯¹")
                return [], {}

            # é€šè¿‡APIè·å–æ•°æ®è®¡ç®—mean daily amount
            mean_amounts = {}

            logger.info(f"å¼€å§‹é€šè¿‡APIè·å– {len(eligible_symbols)} ä¸ªäº¤æ˜“å¯¹çš„å†å²æ•°æ®...")

            for i, symbol in enumerate(eligible_symbols):
                try:
                    # å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ—¶é—´æˆ³
                    start_ts = self._date_to_timestamp_start(t1_start_date)
                    end_ts = self._date_to_timestamp_end(rebalance_date)

                    # å‚æ•°åŒ–çš„é¢‘ç‡æ§åˆ¶
                    if i > 0:  # ç¬¬ä¸€ä¸ªè¯·æ±‚ä¸éœ€è¦å»¶è¿Ÿ
                        if i % batch_size == 0:  # æ¯æ‰¹æ¬¡è¯·æ±‚å»¶è¿Ÿæ›´é•¿æ—¶é—´
                            logger.info(
                                f"å·²å¤„ç† {i}/{len(eligible_symbols)} ä¸ªäº¤æ˜“å¯¹ï¼Œ"
                                f"ç­‰å¾…{batch_delay_seconds}ç§’..."
                            )
                            time.sleep(batch_delay_seconds)
                        else:
                            time.sleep(api_delay_seconds)  # æ¯ä¸ªè¯·æ±‚ä¹‹é—´çš„åŸºç¡€å»¶è¿Ÿ

                    # è·å–å†å²Kçº¿æ•°æ®
                    klines = self._fetch_symbol_data(
                        symbol=symbol,
                        start_ts=start_ts,
                        end_ts=end_ts,
                        interval=Freq.d1,
                    )

                    if klines:
                        # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
                        expected_days = (
                            pd.to_datetime(rebalance_date) - pd.to_datetime(t1_start_date)
                        ).days + 1
                        actual_days = len(klines)

                        if actual_days < expected_days * 0.8:  # å…è®¸20%çš„æ•°æ®ç¼ºå¤±
                            logger.warning(
                                f"äº¤æ˜“å¯¹ {symbol} æ•°æ®ä¸å®Œæ•´: æœŸæœ›{expected_days}å¤©ï¼Œ"
                                f"å®é™…{actual_days}å¤©"
                            )

                        # è®¡ç®—å¹³å‡æ—¥æˆäº¤é¢
                        amounts = []
                        for kline in klines:
                            try:
                                # kline.raw_data[7] æ˜¯æˆäº¤é¢ï¼ˆUSDTï¼‰
                                if kline.raw_data and len(kline.raw_data) > 7:
                                    amount = float(kline.raw_data[7])
                                    amounts.append(amount)
                            except (ValueError, IndexError):
                                continue

                        if amounts:
                            mean_amount = sum(amounts) / len(amounts)
                            mean_amounts[symbol] = mean_amount
                        else:
                            logger.warning(f"äº¤æ˜“å¯¹ {symbol} åœ¨æœŸé—´å†…æ²¡æœ‰æœ‰æ•ˆçš„æˆäº¤é‡æ•°æ®")

                except RateLimitError:
                    # é‡åˆ°é¢‘ç‡é™åˆ¶æ—¶ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´åé‡è¯•
                    logger.warning(f"é‡åˆ°é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾…60ç§’åç»§ç»­å¤„ç† {symbol}")
                    time.sleep(60)
                    try:
                        # é‡è¯•ä¸€æ¬¡
                        klines = self._fetch_symbol_data(
                            symbol=symbol,
                            start_ts=start_ts,
                            end_ts=end_ts,
                            interval=Freq.d1,
                        )
                        # ... å¤„ç†æ•°æ®çš„ä»£ç ä¿æŒç›¸åŒ
                    except Exception:
                        logger.warning(f"é‡è¯•åä»ç„¶å¤±è´¥ï¼Œè·³è¿‡ {symbol}")
                        continue
                except Exception as e:
                    logger.warning(f"è·å– {symbol} æ•°æ®æ—¶å‡ºé”™ï¼Œè·³è¿‡: {e}")
                    # å¦‚æœæ˜¯APIé”™è¯¯ï¼Œå¢åŠ å»¶è¿Ÿ
                    if "Too many requests" in str(e) or "APIError" in str(e):
                        logger.info("æ£€æµ‹åˆ°APIé”™è¯¯ï¼Œå¢åŠ å»¶è¿Ÿæ—¶é—´")
                        time.sleep(5)
                    continue

            # æŒ‰mean daily amountæ’åºå¹¶é€‰æ‹©top_k
            if mean_amounts:
                sorted_symbols = sorted(mean_amounts.items(), key=lambda x: x[1], reverse=True)
                top_symbols = sorted_symbols[:top_k]

                universe_symbols = [symbol for symbol, _ in top_symbols]
                final_amounts = dict(top_symbols)

                # æ˜¾ç¤ºé€‰æ‹©ç»“æœ
                if len(universe_symbols) <= 10:
                    logger.info(f"é€‰ä¸­çš„äº¤æ˜“å¯¹: {universe_symbols}")
                else:
                    logger.info(f"Top 5: {universe_symbols[:5]}")
                    logger.info("å®Œæ•´åˆ—è¡¨å·²ä¿å­˜åˆ°æ–‡ä»¶ä¸­")
            else:
                # å¦‚æœæ²¡æœ‰å¯ç”¨æ•°æ®ï¼Œè‡³å°‘è¿”å›å‰top_kä¸ªeligible symbols
                universe_symbols = eligible_symbols[:top_k]
                final_amounts = dict.fromkeys(universe_symbols, 0.0)
                logger.warning(f"æ— æ³•é€šè¿‡APIè·å–æ•°æ®ï¼Œè¿”å›å‰{len(universe_symbols)}ä¸ªäº¤æ˜“å¯¹")

            return universe_symbols, final_amounts

        except Exception as e:
            logger.error(f"è®¡ç®—æ—¥æœŸ {rebalance_date} çš„universeæ—¶å‡ºé”™: {e}")
            return [], {}

    def _symbol_exists_before_date(self, symbol: str, cutoff_date: str) -> bool:
        """æ£€æŸ¥äº¤æ˜“å¯¹æ˜¯å¦åœ¨æŒ‡å®šæ—¥æœŸä¹‹å‰å°±å­˜åœ¨ã€‚"""
        try:
            # æ£€æŸ¥åœ¨cutoff_dateä¹‹å‰æ˜¯å¦æœ‰æ•°æ®
            # è¿™é‡Œæˆ‘ä»¬æ£€æŸ¥cutoff_dateå‰ä¸€å¤©çš„æ•°æ®
            check_date = (pd.to_datetime(cutoff_date) - timedelta(days=1)).strftime("%Y-%m-%d")
            return self.check_symbol_exists_on_date(symbol, check_date)
        except Exception:
            # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œé»˜è®¤è®¤ä¸ºå­˜åœ¨
            return True

    def download_universe_data(
        self,
        universe_file: Path | str,
        db_path: Path | str,
        data_path: Path | str | None = None,
        interval: Freq = Freq.h1,
        max_workers: int = 4,
        max_retries: int = 3,
        include_buffer_days: int = 7,
        extend_to_present: bool = True,
    ) -> None:
        """æ ¹æ®universeå®šä¹‰æ–‡ä»¶ä¸‹è½½ç›¸åº”çš„å†å²æ•°æ®åˆ°æ•°æ®åº“ã€‚

        Args:
            universe_file: universeå®šä¹‰æ–‡ä»¶è·¯å¾„ (å¿…é¡»æŒ‡å®š)
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„ (å¿…é¡»æŒ‡å®šï¼Œå¦‚: /path/to/market.db)
            data_path: æ•°æ®æ–‡ä»¶å­˜å‚¨è·¯å¾„ (å¯é€‰ï¼Œç”¨äºå­˜å‚¨å…¶ä»–æ•°æ®æ–‡ä»¶)
            interval: æ•°æ®é¢‘ç‡ (1m, 1h, 4h, 1dç­‰)
            max_workers: å¹¶å‘çº¿ç¨‹æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            include_buffer_days: åœ¨æ•°æ®æœŸé—´å‰åå¢åŠ çš„ç¼“å†²å¤©æ•°
            extend_to_present: æ˜¯å¦å°†æ•°æ®æ‰©å±•åˆ°å½“å‰æ—¥æœŸ

        Example:
            service.download_universe_data(
                universe_file="/path/to/universe.json",
                db_path="/path/to/database/market.db",
                data_path="/path/to/data",  # å¯é€‰
                interval=Freq.h1,
                max_workers=4
            )
        """
        try:
            # éªŒè¯è·¯å¾„
            universe_file_obj = self._validate_and_prepare_path(universe_file, is_file=True)
            db_file_path = self._validate_and_prepare_path(db_path, is_file=True)

            # data_pathæ˜¯å¯é€‰çš„ï¼Œå¦‚æœæä¾›åˆ™éªŒè¯
            data_path_obj = None
            if data_path:
                data_path_obj = self._validate_and_prepare_path(data_path, is_file=False)

            # æ£€æŸ¥universeæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not universe_file_obj.exists():
                raise FileNotFoundError(f"Universeæ–‡ä»¶ä¸å­˜åœ¨: {universe_file_obj}")

            # åŠ è½½universeå®šä¹‰
            universe_def = UniverseDefinition.load_from_file(universe_file_obj)

            # åˆ†ææ•°æ®ä¸‹è½½éœ€æ±‚
            download_plan = self._analyze_universe_data_requirements(
                universe_def, include_buffer_days, extend_to_present
            )

            logger.info("ğŸ“Š æ•°æ®ä¸‹è½½è®¡åˆ’:")
            logger.info(f"   - æ€»äº¤æ˜“å¯¹æ•°: {download_plan['total_symbols']}")
            logger.info(
                f"   - æ—¶é—´èŒƒå›´: {download_plan['overall_start_date']} åˆ° "
                f"{download_plan['overall_end_date']}"
            )
            logger.info(f"   - æ•°æ®é¢‘ç‡: {interval.value}")
            logger.info(f"   - å¹¶å‘çº¿ç¨‹: {max_workers}")
            logger.info(f"   - æ•°æ®åº“è·¯å¾„: {db_file_path}")
            if data_path_obj:
                logger.info(f"   - æ•°æ®æ–‡ä»¶è·¯å¾„: {data_path_obj}")

            # æ‰§è¡Œæ•°æ®ä¸‹è½½
            self.get_perpetual_data(
                symbols=download_plan["unique_symbols"],
                start_time=download_plan["overall_start_date"],
                end_time=download_plan["overall_end_date"],
                db_path=db_file_path,
                interval=interval,
                max_workers=max_workers,
                max_retries=max_retries,
            )

            logger.info("âœ… Universeæ•°æ®ä¸‹è½½å®Œæˆ!")
            logger.info(f"ğŸ“ æ•°æ®å·²ä¿å­˜åˆ°: {db_file_path}")

            # éªŒè¯æ•°æ®å®Œæ•´æ€§
            self._verify_universe_data_integrity(
                universe_def, db_file_path, interval, download_plan
            )

        except Exception as e:
            logger.error(f"[red]ä¸‹è½½universeæ•°æ®å¤±è´¥: {e}[/red]")
            raise MarketDataFetchError(f"ä¸‹è½½universeæ•°æ®å¤±è´¥: {e}") from e

    def _analyze_universe_data_requirements(
        self,
        universe_def: UniverseDefinition,
        buffer_days: int = 7,
        extend_to_present: bool = True,
    ) -> dict[str, Any]:
        """åˆ†æuniverseæ•°æ®ä¸‹è½½éœ€æ±‚ã€‚

        Args:
            universe_def: Universeå®šä¹‰
            buffer_days: ç¼“å†²å¤©æ•°
            extend_to_present: æ˜¯å¦æ‰©å±•åˆ°å½“å‰æ—¥æœŸ

        Returns:
            Dict: ä¸‹è½½è®¡åˆ’è¯¦æƒ…
        """
        import pandas as pd

        # æ”¶é›†æ‰€æœ‰çš„äº¤æ˜“å¯¹å’Œæ—¶é—´èŒƒå›´
        all_symbols = set()
        all_dates = []

        for snapshot in universe_def.snapshots:
            all_symbols.update(snapshot.symbols)
            all_dates.extend(
                [
                    snapshot.period_start_date,
                    snapshot.period_end_date,
                    snapshot.effective_date,
                ]
            )

        # è®¡ç®—æ€»ä½“æ—¶é—´èŒƒå›´
        start_date = pd.to_datetime(min(all_dates)) - timedelta(days=buffer_days)
        end_date = pd.to_datetime(max(all_dates)) + timedelta(days=buffer_days)

        if extend_to_present:
            end_date = max(end_date, pd.to_datetime("today"))

        return {
            "unique_symbols": sorted(all_symbols),
            "total_symbols": len(all_symbols),
            "overall_start_date": start_date.strftime("%Y-%m-%d"),
            "overall_end_date": end_date.strftime("%Y-%m-%d"),
        }

    def _verify_universe_data_integrity(
        self,
        universe_def: UniverseDefinition,
        db_path: Path,
        interval: Freq,
        download_plan: dict[str, Any],
    ) -> None:
        """éªŒè¯ä¸‹è½½çš„universeæ•°æ®å®Œæ•´æ€§ã€‚

        Args:
            universe_def: Universeå®šä¹‰
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            interval: æ•°æ®é¢‘ç‡
            download_plan: ä¸‹è½½è®¡åˆ’
        """
        try:
            from cryptoservice.data import MarketDB

            # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥ - ç›´æ¥ä½¿ç”¨æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            db = MarketDB(str(db_path))

            logger.info("ğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§...")
            incomplete_symbols: list[str] = []
            missing_data: list[dict[str, str]] = []
            successful_snapshots = 0

            for snapshot in universe_def.snapshots:
                try:
                    # æ£€æŸ¥è¯¥å¿«ç…§çš„ä¸»è¦äº¤æ˜“å¯¹æ•°æ®ï¼Œä½¿ç”¨æ›´å®½æ³›çš„æ—¶é—´èŒƒå›´
                    # æ‰©å±•æ—¶é—´èŒƒå›´ä»¥ç¡®ä¿èƒ½å¤Ÿæ‰¾åˆ°æ•°æ®
                    period_start = pd.to_datetime(snapshot.period_start_date) - timedelta(days=3)
                    period_end = pd.to_datetime(snapshot.period_end_date) + timedelta(days=3)

                    df = db.read_data(
                        symbols=snapshot.symbols[:3],  # åªæ£€æŸ¥å‰3ä¸ªä¸»è¦äº¤æ˜“å¯¹
                        start_time=period_start.strftime("%Y-%m-%d"),
                        end_time=period_end.strftime("%Y-%m-%d"),
                        freq=interval,
                        raise_on_empty=False,  # ä¸åœ¨æ²¡æœ‰æ•°æ®æ—¶æŠ›å‡ºå¼‚å¸¸
                    )

                    if df is not None and not df.empty:
                        # æ£€æŸ¥æ•°æ®è¦†ç›–çš„äº¤æ˜“å¯¹æ•°é‡
                        available_symbols = df.index.get_level_values("symbol").unique()
                        missing_symbols = set(snapshot.symbols[:3]) - set(available_symbols)
                        if missing_symbols:
                            incomplete_symbols.extend(missing_symbols)
                            logger.debug(
                                f"å¿«ç…§ {snapshot.effective_date}ç¼ºå°‘äº¤æ˜“å¯¹: {list(missing_symbols)}"
                            )
                        else:
                            successful_snapshots += 1
                            logger.debug(f"å¿«ç…§ {snapshot.effective_date} éªŒè¯æˆåŠŸ")
                    else:
                        logger.debug(f"å¿«ç…§ {snapshot.effective_date} åœ¨æ‰©å±•æ—¶é—´èŒƒå›´å†…æœªæ‰¾åˆ°æ•°æ®")
                        missing_data.append(
                            {
                                "snapshot_date": snapshot.effective_date,
                                "error": "No data in extended time range",
                            }
                        )

                except Exception as e:
                    logger.debug(f"éªŒè¯å¿«ç…§ {snapshot.effective_date} æ—¶å‡ºé”™: {e}")
                    # ä¸å†è®°å½•ä¸ºä¸¥é‡é”™è¯¯ï¼Œåªæ˜¯è®°å½•è°ƒè¯•ä¿¡æ¯
                    missing_data.append({"snapshot_date": snapshot.effective_date, "error": str(e)})

            # æŠ¥å‘ŠéªŒè¯ç»“æœ - æ›´å‹å¥½çš„æŠ¥å‘Šæ–¹å¼
            total_snapshots = len(universe_def.snapshots)
            success_rate = successful_snapshots / total_snapshots if total_snapshots > 0 else 0

            logger.info("âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯å®Œæˆ")
            logger.info(f"   - å·²ä¸‹è½½äº¤æ˜“å¯¹: {download_plan['total_symbols']} ä¸ª")
            logger.info(
                f"   - æ—¶é—´èŒƒå›´: {download_plan['overall_start_date']} åˆ° "
                f"{download_plan['overall_end_date']}"
            )
            logger.info(f"   - æ•°æ®é¢‘ç‡: {interval.value}")
            logger.info(
                f"   - æˆåŠŸéªŒè¯å¿«ç…§: {successful_snapshots}/{total_snapshots} ({success_rate:.1%})"
            )

            # åªæœ‰åœ¨æˆåŠŸç‡å¾ˆä½æ—¶æ‰ç»™å‡ºè­¦å‘Š
            if success_rate < 0.5:
                logger.warning(f"âš ï¸ éªŒè¯æˆåŠŸç‡è¾ƒä½: {success_rate:.1%}")
                if incomplete_symbols:
                    unique_incomplete = set(incomplete_symbols)
                    logger.warning(f"   - æ•°æ®ä¸å®Œæ•´çš„äº¤æ˜“å¯¹: {len(unique_incomplete)} ä¸ª")
                    if len(unique_incomplete) <= 5:
                        logger.warning(f"   - å…·ä½“äº¤æ˜“å¯¹: {list(unique_incomplete)}")

                if missing_data:
                    logger.warning(f"   - æ— æ³•éªŒè¯çš„å¿«ç…§: {len(missing_data)} ä¸ª")
            else:
                logger.info("ğŸ“Š æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå»ºè®®è¿›è¡Œåç»­åˆ†æ")

        except Exception as e:
            logger.warning(f"æ•°æ®å®Œæ•´æ€§éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜ï¼Œä½†ä¸å½±å“æ•°æ®ä½¿ç”¨: {e}")
            logger.info("ğŸ’¡ æç¤º: éªŒè¯å¤±è´¥ä¸ä»£è¡¨æ•°æ®ä¸‹è½½å¤±è´¥ï¼Œå¯ä»¥å°è¯•æŸ¥è¯¢å…·ä½“æ•°æ®è¿›è¡Œç¡®è®¤")

    def download_universe_data_by_periods(
        self,
        universe_file: Path | str,
        db_path: Path | str,
        data_path: Path | str | None = None,
        interval: Freq = Freq.h1,
        max_workers: int = 4,
        max_retries: int = 3,
        include_buffer_days: int = 7,
    ) -> None:
        """æŒ‰å‘¨æœŸåˆ†åˆ«ä¸‹è½½universeæ•°æ®ï¼ˆæ›´ç²¾ç¡®çš„ä¸‹è½½æ–¹å¼ï¼‰ã€‚

        è¿™ç§æ–¹å¼ä¸ºæ¯ä¸ªé‡å¹³è¡¡å‘¨æœŸå•ç‹¬ä¸‹è½½æ•°æ®ï¼Œå¯ä»¥é¿å…ä¸‹è½½ä¸å¿…è¦çš„æ•°æ®ã€‚

        Args:
            universe_file: universeå®šä¹‰æ–‡ä»¶è·¯å¾„ (å¿…é¡»æŒ‡å®š)
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„ (å¿…é¡»æŒ‡å®šï¼Œå¦‚: /path/to/market.db)
            data_path: æ•°æ®æ–‡ä»¶å­˜å‚¨è·¯å¾„ (å¯é€‰ï¼Œç”¨äºå­˜å‚¨å…¶ä»–æ•°æ®æ–‡ä»¶)
            interval: æ•°æ®é¢‘ç‡
            max_workers: å¹¶å‘çº¿ç¨‹æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            include_buffer_days: ç¼“å†²å¤©æ•°
        """
        try:
            # éªŒè¯è·¯å¾„
            universe_file_obj = self._validate_and_prepare_path(universe_file, is_file=True)
            db_file_path = self._validate_and_prepare_path(db_path, is_file=True)

            # data_pathæ˜¯å¯é€‰çš„ï¼Œå¦‚æœæä¾›åˆ™éªŒè¯
            data_path_obj = None
            if data_path:
                data_path_obj = self._validate_and_prepare_path(data_path, is_file=False)

            # æ£€æŸ¥universeæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not universe_file_obj.exists():
                raise FileNotFoundError(f"Universeæ–‡ä»¶ä¸å­˜åœ¨: {universe_file_obj}")

            # åŠ è½½universeå®šä¹‰
            universe_def = UniverseDefinition.load_from_file(universe_file_obj)

            logger.info("ğŸ“Š æŒ‰å‘¨æœŸä¸‹è½½æ•°æ®:")
            logger.info(f"   - æ€»å¿«ç…§æ•°: {len(universe_def.snapshots)}")
            logger.info(f"   - æ•°æ®é¢‘ç‡: {interval.value}")
            logger.info(f"   - å¹¶å‘çº¿ç¨‹: {max_workers}")
            logger.info(f"   - æ•°æ®åº“è·¯å¾„: {db_file_path}")
            if data_path_obj:
                logger.info(f"   - æ•°æ®æ–‡ä»¶è·¯å¾„: {data_path_obj}")

            # ä¸ºæ¯ä¸ªå‘¨æœŸå•ç‹¬ä¸‹è½½æ•°æ®
            for i, snapshot in enumerate(universe_def.snapshots):
                logger.info(
                    f"ğŸ“… å¤„ç†å¿«ç…§ {i + 1}/{len(universe_def.snapshots)}: {snapshot.effective_date}"
                )

                # è®¡ç®—ä¸‹è½½æ—¶é—´èŒƒå›´
                start_date = pd.to_datetime(snapshot.period_start_date) - timedelta(
                    days=include_buffer_days
                )
                end_date = pd.to_datetime(snapshot.period_end_date) + timedelta(
                    days=include_buffer_days
                )

                logger.info(f"   - äº¤æ˜“å¯¹æ•°é‡: {len(snapshot.symbols)}")
                logger.info(
                    f"   - æ•°æ®æœŸé—´: {start_date.strftime('%Y-%m-%d')} åˆ° "
                    f"{end_date.strftime('%Y-%m-%d')}"
                )

                # ä¸‹è½½è¯¥å‘¨æœŸçš„æ•°æ®
                self.get_perpetual_data(
                    symbols=snapshot.symbols,
                    start_time=start_date.strftime("%Y-%m-%d"),
                    end_time=end_date.strftime("%Y-%m-%d"),
                    db_path=db_file_path,
                    interval=interval,
                    max_workers=max_workers,
                    max_retries=max_retries,
                )

                logger.info(f"   âœ… å¿«ç…§ {snapshot.effective_date} ä¸‹è½½å®Œæˆ")

            logger.info("ğŸ‰ æ‰€æœ‰universeæ•°æ®ä¸‹è½½å®Œæˆ!")
            logger.info(f"ğŸ“ æ•°æ®å·²ä¿å­˜åˆ°: {db_file_path}")

        except Exception as e:
            logger.error(f"[red]æŒ‰å‘¨æœŸä¸‹è½½universeæ•°æ®å¤±è´¥: {e}[/red]")
            raise MarketDataFetchError(f"æŒ‰å‘¨æœŸä¸‹è½½universeæ•°æ®å¤±è´¥: {e}") from e
