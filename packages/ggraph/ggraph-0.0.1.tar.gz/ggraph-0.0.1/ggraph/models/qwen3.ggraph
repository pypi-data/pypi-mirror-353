model Qwen3 {
    tensors {
        input_tokens: I32, [params.batch.n_tokens], is_input
        input_positions: I32, [params.batch.n_tokens], is_input
        kq_mask: F32, [params.context.n_ctx, params.batch.n_tokens], is_input
        k_cache.%d: F16, [params.context.n_ctx * params.model.n_embd_k_gqa], is_state
        v_cache.%d: F16, [params.context.n_ctx * params.model.n_embd_v_gqa], is_state
    }

    graph {
        layer_input = get_rows(token_embd.weight, input_tokens)
        repeat cur_layer = params.model.n_layer {
        # repeat cur_layer = 1 {
            self_attention_input = layer_input

            # layer norm
            cur = mul(rms_norm(layer_input, params.model.f_norm_rms_eps), blk.%d.attn_norm.weight)

            # self attention

            # project q, k, and v
            qCur = reshape_3d(mul_mat(blk.%d.attn_q.weight, cur), params.model.n_embd_head_v, params.model.n_head, params.batch.n_tokens)
            kCur = reshape_3d(mul_mat(blk.%d.attn_k.weight, cur), params.model.n_embd_head_v, params.model.n_head_kv, params.batch.n_tokens)
            vCur = reshape_3d(mul_mat(blk.%d.attn_v.weight, cur), params.model.n_embd_head_v, params.model.n_head_kv, params.batch.n_tokens)

            # norm
            qCur = mul(rms_norm(qCur, params.model.f_norm_rms_eps), blk.%d.attn_q_norm.weight)
            kCur = mul(rms_norm(kCur, params.model.f_norm_rms_eps), blk.%d.attn_k_norm.weight)
            
            # apply RoPE
            qCur = rope(qCur, input_positions, params.model.n_rot, 2, params.model.n_ctx, params.context.rope_freq_base, params.context.rope_freq_scale, params.context.yarn_ext_factor, params.context.yarn_attn_factor, params.context.yarn_beta_fast, params.context.yarn_beta_slow)
            kCur = rope(kCur, input_positions, params.model.n_rot, 2, params.model.n_ctx, params.context.rope_freq_base, params.context.rope_freq_scale, params.context.yarn_ext_factor, params.context.yarn_attn_factor, params.context.yarn_beta_fast, params.context.yarn_beta_slow)

            # store roped k/v into k_cache/v_cache using token_offset
            kView = view_1d(k_cache.%d, 
                params.batch.n_tokens * params.model.n_embd_k_gqa,
                row_size(k_cache.%d, params.model.n_embd_k_gqa) * params.batch.kv_output_pos
            )
            cpy(kCur, kView)

            vCur = transpose(reshape_2d(vCur, params.model.n_embd_v_gqa, params.batch.n_tokens))
            vView = view_2d(v_cache.%d,
                params.batch.n_tokens, params.model.n_embd_v_gqa,
                params.context.n_ctx * element_size(v_cache.%d),
                params.batch.kv_output_pos * element_size(v_cache.%d)
            )
            cpy(vCur, vView)

            # extract usable window from kv cache for attention
            k = view_3d(k_cache.%d, 
                params.model.n_embd_head_k, params.model.n_head_kv, params.context.n_ctx,
                row_size(k_cache.%d, params.model.n_embd_head_k),
                row_size(k_cache.%d, params.model.n_embd_k_gqa),
                0
            )
            v = view_3d(v_cache.%d,
                params.context.n_ctx, params.model.n_head_kv, params.model.n_embd_head_v,
                row_size(v_cache.%d, params.model.n_embd_head_v) * params.context.n_ctx,
                row_size(v_cache.%d, params.context.n_ctx),
                0
            )
            
            k = permute(k, 0, 2, 1, 3)
            q = permute(qCur, 0, 2, 1, 3)
            v = permute(v, 0, 2, 1, 3)

            # calculate attention scores
            kq = mul_mat(k, q)
            kq = soft_max(kq, kq_mask, 1.0/sqrt(params.model.n_embd_head_v))

            kqv = mul_mat(v, kq)
            kqv = permute(kqv, 0, 2, 1, 3)
            kqv = cont_2d(kqv, params.model.n_embd_head_v * params.model.n_head, params.batch.n_tokens)

            attention = mul_mat(blk.%d.attn_output.weight, kqv)

            # fully connected
            ffn_inp = add(self_attention_input, attention)

            # ffn norm
            cur = mul(rms_norm(ffn_inp, params.model.f_norm_rms_eps), blk.%d.ffn_norm.weight)

            # up * silu(gate) * down + residual
            tmp = mul_mat(blk.%d.ffn_up.weight, cur)
            cur = mul_mat(blk.%d.ffn_gate.weight, cur)
            cur = silu(cur)
            cur = mul(cur, tmp)
            cur = mul_mat(blk.%d.ffn_down.weight, cur)
            layer_input = add(ffn_inp, cur)
        }

        # do final norm
        cur = mul(rms_norm(layer_input, params.model.f_norm_rms_eps), output_norm.weight)

        # lm_head
        output = mul_mat(output.weight, cur)
    }
}
