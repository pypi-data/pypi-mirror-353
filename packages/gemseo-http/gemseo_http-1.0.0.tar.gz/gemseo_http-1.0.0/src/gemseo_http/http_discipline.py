# Copyright 2021 IRT Saint ExupÃ©ry, https://www.irt-saintexupery.com
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License version 3 as published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with this program; if not, write to the Free Software Foundation,
# Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

"""A discipline to execute remote GEMSEO processes through HTTP."""

from __future__ import annotations

import time
from collections.abc import Iterable
from collections.abc import Mapping
from collections.abc import Sequence
from hashlib import sha256
from logging import getLogger
from pathlib import Path
from typing import TYPE_CHECKING
from typing import Any

from gemseo.core.discipline import MDODiscipline
from gemseo.utils.constants import READ_ONLY_EMPTY_DICT
from gemseo.utils.directory_creator import DirectoryCreator
from gemseo.utils.directory_creator import DirectoryNamingMethod
from httpx import Client

from _gemseo_http_common.constants import ExecutionMode as _ExecutionMode
from _gemseo_http_common.data_conversion import convert_dict_array_to_list
from _gemseo_http_common.data_conversion import convert_dict_list_to_array
from _gemseo_http_common.database import File
from _gemseo_http_common.database import Job
from _gemseo_http_common.database import JobStatus
from _gemseo_http_common.models import DisciplineOptions
from _gemseo_http_common.models import ExecuteInput
from gemseo_http._data_conversion import convert_dict_of_dict_list_to_array
from gemseo_http._files import compute_sha256sum
from gemseo_http._files import flatten_file_paths

if TYPE_CHECKING:
    from numpy.typing import NDArray

LOGGER = getLogger(__name__)


class HTTPDiscipline(MDODiscipline):
    """A discipline to execute a remote GEMSEO process exposed as an HTTP service."""

    ExecutionMode = _ExecutionMode
    """The execution mode."""

    __is_asynchronous: bool
    """Whether the execution mode is asynchronous."""

    __discipline_options: dict[str, Any]
    """The arguments passed to the discipline constructor."""

    __domain_port: str
    """The domain port of the service."""

    __file_paths_to_upload: tuple[Path, ...]
    """The paths of the files to upload before execution."""

    __file_paths_to_download: tuple[Path, ...]
    """The paths of the files to download after execution."""

    __linearize_after_execution: bool
    """Whether to linearize after execute."""

    __linearize_options: dict[str, Any]
    """The arguments passed to linearize."""

    __use_long_polling: bool
    """Whether to use long polling in asynchronous mode."""

    __polling_wait_time: float
    """The wait time in case of polling"""

    __execution_timeout: int
    """The maximum execution time in seconds."""

    __chunk_size: int
    """The size of the chunks to upload the files."""

    def __init__(
        self,
        name: str,
        class_name: str,
        url: str,
        port: int = 443,
        user: str = "",
        password: str = "",
        token: str = "",
        discipline_options: dict[str, Any] = READ_ONLY_EMPTY_DICT,
        is_asynchronous: bool = False,
        root_path: str | Path = "",
        directory_naming_method: DirectoryNamingMethod = DirectoryNamingMethod.NUMBERED,
        inputs_to_upload: Iterable[str] = (),
        outputs_to_download: Iterable[str] = (),
        file_paths_to_upload: Iterable[str | Path] = (),
        file_paths_to_download: Iterable[str | Path] = (),
        httpx_client_options: dict[str, Any] = READ_ONLY_EMPTY_DICT,
        use_long_polling: bool = True,
        polling_wait_time: float = 10.0,
        linearize_after_execution: bool = False,
        linearize_options: dict[str, Any] = READ_ONLY_EMPTY_DICT,
        http_client: Client = None,
        execution_timeout: int = 3600 * 5,
        chunk_size: int = 1024 * 1000,
    ) -> None:
        """
        Args:
            class_name: The name of the remote discipline class.
            url: The URL of the service.
            port: The port of the service.
            user: The user's name, only used if token is not set.
            password: The user's password, only used if token is not set.
            token: An access token, if any.
            discipline_options: The arguments passed to the discipline constructor.
            is_asynchronous: Whether to perform the remote computations
                in background in an async manner.
            root_path: The path to the root directory.
            directory_naming_method: The method used to name the created directories.
            inputs_to_upload: The names of the discipline inputs that correspond
                to files that must be uploaded before execution.
            outputs_to_download: The names of the discipline outputs that correspond
                to files that must be downloaded after execution.
            file_paths_to_upload: The paths of the files to upload before execution.
            file_paths_to_download: The paths of the files to download after execution.
            httpx_client_options: The arguments passed to the httpx client constructor.
            use_long_polling: Whether to use long polling in asynchronous mode.
            polling_wait_time: The wait time in case of polling.
            linearize_after_execution: Whether to linearize after execute.
            linearize_options: The arguments passed to linearize.
            http_client: A HTTP client that can be injected
             (for testing purposes or custom network configuration).
            execution_timeout: The maximum execution time in seconds.
            chunk_size: The size of the chunks to upload the files.
        """  # noqa: D205, D212, D415
        self.__is_asynchronous = is_asynchronous
        self.__linearize_after_execution = linearize_after_execution
        self.__linearize_options = linearize_options

        self._http_client = http_client or Client(**httpx_client_options)

        self.__discipline_options = discipline_options

        self.__domain_port = f"{url}:{port}"
        self.__execute_endpoint = f"/v1/discipline/{class_name}/execute"

        self.__inputs_to_upload = inputs_to_upload
        self.__outputs_to_download = outputs_to_download

        self.__file_paths_to_upload = tuple(map(Path, file_paths_to_upload))
        self.__file_paths_to_download = tuple(map(Path, file_paths_to_download))

        self.__directory_creator = DirectoryCreator(root_path, directory_naming_method)

        if not token:
            token = self.__authenticate(user, password)

        self._http_client.headers.update({"Authorization": f"Bearer {token}"})

        self.__use_long_polling = use_long_polling
        self.__polling_wait_time = polling_wait_time

        self.__execution_timeout = execution_timeout
        self.__chunk_size = chunk_size

        super().__init__(name=name)
        self.input_grammar.update_from_schema(
            self.__get_grammar_schema(class_name, "inputs")
        )
        self.output_grammar.update_from_schema(
            self.__get_grammar_schema(class_name, "outputs")
        )

        # Input variables storing the hash of the input files
        # self.input_grammar.update_from_types({"files_hash": str})
        # self.default_inputs = {"files_hash": ""}

    def __authenticate(self, user: str, password: str) -> str:
        """Authenticate the user.

        Args:
            user: The user.
            password: The password.

        Returns:
            The token.
        """
        data = {"username": user, "password": password}
        headers = {"Content-Type": "application/x-www-form-urlencoded"}
        request = self._http_client.post(
            f"{self.__domain_port}/token",
            data=data,
            headers=headers,
        )
        if request.status_code != 200:
            msg = (
                "Cannot obtain a token with the current login/password."
                f"Request ended with {request.status_code}."
            )
            raise ConnectionError(msg)
        token = request.json()["token"]
        LOGGER.debug("Authentication successful.")
        LOGGER.debug("Access token: %s", token)
        return token

    def __get_grammar_schema(
        self,
        class_name: str,
        schema_prefix: str,
    ) -> dict[str, Any]:
        """Return the schema for a JSON grammar.

        Args:
            class_name: The name of the remote discipline class.
            schema_prefix: The prefix for the schema endpoint.

        Returns:
            The schema.

        Raises:
            RuntimeError: The request has failed with a given error.
        """
        endpoint = (
            f"{self.__domain_port}/v1/discipline/{class_name}/{schema_prefix}_schema"
        )
        request = self._http_client.post(
            endpoint,
            json=DisciplineOptions(
                discipline_options=self.__discipline_options
            ).model_dump(),
        )
        if request.status_code != 200:
            msg = (
                f"Request {endpoint} failed with error code {request.status_code}:"
                f" the discipline {class_name} does not exist distantly."
                f" Check you distant GEMSEO installation."
            )
            raise RuntimeError(msg)

        schema = request.json()
        LOGGER.debug("Schema for %s: %s", schema_prefix, schema)
        return schema

    def __upload_files(self) -> list[File]:
        """Upload all the files.

        Returns:
            The schemas of the uploaded files.
        """
        file_paths = self.__file_paths_to_upload + tuple(
            Path(self.local_data[key]) for key in self.__inputs_to_upload
        )

        LOGGER.debug("Uploading files to the server...")

        endpoint = f"{self.__domain_port}/v1/file/upload"
        remote_file_schemas = []

        for file_path in file_paths:
            nb_chunks, remaining_size = divmod(
                file_path.stat().st_size, self.__chunk_size
            )
            chunk_sizes = [self.__chunk_size] * nb_chunks + [remaining_size]
            nb_chunks += 1
            with file_path.open("rb") as file_handler:
                # TODO: handle empty files.
                for i, chunk_size in enumerate(chunk_sizes):
                    data = {"total_chunks": nb_chunks, "chunk_number": i}
                    files = {"files": (file_path.name, file_handler.read(chunk_size))}
                    req = self._http_client.post(endpoint, files=files, data=data)
                    if req.status_code != 200:
                        msg = f"Failed to upload the file: {req.status_code}."
                        raise RuntimeError(msg)
                remote_file_schema = File(**req.json()[0])
                remote_file_schemas.append(remote_file_schema)
                LOGGER.debug("File %s uploaded.", file_path)
                LOGGER.debug("File %s details: %s", file_path, remote_file_schema)

        self.__check_file_hashes(file_paths, remote_file_schemas)

        return remote_file_schemas

    def __check_file_hashes(
        self,
        file_paths: Sequence[str | Path],
        remote_file_schemas: Sequence[File],
    ) -> None:
        """Check the file hashes of local files against provided remote file schemas.

        Args:
            file_paths : The file paths to the local files whose hashes
                need to be validated.
            remote_file_schemas: The file schemas containing the expected
                SHA-256 hashes against which the local file hashes are compared.

        Raises:
            ValueError: If a file's calculated hash does not match the expected hash.
        """
        _, file_hashes = self.__get_file_hashes(file_paths)
        for hash_, schema, path in zip(file_hashes, remote_file_schemas, file_paths):
            if schema.sha256sum != hash_:
                msg = (
                    "File hash mismatch for %s: %s != %s",
                    path,
                    schema.sha256sum,
                    hash_,
                )
                raise ValueError(msg)
        LOGGER.debug("File hashes OK.")

    def __download_files(self, job_id: int) -> None:
        """Download the files of a job.

        Args:
            job_id: The job id.

        Raises:
            ValueError: If a file bound to a variable is not correct.
        """
        if not (self.__outputs_to_download or self.__file_paths_to_download):
            return

        LOGGER.debug("Downloading the files...")

        directory = self.__directory_creator.create()

        output_paths_to_download = []
        for name in self.__outputs_to_download:
            value = self.local_data[name]
            if isinstance(value, str):
                path = Path(value)
                output_paths_to_download.append(path)
                new_value = str(directory / path)
            elif isinstance(value, Iterable):
                output_paths_to_download += map(Path, value)
                new_value = [str(directory / Path(val)) for val in value]
            else:
                msg = f"Wrong type for the variable holding a file: {value}"
                raise TypeError(msg)
            self.local_data[name] = new_value

        # Retrieve all the remote files for the current job.
        job_url = f"{self.__domain_port}/v1/job/{job_id}"
        endpoint = f"{job_url}/files"
        r = self._http_client.get(endpoint)
        remote_files_and_directories = r.json()

        # Transfer output names may contain directory or subdirectories.
        # Walk these directories and detect all the files inside them to be downloaded.
        file_paths_to_download = []
        for path in self.__file_paths_to_download:
            file_paths_to_download += flatten_file_paths(
                path, remote_files_and_directories
            )

        for file_path in set(file_paths_to_download + output_paths_to_download):
            endpoint = f"{job_url}/files/download?filename={file_path}"
            with self._http_client.stream("GET", endpoint) as r:
                r.raise_for_status()
                complete_path = Path(directory) / Path(file_path)
                parent = complete_path.parent
                parent.mkdir(parents=True, exist_ok=True)
                with complete_path.open("wb") as f:
                    for chunk in r.iter_bytes():
                        f.write(chunk)
                LOGGER.debug(
                    "File %s downloaded to the full path %s.", file_path, complete_path
                )

    @staticmethod
    def __get_file_hashes(file_paths: Iterable[Path]) -> tuple[str, list[str]]:
        """Compute the hash keys for the files.

        Args:
            file_paths: The file paths.

        Returns:
            The global hash and the file's hashes.
        """
        file_hashes = []
        global_hash = sha256()
        for file_path in file_paths:
            file_hash = compute_sha256sum(file_path)
            file_hashes.append(file_hash)
            global_hash.update(file_hash.encode())
        LOGGER.debug("Files hash keys: %s", file_hashes)
        LOGGER.debug("Global hash key: %s", global_hash)
        return global_hash.hexdigest(), file_hashes

    def __execute_on_remote(
        self,
        input_files: list[File],
        input_data: Mapping[str, Any],
    ) -> Job:
        """Execute the discipline remotely.

        Args:
            input_files: The input file schemas.
            input_data: The input data.

        Returns:
            The request job.
        """
        execute_inputs = ExecuteInput(
            name=self.name,
            discipline_options=self.__discipline_options,
            input_files=input_files,
            input_data=input_data,
            linearize=self.__linearize_after_execution,
            linearize_options=self.__linearize_options,
            linearization_mode=self._linearization_mode,
            differentiated_inputs=self._differentiated_inputs,
            differentiated_outputs=self._differentiated_outputs,
        )

        execution_mode = (
            self.ExecutionMode.asynchronous
            if self.__is_asynchronous
            else self.ExecutionMode.synchronous
        )
        request = self._http_client.post(
            f"{self.__domain_port}{self.__execute_endpoint}?execution_mode={execution_mode}",
            json=execute_inputs.model_dump(),
        )

        if request.status_code not in (200, 202):
            msg = (
                "Something went wrong while calling the server."
                " Please contact the server administrator."
            )
            raise RuntimeError(msg)

        job = Job(**request.json())

        LOGGER.debug("Job %s submitted to the server.", job.id)
        return job

    def __get_finished_job(self, job: Job) -> Job:
        """Return the output data from the remote.

        Args:
            job: The job.

        Returns:
            The output data.

        Raises:
            RuntimeError: If the timeout is reached.
        """
        if not self.__is_asynchronous:
            if job.job_status != JobStatus.finished:
                LOGGER.info("Job %s failed. Remote traceback below:", job.id)
                raise RuntimeError(job.traceback)
            return job

        initial_time = time.time()
        LOGGER.debug("Using long-polling: %s", self.__use_long_polling)
        LOGGER.debug(
            "The polling timeout is set to %s seconds.", self.__polling_wait_time
        )
        while True:
            job = self.__poll_job(job.id)

            if job.job_status in (JobStatus.finished.value, JobStatus.failed.value):
                break

            elapsed_time = time.time() - initial_time
            if elapsed_time > self.__execution_timeout:
                msg = f"Execution time exceeded {self.__execution_timeout}s."
                raise RuntimeError(msg)
            if not self.__use_long_polling:
                LOGGER.debug(
                    "Waiting %s seconds before polling.", self.__polling_wait_time
                )
                time.sleep(self.__polling_wait_time)

        if job.job_status == JobStatus.failed.value:
            LOGGER.info("Job %s failed. Remote traceback below:", job.id)
            raise RuntimeError(job.traceback)

        return job

    def __poll_job(self, job_id: int) -> Job:
        """Poll the server for the job status.

        Args:
             job_id: The job id.

        Returns:
            The job.
        """
        LOGGER.debug("Polling the server for job %s...", job_id)
        start_polling = time.time()
        endpoint = (
            f"{self.__domain_port}/v1/job/{job_id}?"
            f"long_polling={self.__use_long_polling}"
            f"&long_polling_timeout={self.__polling_wait_time}"
        )
        request = self._http_client.get(endpoint)
        elapsed_time = time.time() - start_polling
        LOGGER.debug("Polling the server done in %s", elapsed_time)
        return Job(**request.json())

    def __get_input_data(self) -> dict[str, Any]:
        """Return the input data.

        The local files are converted to their name only,
        as their absolute path is not relevant remotely.
        A corollary is that the remote discipline should access the files
        at their root directory level.

        Returns:
            The input data.
        """
        # Currently, Numpy arrays are converted to list, which is highly inefficient
        # Another serialization technique shall be considered instead.
        input_data = convert_dict_array_to_list(self.local_data)
        for name in self.__inputs_to_upload:
            input_data[name] = Path(input_data[name]).name
        return input_data

    def _run(self) -> None:
        input_files = self.__upload_files()
        input_data = self.__get_input_data()

        job = self.__execute_on_remote(input_files, input_data)

        job = self.__get_finished_job(job)
        self.local_data.update(convert_dict_list_to_array(job.output_data))
        self.__download_files(job.id)

        if self.__linearize_after_execution:
            self.jac = convert_dict_of_dict_list_to_array(job.jacobian_data)
            self._is_linearized = True

    def linearize(  # noqa: D102
        self,
        input_data: Mapping[str, Any] | None = None,
        compute_all_jacobians: bool = False,
        execute: bool = True,
    ) -> Mapping[str, Mapping[str, NDArray[float]]]:
        self.__linearize_options = {
            "compute_all_jacobians": compute_all_jacobians,
            "execute": execute,
        }
        self.__linearize_after_execution = True
        return super().linearize(input_data, compute_all_jacobians)
