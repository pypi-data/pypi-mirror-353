#!/usr/bin/env python3
"""
Complete demonstration of three field mapping approaches:
1. Full FieldMapping (comple        balanced_mapping = create_balanced_mapping(
            title_field='headline',
            content_fields=['body', 'summary'],
            id_field='permalink',
            category_field='tags',
            publish_time_field='published_at',
            content_separator=' | '
        )ionality)
2. Balanced FieldMapping (core + important fields)
3. Minimal FieldMapping (core fields only)
"""

import os
import sys
import json
import tempfile
from pathlib import Path

# Add the project root to Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from content_dedup import ContentDeduplicator
from content_dedup.config.field_mapping import create_custom_mapping
from content_dedup.config.field_mapping_balanced import create_balanced_mapping
from content_dedup.config.field_mapping_minimal import create_minimal_custom_mapping


def create_sample_data():
    """Create sample data for testing"""
    return [
        {
            "headline": "AIæŠ€è¡“é‡å¤§çªç ´",
            "body": "äººå·¥æ™ºæ…§é ˜åŸŸå–å¾—äº†å‰æ‰€æœªæœ‰çš„é‡å¤§é€²å±•",
            "summary": "ç ”ç©¶åœ˜éšŠé–‹ç™¼å‡ºé©å‘½æ€§çš„æ–°ç®—æ³•",
            "permalink": "https://tech.example.com/ai-breakthrough-1",
            "writer": "è³‡æ·±ç§‘æŠ€è¨˜è€…",
            "tags": ["AI", "äººå·¥æ™ºæ…§", "ç§‘æŠ€çªç ´"],
            "published_at": "2025/01/15 10:00:00",
            "images": ["ai_breakthrough.jpg", "algorithm_diagram.png"],
            "crawled_at": "2025/01/15 11:30:00"
        },
        {
            "headline": "æ©Ÿå™¨å­¸ç¿’æ–°é‡Œç¨‹ç¢‘",
            "body": "æ©Ÿå™¨å­¸ç¿’æŠ€è¡“é”åˆ°æ–°çš„é‡Œç¨‹ç¢‘",
            "summary": "ç§‘å­¸å®¶ç™¼ç¾äº†æ”¹é€²å­¸ç¿’æ•ˆç‡çš„æ–¹æ³•",
            "permalink": "https://tech.example.com/ml-milestone-1",
            "writer": "æŠ€è¡“å°ˆå®¶",
            "tags": ["æ©Ÿå™¨å­¸ç¿’", "ç§‘æŠ€", "å‰µæ–°"],
            "published_at": "2025/01/15 14:00:00",
            "images": ["ml_chart.jpg"],
            "crawled_at": "2025/01/15 15:00:00"
        },
        {
            "headline": "æ·±åº¦å­¸ç¿’æ¼”ç®—æ³•å„ªåŒ–",
            "body": "æ·±åº¦å­¸ç¿’æ¼”ç®—æ³•ç²å¾—é‡è¦å„ªåŒ–æ”¹é€²",
            "summary": "æ–°çš„è¨“ç·´æ–¹æ³•æå‡äº†æ¨¡å‹æ•ˆèƒ½",
            "permalink": "https://tech.example.com/dl-optimization-1",
            "writer": "AIç ”ç©¶å“¡",
            "tags": ["æ·±åº¦å­¸ç¿’", "æ¼”ç®—æ³•", "å„ªåŒ–"],
            "published_at": "2025/01/15 16:00:00",
            "images": ["neural_network.png", "performance_chart.jpg"],
            "crawled_at": "2025/01/15 17:00:00"
        }
    ]


def demo_mapping_comparison():
    """æ¯”è¼ƒä¸‰ç¨®æ˜ å°„æ–¹å¼çš„æ•ˆæœ"""
    print("ğŸ” æ¬„ä½æ˜ å°„æ–¹å¼æ¯”è¼ƒæ¼”ç¤º")
    print("=" * 60)
    
    # å‰µå»ºæ¸¬è©¦è³‡æ–™
    sample_data = create_sample_data()
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:
        for item in sample_data:
            json.dump(item, f, ensure_ascii=False)
            f.write('\n')
        temp_file = f.name
    
    try:
        # 1. å®Œæ•´ç‰ˆæ˜ å°„ (Full FieldMapping)
        print("\nğŸ“‹ 1. å®Œæ•´ç‰ˆæ˜ å°„ (Full FieldMapping)")
        print("-" * 40)
        
        full_mapping = create_custom_mapping(
            title_field='headline',
            content_fields=['body', 'summary'],
            id_field='permalink',
            category_field='tags',
            publish_time_field='published_at',
            content_separator=' | '
        )
        
        deduplicator_full = ContentDeduplicator(
            similarity_threshold=0.7,
            field_mapping=full_mapping
        )
        deduplicator_full.load_jsonl(temp_file)
        clusters_full = deduplicator_full.cluster_and_deduplicate()
        
        print(f"âœ… è¼‰å…¥ {len(deduplicator_full.content_items)} é …ç›®")
        print(f"ğŸ“Š ç”¢ç”Ÿ {len(clusters_full)} å€‹ç¾¤é›†")
        print("ğŸ“ ä»£è¡¨é …ç›®ç¯„ä¾‹ï¼š")
        if clusters_full:
            rep = clusters_full[0].representative
            print(f"   æ¨™é¡Œ: {rep.title}")
            print(f"   ä½œè€…: {rep.author}")
            print(f"   åˆ†é¡: {rep.category}")
            print(f"   åœ–ç‰‡: {len(rep.images)} å¼µ")
            print(f"   ç™¼å¸ƒæ™‚é–“: {rep.publish_time}")
        
        # 2. å¹³è¡¡ç‰ˆæ˜ å°„ (Balanced FieldMapping) 
        print("\nâš–ï¸ 2. å¹³è¡¡ç‰ˆæ˜ å°„ (Balanced FieldMapping)")
        print("-" * 40)
        
        balanced_mapping = create_balanced_mapping(
            title_field='headline',
            content_fields=['body', 'summary'],
            id_field='permalink',
            category_field='tags',
            publish_time_field='published_at',
            content_separator=' | '
        )
        
        deduplicator_balanced = ContentDeduplicator(
            similarity_threshold=0.7,
            field_mapping=balanced_mapping
        )
        deduplicator_balanced.load_jsonl(temp_file)
        clusters_balanced = deduplicator_balanced.cluster_and_deduplicate()
        
        print(f"âœ… è¼‰å…¥ {len(deduplicator_balanced.content_items)} é …ç›®")
        print(f"ğŸ“Š ç”¢ç”Ÿ {len(clusters_balanced)} å€‹ç¾¤é›†")
        print("ğŸ“ ä»£è¡¨é …ç›®ç¯„ä¾‹ï¼š")
        if clusters_balanced:
            rep = clusters_balanced[0].representative
            print(f"   æ¨™é¡Œ: {rep.title}")
            print(f"   ä½œè€…: {rep.author}")
            print(f"   åˆ†é¡: {rep.category} (é è¨­å€¼)")
            print(f"   åœ–ç‰‡: {len(rep.images)} å¼µ (é è¨­å€¼)")
            print(f"   ç™¼å¸ƒæ™‚é–“: {rep.publish_time}")
        
        # 3. ç²¾ç°¡ç‰ˆæ˜ å°„ (Minimal FieldMapping)
        print("\nğŸ¯ 3. ç²¾ç°¡ç‰ˆæ˜ å°„ (Minimal FieldMapping)")
        print("-" * 40)
        
        minimal_mapping = create_minimal_custom_mapping(
            title_field='headline',
            content_fields=['body', 'summary'],
            id_field='permalink',
            content_separator=' | '
        )
        
        deduplicator_minimal = ContentDeduplicator(
            similarity_threshold=0.7,
            field_mapping=minimal_mapping
        )
        deduplicator_minimal.load_jsonl(temp_file)
        clusters_minimal = deduplicator_minimal.cluster_and_deduplicate()
        
        print(f"âœ… è¼‰å…¥ {len(deduplicator_minimal.content_items)} é …ç›®")
        print(f"ğŸ“Š ç”¢ç”Ÿ {len(clusters_minimal)} å€‹ç¾¤é›†")
        print("ğŸ“ ä»£è¡¨é …ç›®ç¯„ä¾‹ï¼š")
        if clusters_minimal:
            rep = clusters_minimal[0].representative
            print(f"   æ¨™é¡Œ: {rep.title}")
            print(f"   ä½œè€…: {rep.author} (é è¨­å€¼)")
            print(f"   åˆ†é¡: {rep.category} (é è¨­å€¼)")
            print(f"   åœ–ç‰‡: {len(rep.images)} å¼µ (é è¨­å€¼)")
            print(f"   ç™¼å¸ƒæ™‚é–“: {rep.publish_time} (é è¨­å€¼)")
        
        # 4. å»é‡æ•ˆæœæ¯”è¼ƒ
        print("\nğŸ“ˆ 4. å»é‡æ•ˆæœæ¯”è¼ƒ")
        print("-" * 40)
        print(f"å®Œæ•´ç‰ˆç¾¤é›†æ•¸: {len(clusters_full)}")
        print(f"å¹³è¡¡ç‰ˆç¾¤é›†æ•¸: {len(clusters_balanced)}")
        print(f"ç²¾ç°¡ç‰ˆç¾¤é›†æ•¸: {len(clusters_minimal)}")
        
        # æ ¸å¿ƒå»é‡åŠŸèƒ½æ‡‰è©²ç›¸åŒ
        if len(clusters_full) == len(clusters_balanced) == len(clusters_minimal):
            print("âœ… æ‰€æœ‰ç‰ˆæœ¬çš„å»é‡æ•ˆæœä¸€è‡´")
        else:
            print("âš ï¸ ä¸åŒç‰ˆæœ¬çš„å»é‡æ•ˆæœæœ‰å·®ç•°")
        
        # 5. è¨˜æ†¶é«”å’Œé…ç½®è¤‡é›œåº¦æ¯”è¼ƒ
        print("\nğŸ’¾ 5. é…ç½®è¤‡é›œåº¦æ¯”è¼ƒ")
        print("-" * 40)
        
        # çµ±è¨ˆæ˜ å°„ç‰©ä»¶çš„å±¬æ€§æ•¸é‡
        full_attrs = len([attr for attr in dir(full_mapping) if not attr.startswith('_') and not callable(getattr(full_mapping, attr))])
        balanced_attrs = len([attr for attr in dir(balanced_mapping) if not attr.startswith('_') and not callable(getattr(balanced_mapping, attr))])
        minimal_attrs = len([attr for attr in dir(minimal_mapping) if not attr.startswith('_') and not callable(getattr(minimal_mapping, attr))])
        
        print(f"å®Œæ•´ç‰ˆé…ç½®å±¬æ€§: {full_attrs}")
        print(f"å¹³è¡¡ç‰ˆé…ç½®å±¬æ€§: {balanced_attrs}")
        print(f"ç²¾ç°¡ç‰ˆé…ç½®å±¬æ€§: {minimal_attrs}")
        
        print(f"\né…ç½®ç°¡åŒ–æ•ˆæœ:")
        print(f"  å¹³è¡¡ç‰ˆ vs å®Œæ•´ç‰ˆ: {(1 - balanced_attrs/full_attrs)*100:.1f}% æ¸›å°‘")
        print(f"  ç²¾ç°¡ç‰ˆ vs å®Œæ•´ç‰ˆ: {(1 - minimal_attrs/full_attrs)*100:.1f}% æ¸›å°‘")
        
    finally:
        Path(temp_file).unlink()


def demo_predefined_mapping_types():
    """æ¼”ç¤ºé å®šç¾©æ˜ å°„çš„ä¸‰ç¨®é¡å‹"""
    print("\n\nğŸ—‚ï¸ é å®šç¾©æ˜ å°„é¡å‹æ¼”ç¤º")
    print("=" * 60)
    
    # ä½¿ç”¨å­—ä¸²å‰ç¶´æŒ‡å®šæ˜ å°„é¡å‹
    mapping_examples = [
        ("news", "æ¨™æº–æ–°èæ ¼å¼"),
        ("balanced-news", "å¹³è¡¡ç‰ˆæ–°èæ ¼å¼"),
        ("minimal-news", "ç²¾ç°¡ç‰ˆæ–°èæ ¼å¼"),
        ("blog", "æ¨™æº–éƒ¨è½æ ¼æ ¼å¼"),
        ("balanced-blog", "å¹³è¡¡ç‰ˆéƒ¨è½æ ¼æ ¼å¼"),
        ("minimal-blog", "ç²¾ç°¡ç‰ˆéƒ¨è½æ ¼æ ¼å¼")
    ]
    
    for mapping_name, description in mapping_examples:
        print(f"\nğŸ“Œ {description} ({mapping_name})")
        try:
            deduplicator = ContentDeduplicator(field_mapping=mapping_name)
            mapping = deduplicator.field_mapping
            print(f"   é¡å‹: {type(mapping).__name__}")
            print(f"   æ¨™é¡Œæ¬„ä½: {mapping.title_field}")
            print(f"   å…§å®¹æ¬„ä½: {mapping.content_fields}")
            print(f"   IDæ¬„ä½: {mapping.id_field}")
            
            # æª¢æŸ¥æ˜¯å¦æœ‰é¡å¤–æ¬„ä½
            if hasattr(mapping, 'category_field'):
                print(f"   åˆ†é¡æ¬„ä½: {mapping.category_field}")
            if hasattr(mapping, 'publish_time_field'):
                print(f"   æ™‚é–“æ¬„ä½: {mapping.publish_time_field}")
                
        except ValueError as e:
            print(f"   âŒ éŒ¯èª¤: {e}")


def demo_performance_comparison():
    """æ•ˆèƒ½æ¯”è¼ƒæ¼”ç¤º"""
    print("\n\nâš¡ æ•ˆèƒ½æ¯”è¼ƒæ¼”ç¤º")
    print("=" * 60)
    
    import time
    
    # å‰µå»ºè¼ƒå¤§çš„æ¸¬è©¦è³‡æ–™é›†
    large_sample_data = []
    base_data = create_sample_data()
    
    for i in range(50):  # å‰µå»º 150 å€‹é …ç›®
        for j, item in enumerate(base_data):
            new_item = item.copy()
            new_item['headline'] = f"{item['headline']} - è®Šé«” {i}-{j}"
            new_item['permalink'] = f"{item['permalink']}-{i}-{j}"
            large_sample_data.append(new_item)
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:
        for item in large_sample_data:
            json.dump(item, f, ensure_ascii=False)
            f.write('\n')
        temp_file = f.name
    
    try:
        print(f"ğŸ“Š æ¸¬è©¦è³‡æ–™: {len(large_sample_data)} å€‹é …ç›®")
        
        # æ¸¬è©¦ä¸‰ç¨®æ˜ å°„çš„è™•ç†æ™‚é–“
        mapping_types = [
            ("å®Œæ•´ç‰ˆ", create_custom_mapping(
                title_field='headline',
                content_fields=['body', 'summary'],
                id_field='permalink',
                category_field='tags',
                publish_time_field='published_at'
            )),
            ("å¹³è¡¡ç‰ˆ", create_balanced_mapping(
                title_field='headline',
                content_fields=['body', 'summary'],
                id_field='permalink',
                category_field='tags',
                publish_time_field='published_at'
            )),
            ("ç²¾ç°¡ç‰ˆ", create_minimal_custom_mapping(
                title_field='headline',
                content_fields=['body', 'summary'],
                id_field='permalink'
            ))
        ]
        
        for name, mapping in mapping_types:
            start_time = time.time()
            
            deduplicator = ContentDeduplicator(
                similarity_threshold=0.8,
                field_mapping=mapping
            )
            deduplicator.load_jsonl(temp_file)
            clusters = deduplicator.cluster_and_deduplicate()
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            print(f"\n{name}æ˜ å°„:")
            print(f"   è™•ç†æ™‚é–“: {processing_time:.3f} ç§’")
            print(f"   ç¾¤é›†æ•¸é‡: {len(clusters)}")
            print(f"   è¨˜æ†¶é«”é …ç›®: {len(deduplicator.content_items)}")
        
    finally:
        Path(temp_file).unlink()


if __name__ == "__main__":
    demo_mapping_comparison()
    demo_predefined_mapping_types()
    demo_performance_comparison()
    
    print("\n\nğŸ¯ ç¸½çµå»ºè­°")
    print("=" * 60)
    print("ğŸ’¡ ç²¾ç°¡ç‰ˆ (MinimalFieldMapping):")
    print("   âœ… é©ç”¨æ–¼: æ•ˆèƒ½æ•æ„Ÿã€å¤§é‡è³‡æ–™è™•ç†ã€ç°¡å–®å»é‡éœ€æ±‚")
    print("   âœ… å„ªé»: é…ç½®ç°¡å–®ã€è¨˜æ†¶é«”ä½¿ç”¨å°‘ã€è™•ç†é€Ÿåº¦å¿«")
    print("   âš ï¸ é™åˆ¶: ä»£è¡¨é¸æ“‡å“è³ªè¼ƒä½ã€è³‡æ–™çµæ§‹è¼ƒç°¡å–®")
    
    print("\nâš–ï¸ å¹³è¡¡ç‰ˆ (BalancedFieldMapping):")
    print("   âœ… é©ç”¨æ–¼: ä¸€èˆ¬ä½¿ç”¨å ´æ™¯ã€éœ€è¦è¼ƒå¥½ä»£è¡¨é¸æ“‡")
    print("   âœ… å„ªé»: å¹³è¡¡åŠŸèƒ½æ€§èˆ‡ç°¡æ½”æ€§ã€æ›´å¥½çš„ä»£è¡¨é¸æ“‡")
    print("   âš ï¸ é™åˆ¶: æ¯”ç²¾ç°¡ç‰ˆç¨è¤‡é›œ")
    
    print("\nğŸ“‹ å®Œæ•´ç‰ˆ (FieldMapping):")
    print("   âœ… é©ç”¨æ–¼: è¤‡é›œéœ€æ±‚ã€å®Œæ•´è³‡æ–™çµæ§‹ã€å¤šæ¨£åŒ–è¼¸å‡º")
    print("   âœ… å„ªé»: åŠŸèƒ½å®Œæ•´ã€è³‡æ–™çµæ§‹è±å¯Œ")
    print("   âš ï¸ é™åˆ¶: é…ç½®è¤‡é›œã€è¨˜æ†¶é«”ä½¿ç”¨è¼ƒå¤š")
    
    print("\nğŸš€ å»ºè­°ä½¿ç”¨é †åº: ç²¾ç°¡ç‰ˆ â†’ å¹³è¡¡ç‰ˆ â†’ å®Œæ•´ç‰ˆ")
    print("   æ ¹æ“šå¯¦éš›éœ€æ±‚é¸æ“‡æœ€é©åˆçš„ç‰ˆæœ¬ï¼")
