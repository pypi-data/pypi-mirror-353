import os
import sys
import json
import glob
import shutil
import pickle
import requests
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
import typer
from langchain_community.vectorstores.faiss import FAISS
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

# Typer CLI instance
app = typer.Typer()

# Add src/ to import path
sys.path.append(str(Path(__file__).resolve().parent / "src"))

# Local modules
from processor import DocumentLoader, Chunker, Embedder, VectorStore, QueryEngine
from processor.project_vector import ProjectVectorManager

# === Constants ===
PROJECTS_DIR = ".rpl/projects"
CONFIG_PATH = ".rpl/config.json"


# === Helper: Load API Keys from Hosted Server ===
def get_keys_from_backend(project_id="demo-lab-1"):
    url = f"https://rpl-render.onrender.com/keys/{project_id}"
    res = requests.get(url)
    if res.status_code != 200:
        raise RuntimeError("‚ùå Failed to fetch API keys from backend.")
    keys = res.json()
    os.environ["OPENAI_API_KEY"] = keys["openai_key"]
    os.environ["GROQ_API_KEY"] = keys["groq_key"]
    print("üîê API keys set from backend")


# === Helper: Get Current Project Context ===
class ProjectContext:
    @staticmethod
    def current():
        if not os.path.exists(CONFIG_PATH):
            raise typer.Exit("‚ùå No project initialized. Run `rpl init <name>`.")
        with open(CONFIG_PATH) as f:
            return json.load(f)["current_project"]

    @staticmethod
    def set(project_name):
        os.makedirs(".rpl", exist_ok=True)
        with open(CONFIG_PATH, "w") as f:
            json.dump({"current_project": project_name}, f, indent=2)


# === Command: Init a New Project ===
@app.command()
def init(project_name: str):
    path = os.path.join(PROJECTS_DIR, project_name)
    os.makedirs(os.path.join(path, "uploads"), exist_ok=True)
    with open(os.path.join(path, "metadata.json"), "w") as f:
        json.dump({"project": project_name, "files": []}, f, indent=2)
    ProjectContext.set(project_name)
    print(f"‚úÖ Initialized project '{project_name}'.")


# === Command: Log a New Experiment (placeholder logic) ===
@app.command()
def log(
    title: str = typer.Option(..., help="Title of the experiment"),
    notes: str = typer.Option("", help="Detailed notes or description"),
    tags: str = typer.Option("", help="Comma-separated tags")
):
    """Log an experiment entry under the current project."""
    project = ProjectContext.current()
    path = os.path.join(PROJECTS_DIR, project)
    logs_dir = os.path.join(path, "logs")
    os.makedirs(logs_dir, exist_ok=True)

    entry = {
        "title": title,
        "notes": notes,
        "tags": [t.strip() for t in tags.split(",")] if tags else [],
        "timestamp": datetime.utcnow().isoformat()
    }

    # Save individual JSON file
    log_file = os.path.join(logs_dir, f"{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.json")
    with open(log_file, "w") as f:
        json.dump(entry, f, indent=2)

    # Append to logbook.json
    logbook_path = os.path.join(logs_dir, "logbook.json")
    if os.path.exists(logbook_path):
        with open(logbook_path, "r") as f:
            all_logs = json.load(f)
    else:
        all_logs = []

    all_logs.append(entry)
    with open(logbook_path, "w") as f:
        json.dump(all_logs, f, indent=2)

    print(f"üìù Logged experiment: {title}")
# === Command: List Projects ===
@app.command()
def list_projects():
    """List all initialized projects."""
    projects = os.listdir(PROJECTS_DIR)
    if not projects:
        print("‚ùå No projects found.")
    for project in projects:
        print(f"üìÅ {project}")


# === Command: Upload & Embed Documents ===
@app.command()
def upload(folder_path: str):
    project = ProjectContext.current()
    path = os.path.join(PROJECTS_DIR, project)
    uploads_dir = os.path.join(path, "uploads")
    meta_path = os.path.join(path, "metadata.json")

    files = [Path(f).name for f in glob.glob(folder_path + "/*")]
    vectorstore = None

    # Try to load existing index
    try:
        vectorstore = store_mgr.load(os.path.join(path, "faiss_index"), allow_dangerous_deserialization=True)
        print("üîÅ Loaded existing vectorstore.")
    except:
        print("üß† Creating new vectorstore.")

    for file in files:
        
        full_path = os.path.join(folder_path, file)
        if not os.path.exists(full_path):
            print(f"‚ö†Ô∏è Skipping missing file: {full_path}")
            continue

        print(f"üì• Uploading `{file}` to `{project}`...")

        # Load and chunk

        try:
            docs = doc_loader.load(full_path)
            chunks = chunker.chunk(docs)
            for chunk in chunks:
                chunk.metadata["source"] = file

            # Add to vectorstore
            if vectorstore:
                vectorstore.add_documents(chunks)
            else:
                vectorstore = store_mgr.create_index(chunks)

            # Copy to uploads/
            os.makedirs(uploads_dir, exist_ok=True)
            shutil.copy(full_path, os.path.join(uploads_dir, file))

            # Update metadata
            with open(meta_path, "r") as f:
                metadata = json.load(f)
            metadata["files"].append({
                "file_name": file,
                "uploaded_at": datetime.utcnow().isoformat()
            })
            with open(meta_path, "w") as f:
                json.dump(metadata, f, indent=2)

            print("‚úÖ File processed and saved.")
        except Exception as e:
            print(f"‚ùå Failed to load {full_path}: {e}")
            

    store_mgr.save(vectorstore, os.path.join(path, "faiss_index"))
    print("üíæ Index saved.")


# === Command: Query FAISS Only ===
@app.command()
def query(question: str):
    project = ProjectContext.current()
    path = os.path.join(PROJECTS_DIR, project)
    vectorstore = store_mgr.load(os.path.join(path, "faiss_index"), allow_dangerous_deserialization=True)
    engine = QueryEngine.QueryEngine(vectorstore)
    answer = engine.ask(question)
    print("ü§ñ Answer:", answer)


# === Command: Hybrid FAISS + BM25 Retrieval ===
@app.command()
def hybrid(query: str, k: int = 5):
    project = ProjectContext.current()
    path = os.path.join(PROJECTS_DIR, project)
    uploads_dir = os.path.join(path, "uploads")
    meta_path = os.path.join(path, "metadata.json")

    # Reload and chunk all uploaded docs
    all_docs = []
    with open(meta_path, "r") as f:
        meta = json.load(f)

    for entry in meta["files"]:
        file_path = os.path.join(uploads_dir, entry["file_name"])
        if not os.path.exists(file_path):
            print(f"‚ö†Ô∏è Skipping missing file: {file_path}")
            continue
        docs = doc_loader.load(file_path)
        chunks = chunker.chunk(docs)
        for chunk in chunks:
            chunk.metadata["source"] = entry["file_name"]
        all_docs.extend(chunks)

    if not all_docs:
        raise typer.Exit("‚ùå No documents loaded for hybrid search.")

    bm25_retriever = BM25Retriever.from_documents(all_docs)
    bm25_retriever.k = k

    vectorstore = store_mgr.load(os.path.join(path, "faiss_index"), allow_dangerous_deserialization=True)
    faiss_retriever = vectorstore.as_retriever(search_kwargs={"k": k})

    hybrid = EnsembleRetriever(
        retrievers=[faiss_retriever, bm25_retriever],
        weights=[0.7, 0.3]
    )

    results = hybrid.get_relevant_documents(query)
    print(f"\nüîç Results for: '{query}'\n")
    for i, doc in enumerate(results, 1):
        source = doc.metadata.get("source", "unknown")
        content = doc.page_content.strip().replace("\n", " ")[:300]
        print(f"[{i}] üìÑ {source}")
        print(f"    {content}\n")


# === Command: Push (Preview sync ‚Äî future API upload) ===
@app.command()
def push():
    project = ProjectContext.current()
    path = os.path.join(PROJECTS_DIR, project)
    meta_path = os.path.join(path, "metadata.json")

    with open(meta_path, "r") as f:
        meta = json.load(f)

    print(f"üì§ Preparing to push project: {project}")
    for file in meta["files"]:
        file_path = os.path.join(path, "uploads", file["file_name"])
        size = os.path.getsize(file_path) / 1024
        print(f" - {file['file_name']}: {size:.1f} KB")
    print("‚úÖ Push preview complete.")


# === Startup Setup ===
# Inject keys and initialize components
get_keys_from_backend()
doc_loader = DocumentLoader.DocumentLoader()
chunker = Chunker.TextChunker(chunk_size=500, chunk_overlap=50)
embedder = Embedder.Embedder(os.environ["OPENAI_API_KEY"])
store_mgr = VectorStore.VectorStoreManager(embedder.model)
