Metadata-Version: 2.3
Name: wronai
Version: 0.1.0
Summary: A Python package for AI model management and deployment
License: Apache-2.0
Author: WronAI Team
Author-email: info@softreck.dev
Requires-Python: >=3.8,<4.0
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Requires-Dist: click (>=8.1.7,<9.0.0)
Requires-Dist: requests (>=2.31.0,<3.0.0)
Requires-Dist: rich (>=13.7.0,<14.0.0)
Description-Content-Type: text/markdown

---
license: apache-2.0
base_model:
- mistralai/Mistral-7B-Instruct-v0.3
pipeline_tag: translation
tags:
- llm
- devops
- development
- polish
- english
- python
- iac
---
# üöÄ WronAI - W≈Çasny model jƒôzykowy po polsku

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

![WronAI Demo](img.png)

Kompletne narzƒôdzie do tworzenia, dostrajania i wdra≈ºania w≈Çasnych modeli jƒôzykowych opartych na Mistral 7B, z pe≈Çnym wsparciem dla jƒôzyka polskiego.

## üìã Spis tre≈õci
- [Szybki start](#-szybki-start)
- [Funkcje](#-funkcje)
- [Wymagania](#-wymagania)
- [Instalacja](#-instalacja)
- [U≈ºycie](#-u≈ºycie)
- [Struktura projektu](#-struktura-projektu)
- [Przyk≈Çady u≈ºycia](#-przyk≈Çady-u≈ºycia)
- [Wdra≈ºanie](#-wdra≈ºanie)
- [Licencja](#-licencja)

## üöÄ Szybki start

### Wymagania wstƒôpne
- Python 3.8+
- [Ollama](https://ollama.ai/) (zalecane)
- CUDA (opcjonalne, do akceleracji GPU)

### Instalacja
```bash
# 1. Sklonuj repozytorium
git clone https://github.com/wronai/llm-demo.git
cd llm-demo

# 2. Utw√≥rz i aktywuj ≈õrodowisko wirtualne
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# lub
.venv\Scripts\activate    # Windows

# 3. Zainstaluj zale≈ºno≈õci
pip install -r model_requirements.txt
```

### Uruchomienie demo
```bash
# Uruchom interfejs webowy
streamlit run app/main.py
```

## üíª Przyk≈Çady kodu

### Rozmowa z modelem
```python
import requests

response = requests.post(
    "http://localhost:11434/api/generate",
    json={
        "model": "wronai",
        "prompt": "Napisz kr√≥tki wiersz o sztucznej inteligencji"
    }
)
print(response.json()["response"])
```

### Integracja z Pythonem
```python
from transformers import pipeline

# Za≈Çaduj model
pipe = pipeline("text-generation", model="wronai")

# Wygeneruj tekst
result = pipe("Jakie sƒÖ zalety uczenia maszynowego?")
print(result[0]["generated_text"])
```

## ‚ú® Funkcje

- **Gotowy model WronAI** - Dzia≈Ça od razu po zainstalowaniu
- **Dostosowywanie** - Mo≈ºliwo≈õƒá dostrojenia pod w≈Çasne potrzeby
- **Interfejs webowy** - Prosty interfejs do rozmowy z modelem
- **Obs≈Çuga GPU** - Pe≈Çne wsparcie dla akceleracji sprzƒôtowej
- **Gotowy do produkcji** - ≈Åatwe wdro≈ºenie z Dockerem

## üõ†Ô∏è Wymagania

- System operacyjny: Linux, macOS lub Windows (z WSL2)
- RAM: Minimum 16GB (32GB zalecane)
- Dysk: Minimum 10GB wolnego miejsca
- Karta graficzna: NVIDIA z obs≈ÇugƒÖ CUDA (opcjonalnie)

## üìÅ Struktura projektu

```
llm-demo/
‚îú‚îÄ‚îÄ app/                    # Aplikacja Streamlit
‚îÇ   ‚îî‚îÄ‚îÄ main.py             # G≈Ç√≥wny plik aplikacji
‚îú‚îÄ‚îÄ models/                 # Modele i wagi
‚îú‚îÄ‚îÄ data/                   # Zbiory danych
‚îú‚îÄ‚îÄ scripts/                # Przydatne skrypty
‚îú‚îÄ‚îÄ docker-compose.yml       # Konfiguracja Docker Compose
‚îú‚îÄ‚îÄ Dockerfile              # Konfiguracja kontenera
‚îú‚îÄ‚îÄ requirements.txt         # Zale≈ºno≈õci Pythona
‚îú‚îÄ‚îÄ model_requirements.txt   # Zale≈ºno≈õci do modeli
‚îî‚îÄ‚îÄ README.md               # Ten plik
```

## üöÄ U≈ºycie

### Uruchomienie modelu WronAI

```bash
# Upewnij siƒô, ≈ºe Ollama jest uruchomiony
ollama serve &


# Uruchom model WronAI
ollama run wronai "Cze≈õƒá! Jak mogƒô Ci pom√≥c?"
```

### Dostosowywanie modelu

1. Przygotuj dane treningowe w formacie JSONL:
```json
{"instruction": "Napisz wiadomo≈õƒá powitalnƒÖ", "input": "", "output": "Witaj! Jak mogƒô Ci pom√≥c?"}
{"instruction": "Wyja≈õnij czym jest AI", "input": "", "output": "Sztuczna inteligencja to dziedzina informatyki..."}
```

2. Uruchom proces dostrajania:
```bash
python create_custom_model.py
```

3. Wybierz odpowiedniƒÖ opcjƒô z menu.

### Krok 2: Przygotowanie danych
```bash
python create_custom_model.py
# Wybierz opcjƒô 1: Stw√≥rz sample dataset
```

Przyk≈Çad danych treningowych:
```json
[
  {
    "instruction": "Jak nazywa siƒô stolica Polski?",
    "input": "",
    "output": "Stolica Polski to Warszawa."
  },
  {
    "instruction": "Wyja≈õnij czym jest sztuczna inteligencja",
    "input": "",
    "output": "Sztuczna inteligencja (AI) to dziedzina informatyki..."
  }
]
```

### Krok 3: Fine-tuning modelu
```bash
# Uruchom fine-tuning (wymaga GPU)
python create_custom_model.py
# Wybierz opcjƒô 2: Fine-tune model

# Lub pe≈Çny pipeline
python create_custom_model.py
# Wybierz opcjƒô 6: Pe≈Çny pipeline
```

**Optymalizacje dla RTX 3050:**
- 4-bit quantization
- LoRA (Low-Rank Adaptation)
- Batch size = 1
- Gradient accumulation = 4
- Mixed precision (FP16)

### Krok 4: Konwersja do GGUF
```bash
# Automatycznie generowany skrypt
./convert_to_gguf.sh
```

### Krok 5: Stworzenie modelu w Ollama
```bash
# Utw√≥rz Modelfile
python create_custom_model.py  # wybierz opcjƒô 4

# Stw√≥rz model w Ollama
ollama create wronai -f Modelfile

# Uruchom model
ollama run wronai
```

### Uruchamianie skryptu
Skrypt `create_custom_model.py` oferuje interaktywne menu z nastƒôpujƒÖcymi opcjami:

```bash
python create_custom_model.py
```

![img_2.png](img_2.png)

Dostƒôpne opcje:
1. Stw√≥rz przyk≈Çadowy dataset
2. Wykonaj fine-tuning modelu
3. Konwertuj model do formatu GGUF
4. Utw√≥rz Modelfile dla Ollama
5. Opublikuj model na Hugging Face
6. Wykonaj pe≈Çny pipeline (1-5)

### Wymagania wstƒôpne
- Python 3.8+
- PyTorch z obs≈ÇugƒÖ CUDA (zalecane)
- Biblioteki wymienione w `model_requirements.txt`
- Konto na [Hugging Face](https://huggingface.co/) (do publikacji modelu)

### RozwiƒÖzywanie problem√≥w

#### B≈ÇƒÖd sk≈Çadni w skrypcie
Je≈õli napotkasz b≈ÇƒÖd sk≈Çadni, upewnij siƒô, ≈ºe:
1. U≈ºywasz Pythona 3.8 lub nowszego
2. Wszystkie zale≈ºno≈õci sƒÖ zainstalowane
3. Plik nie zosta≈Ç uszkodzony podczas pobierania

#### Problemy z zale≈ºno≈õciami
```bash
# Utw√≥rz i aktywuj ≈õrodowisko wirtualne
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
.venv\Scripts\activate    # Windows

# Zainstaluj zale≈ºno≈õci
pip install -r model_requirements.txt
```

#### BrakujƒÖce uprawnienia
Je≈õli napotkasz problemy z uprawnieniami, spr√≥buj:
```bash
# Nadaj uprawnienia do wykonywania skrypt√≥w
chmod +x *.sh

# Uruchom z uprawnieniami administratora (je≈õli potrzebne)
sudo python create_custom_model.py
```

### Kontrybucja
Zapraszamy do zg≈Çaszania problem√≥w i propozycji zmian poprzez Issues i Pull Requests.

# Test modelu
```
ollama run wronai "Cze≈õƒá! Kim jeste≈õ?"
```

```
make model-run
```

![img_4.png](img_4.png)

```
### Krok 6: Publikacja modelu

#### **Opcja A: Ollama Registry**
```bash
# Push do Ollama Library
ollama push wronai

# Teraz ka≈ºdy mo≈ºe u≈ºyƒá:
ollama pull your-username/wronai
```

#### **Opcja B: Hugging Face Hub**
```bash
# Publikacja na HF
python publish_to_hf.py

# Model dostƒôpny na:
# https://huggingface.co/your-username/my-custom-mistral-7b
```

#### **Opcja C: Docker Registry**
```bash
# Spakuj do Docker image
docker build -t my-custom-llm .
docker tag my-custom-llm your-registry/my-custom-llm
docker push your-registry/my-custom-llm
```

## üéØ **CZƒò≈öƒÜ 3: Gotowe alternatywy (zero kodu)**

### **1. Najprostsze - Ollama**
```bash
# Instalacja
curl -fsSL https://ollama.ai/install.sh | sh

# Uruchomienie modelu
ollama run mistral:7b-instruct

# API automatycznie na localhost:11434
```

### **2. Hugging Face Inference API**
```python
import requests

headers = {"Authorization": "Bearer YOUR_HF_TOKEN"}
response = requests.post(
    "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1",
    headers=headers,
    json={"inputs": "Hello!"}
)
```

### **3. Groq (ultra szybkie)**
```python
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_GROQ_KEY",
    base_url="https://api.groq.com/openai/v1"
)

response = client.chat.completions.create(
    model="mistral-7b-instruct",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

### **4. Together.ai**
```python
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_TOGETHER_KEY",
    base_url="https://api.together.xyz/v1"
)

# Kompatybilne z OpenAI API
```

### **5. Modal.com (serverless GPU)**
```python
import modal

stub = modal.Stub("llm-api")

@stub.function(gpu="T4")
def generate(prompt: str):
    # Tw√≥j kod modelu
    return model.generate(prompt)

# Deploy jednƒÖ komendƒÖ
# modal deploy
```

## üöÄ Wdra≈ºanie

### Z Dockerem

```bash
# Zbuduj i uruchom kontenery
docker-compose up --build

# Tylko budowanie
# docker-compose build

# Uruchomienie w tle
# docker-compose up -d

# Wy≈õwietl logi
# docker-compose logs -f
```

### Konfiguracja ≈õrodowiska produkcyjnego

1. **Nginx jako reverse proxy**
   ```nginx
   server {
       listen 80;
       server_name twojadomena.pl;

       location / {
           proxy_pass http://localhost:8501;
           proxy_http_version 1.1;
           proxy_set_header Upgrade $http_upgrade;
           proxy_set_header Connection 'upgrade';
           proxy_set_header Host $host;
           proxy_cache_bypass $http_upgrade;
       }
   }
   ```

2. **Konfiguracja systemd**
   ```ini
   # /etc/systemd/system/wronai.service
   [Unit]
   Description=WronAI Service
   After=network.target

   [Service]
   User=www-data
   WorkingDirectory=/path/to/llm-demo
   ExecStart=/usr/bin/docker-compose up
   Restart=always

   [Install]
   WantedBy=multi-user.target
   ```

3. **Monitorowanie**
   - U≈ºyj `docker stats` do monitorowania zu≈ºycia zasob√≥w
   - Skonfiguruj alerty w przypadku awarii
   - Regularnie sprawdzaj logi aplikacji

## üõ†Ô∏è Utrzymanie i rozw√≥j

### Testowanie
```bash
# Uruchom testy jednostkowe
pytest tests/

# Sprawd≈∫ jako≈õƒá kodu
flake8 .
# Sprawd≈∫ bezpiecze≈Ñstwo zale≈ºno≈õci
safety check
```

### Wersjonowanie

U≈ºywamy [SemVer](https://semver.org/) do wersjonowania. Dostƒôpne wersje mo≈ºesz zobaczyƒá w [tagach repozytorium](https://github.com/wronai/llm-demo/tags).

## ü§ù Kontrybucja

1. Sforkuj repozytorium
2. Utw√≥rz nowy branch (`git checkout -b feature/nowa-funkcjonalnosc`)
3. Zatwierd≈∫ zmiany (`git commit -am 'Dodano nowƒÖ funkcjonalno≈õƒá'`)
4. Wypchnij zmiany (`git push origin feature/nowa-funkcjonalnosc`)
5. Otw√≥rz Pull Request

## üìú Licencja

Ten projekt jest dostƒôpny na licencji MIT - zobacz plik [LICENSE](LICENSE) aby poznaƒá szczeg√≥≈Çy.

## üìû Kontakt

- **Strona internetowa**: [wronai.pl](https://wronai.pl)
- **Email**: kontakt@wronai.pl
- **Twitter**: [@wronai](https://twitter.com/wronai)

---

<div align="center">
  <p>Made with ‚ù§Ô∏è by <a href="https://wronai.pl">WronAI Team</a></p>
  <p>Je≈õli podoba Ci siƒô ten projekt, daj nam ‚≠ê na <a href="https://github.com/wronai/llm-demo">GitHubie</a>!</p>
</div>

### **1. Streamlit (Python)**
```python
import streamlit as st

st.title("My LLM Chat")
prompt = st.text_input("Message:")
if st.button("Send"):
    response = generate(prompt)
    st.write(response)
```

### **2. Gradio (Python)**
```python
import gradio as gr

def chat(message, history):
    response = generate(message)
    history.append([message, response])
    return "", history

gr.ChatInterface(chat).launch()
```

### **3. Next.js + Vercel AI SDK**
```tsx
import { useChat } from 'ai/react'

export default function Chat() {
  const { messages, input, handleInputChange, handleSubmit } = useChat()
  
  return (
    <div>
      {messages.map(m => <div key={m.id}>{m.content}</div>)}
      <form onSubmit={handleSubmit}>
        <input value={input} onChange={handleInputChange} />
      </form>
    </div>
  )
}
```

## üéØ **CZƒò≈öƒÜ 5: Por√≥wnanie rozwiƒÖza≈Ñ**

| RozwiƒÖzanie | Setup Time | K√≥d | Hosting | GPU |
|-------------|------------|-----|---------|-----|
| **Ollama + Streamlit** | 2 min | 50 linijek | Local/Docker | Optional |
| **Hugging Face API** | 30 sec | 5 linijek | Cloud | No |
| **Groq API** | 1 min | 5 linijek | Cloud | No |
| **Modal.com** | 5 min | 20 linijek | Serverless | Auto |
| **Custom Fine-tuning** | 2 hours | 200 linijek | Self-hosted | Required |

## üõ†Ô∏è **Debugging & Tips**

### Typowe problemy
```bash
# Model nie ≈Çaduje siƒô
docker logs ollama-engine

# Brak GPU
docker run --rm --gpus all nvidia/cuda:11.8-base nvidia-smi

# Port zajƒôty
sudo netstat -tlnp | grep 11434

# Restart wszystkiego
docker compose down && docker compose up -d
```

### Optymalizacje RTX 3050
```python
# W fine-tuningu
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,        # 4-bit quantization
    torch_dtype=torch.float16 # Half precision
)

# Training args
TrainingArguments(
    per_device_train_batch_size=1,   # Ma≈Çy batch
    gradient_accumulation_steps=4,   # Gradients accumulation
    fp16=True                        # Mixed precision
)
```

### Monitoring zasob√≥w
```bash
# GPU monitoring
watch -n 1 nvidia-smi

# Container resources
docker stats

# Model memory usage
docker exec -it ollama-engine ollama ps
```

## üéØ **Nastƒôpne kroki**

### Dla nauki:
1. **Eksperymentuj z r√≥≈ºnymi modelami** - Llama, CodeLlama, Phi-3
2. **Testuj r√≥≈ºne techniki fine-tuningu** - LoRA, QLoRA, Full fine-tuning
3. **Dodaj RAG** - Retrieval Augmented Generation
4. **Stw√≥rz multi-agent system**

### Dla produkcji:
1. **Przejd≈∫ na managed service** - Groq, Together.ai
2. **Setup monitoring** - LangSmith, Weights & Biases
3. **Dodaj cache** - Redis dla odpowiedzi
4. **Implement rate limiting**

### Dla biznesu:
1. **Fine-tune na w≈Çasnych danych**
2. **Setup A/B testing** r√≥≈ºnych modeli
3. **Dodaj feedback loop** od u≈ºytkownik√≥w
4. **Monetize API**

## üéâ **Podsumowanie**

**Wybierz opcjƒô wed≈Çug potrzeb:**

- **Demo/nauka**: Ollama + Streamlit (to rozwiƒÖzanie)
- **Prototyp**: Hugging Face API + Gradio
- **MVP**: Groq API + Next.js
- **Produkcja**: Modal/RunPod + custom frontend
- **Enterprise**: Fine-tuned model + w≈Çasna infrastruktura

**Minimalne rozwiƒÖzanie = 5 plik√≥w, 50 linijek kodu, 2 minuty setup!**


