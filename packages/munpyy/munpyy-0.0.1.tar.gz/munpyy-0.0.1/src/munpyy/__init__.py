# -*- coding: utf-8 -*-
"""munpy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sVk5-t2B_H0ygrkdy2PboCiHkvjxDGv2

# Практика
"""

def p1():
    theory = """
## 1. Метод половинного деления (бисекции)

### Теоретическое описание:
Метод половинного деления — это простейший и самый надежный численный метод для нахождения корня нелинейного уравнения f(x) = 0.

**Основная идея:**
Метод основан на теореме о промежуточных значениях (теореме Больцано-Коши). Если непрерывная функция f(x) на отрезке [a, b] принимает на концах значения разных знаков (т.е. f(a) * f(b) < 0), то на этом отрезке существует как минимум один корень.

**Алгоритм:**
1.  **Начальный шаг:** Выбирается отрезок [a, b], на концах которого функция имеет разные знаки.
2.  **Итерация:** Отрезок делится пополам, вычисляется середина c = (a + b) / 2.
3.  **Проверка:**
    -   Если |f(c)| меньше заданной точности (tol), то c — искомый корень.
    -   Если знаки f(a) и f(c) совпадают, то корень находится на отрезке [c, b]. Новый отрезок для поиска становится [c, b].
    -   Если знаки f(b) и f(c) совпадают, то корень находится на отрезке [a, c]. Новый отрезок для поиска становится [a, c].
4.  **Повторение:** Процесс повторяется, сужая отрезок поиска вдвое на каждом шаге, до тех пор, пока не будет достигнута требуемая точность.

**Преимущества:**
-   Метод всегда сходится для любой непрерывной функции, если выполнены начальные условия.
-   Прост в реализации и очень надежен.

**Недостатки:**
-   Относительно медленная скорость сходимости (линейная).
-   Неприменим для отыскания корней четного порядка (например, в точке минимума/максимума, касающейся оси), когда график функции не пересекает ось.
"""

    code = """
### Код реализации:
```python
import numpy as np

def bisection(f, a, b, tol):
    # Проверка, что на концах отрезка функция имеет разные знаки
    if np.sign(f(a)) == np.sign(f(b)):
        raise Exception('Заданные a и b не локализуют корень, т.к. f(a) и f(b) одного знака.')

    # Итерационный процесс
    while (b - a) / 2.0 > tol:
        m = (a + b) / 2
        if f(m) == 0:
            return m  # Точный корень найден
        elif np.sign(f(a)) == np.sign(f(m)):
            a = m
        else:
            b = m

    return (a + b) / 2.0

# Пример использования:
# Найдём корень уравнения x^2 - 2 = 0 на отрезке [0, 2]
# f = lambda x: x**2 - 2
# root = bisection(f, 0, 2, 0.000001)
# print(f'Найденный корень: {root}')
# print(f'Значение функции в корне: {f(root)}')"""

    return theory + code

def p2():
    theory = """
## 2. Методы функциональной итерации (метод простых итераций)

### Теоретическое описание:
Этот метод является одним из самых фундаментальных для решения нелинейных уравнений. Он заключается в преобразовании исходного уравнения f(x) = 0 в эквивалентную форму x = φ(x).

**Основная идея:**
Если найдена такая функция φ(x), то корень исходного уравнения является её неподвижной точкой (т.е. точкой, где y = x пересекается с y = φ(x)). Решение ищется с помощью итерационного процесса:
`x_{k+1} = φ(x_k)`

**Условие сходимости:**
Метод сходится к единственному корню на отрезке [a, b], если выполнено **условие сжимающего отображения**:
`|φ'(x)| < 1` для всех x из отрезка [a, b].
Чем меньше значение |φ'(x)|, тем быстрее сходимость. Если |φ'(x)| > 1, метод расходится.

**Алгоритм:**
1.  Преобразовать уравнение f(x) = 0 к виду x = φ(x). Это можно сделать разными способами, например, `x = x - α*f(x)`, где α — параметр, который подбирается для выполнения условия сходимости.
2.  Выбрать начальное приближение `x0`.
3.  Выполнять итерации `x_{k+1} = φ(x_k)` до тех пор, пока `|x_{k+1} - x_k|` не станет меньше заданной точности.

**Преимущества:**
-   Концептуально прост и является основой для многих других методов.

**Недостатки:**
-   Сходимость не гарантирована и сильно зависит от выбора функции φ(x).
-   Скорость сходимости, как правило, линейная и может быть очень медленной.
"""
    code = """
### Код реализации:
```python
import numpy as np

def f_iteration(g, x0, n_iterations=100, tol=1e-5):
    '''
    Реализация метода простой итерации для уравнения x = g(x)

    Параметры:
        g (function): Функция, для которой ищется неподвижная точка.
        x0 (float): Начальное приближение.
        n_iterations (int): Максимальное количество итераций.
        tol (float): Точность для остановки.
    '''
    x = x0
    for n in range(n_iterations):
        x_new = g(x)
        if abs(x_new - x) < tol:
            return x_new
        x = x_new
    return x

# Пример использования:
# Решим уравнение x = cos(x), что эквивалентно f(x) = x - cos(x) = 0.
# Здесь φ(x) = cos(x). Проверим условие сходимости: |φ'(x)| = |-sin(x)| <= 1.
# Условие выполнено.
# g = lambda x: np.cos(x)
# root = f_iteration(g, x0=0.5)
# print(f'Найденный корень: {root}')"""
    return theory + code

### Метод хорд (секущих)

def p3():
    theory = """
## 3. Метод хорд (метод секущих)

### Теоретическое описание:
Метод хорд является усовершенствованием метода простых итераций и альтернативой методу Ньютона. Он не требует вычисления производной, что является его ключевым преимуществом.

**Основная идея:**
Вместо касательной, как в методе Ньютона, через две последние точки `(x_{k-1}, f(x_{k-1}))` и `(x_k, f(x_k))` проводится *секущая* (хорда). Следующее приближение `x_{k+1}` — это точка пересечения этой секущей с осью Ox.

Производная `f'(x_k)` из метода Ньютона аппроксимируется конечной разностью:
`f'(x_k) ≈ (f(x_k) - f(x_{k-1})) / (x_k - x_{k-1})`

Подставив это в формулу Ньютона, получаем итерационную формулу метода хорд:
`x_{k+1} = x_k - f(x_k) * (x_k - x_{k-1}) / (f(x_k) - f(x_{k-1}))`

**Алгоритм:**
1.  Выбрать два начальных приближения `x0` и `x1`.
2.  Выполнять итерации по приведенной выше формуле.
3.  Остановить процесс, когда `|x_{k+1} - x_k|` станет меньше заданной точности.

**Преимущества:**
-   Не требует вычисления аналитической производной.
-   Скорость сходимости (суперлинейная, ~1.618) выше, чем у метода простых итераций и метода половинного деления.

**Недостатки:**
-   Требует два начальных приближения.
-   Сходимость медленнее, чем у метода Ньютона (который имеет квадратичную сходимость).
-   Как и метод Ньютона, может расходиться, если начальные точки выбраны неудачно.
"""
    code = """
### Код реализации:
```python
def chord_method(f, x0, x1, tol=1e-5, n_iterations=100):
    '''
    Реализация метода хорд (секущих).

    Параметры:
        f (function): Функция, корень которой ищется.
        x0, x1 (float): Два начальных приближения.
        tol (float): Точность.
    '''
    for n in range(n_iterations):
        fx0 = f(x0)
        fx1 = f(x1)

        if abs(fx1 - fx0) < 1e-12: # Избегание деления на ноль
            break

        x_new = x1 - fx1 * (x1 - x0) / (fx1 - fx0)

        if abs(x_new - x1) < tol:
            return x_new

        x0 = x1
        x1 = x_new

    return x1

# Пример использования:
# Найдём корень уравнения x^2 - 2 = 0
# f = lambda x: x**2 - 2
# root = chord_method(f, 1, 2, tol=1e-7)
# print(f'Найденный корень: {root}')
# print(f'Значение функции в корне: {f(root)}')"""
    return theory + code

### Метод Ньютона (касательных)

def p4():
    theory = """
## 4. Метод Ньютона (метод касательных)

### Теоретическое описание:
Метод Ньютона — один из самых мощных и быстросходящихся итерационных методов для нахождения корня f(x) = 0.

**Основная идея:**
Метод основан на замене функции `f(x)` её линейной аппроксимацией — касательной, построенной в точке текущего приближения `x_k`. Следующее приближение `x_{k+1}` находится как точка пересечения этой касательной с осью Ox.

Формула выводится из разложения функции в ряд Тейлора в окрестности `x_k`:
`f(x) ≈ f(x_k) + f'(x_k) * (x - x_k)`

Полагая `x = x_{k+1}` и `f(x_{k+1}) = 0`, получаем итерационную формулу:
`x_{k+1} = x_k - f(x_k) / f'(x_k)`

**Алгоритм:**
1.  Выбрать начальное приближение `x0`.
2.  Найти аналитическое выражение для производной `f'(x)`.
3.  Выполнять итерации по формуле Ньютона до достижения необходимой точности.

**Преимущества:**
-   Очень быстрая (квадратичная) скорость сходимости вблизи корня. Это означает, что количество верных значащих цифр примерно удваивается на каждой итерации.

**Недостатки:**
-   Требует вычисления аналитической производной, что не всегда возможно или удобно.
-   Сходимость сильно зависит от выбора начального приближения `x0`. При неудачном выборе метод может расходиться.
-   Проблемы возникают, если производная `f'(x_k)` близка к нулю (касательная почти горизонтальна).
"""
    code = """
### Код реализации (рекурсивная версия):
```python
def newton_method(f, df, x0, tolerance=1e-7, max_iterations=100):
    '''
    Реализация метода Ньютона.

    Параметры:
        f (function): Функция, корень которой ищется.
        df (function): Производная функции f.
        x0 (float): Начальное приближение.
        tolerance (float): Точность.
    '''
    for i in range(max_iterations):
        fx = f(x0)
        if abs(fx) < tolerance:
            return x0

        dfx = df(x0)
        if dfx == 0:
            break # Деление на ноль

        x0 = x0 - fx / dfx

    return x0

# Пример использования:
# Найдём корень уравнения x**2 - 2 = 0
# f = lambda x: x**2 - 2
# f_prime = lambda x: 2*x
# root = newton_method(f, f_prime, 1.5, tolerance=1e-7)
# print(f'Найденный корень: {root}')
# print(f'Значение функции в корне: {f(root)}')"""
    return theory + code

def p5():
    theory = """
## 5. Модифицированный метод Ньютона

### Теоретическое описание:
Это вариация классического метода Ньютона, предназначенная для ситуаций, когда вычисление производной `f'(x)` на каждой итерации является вычислительно затратным.

**Основная идея:**
Вместо того чтобы пересчитывать значение производной `f'(x_k)` на каждом шаге `k`, мы вычисляем её только один раз в начальной точке `x0` и используем это значение на всех последующих итерациях.

Итерационная формула принимает вид:
`x_{k+1} = x_k - f(x_k) / f'(x0)`

**Геометрическая интерпретация:**
Касательные, которые мы проводим на каждом шаге, параллельны друг другу, так как их наклон определяется постоянной величиной `f'(x0)`.

**Преимущества:**
-   Не требует вычисления производной на каждой итерации, что экономит вычислительные ресурсы, если `f'(x)` — сложная функция.

**Недостатки:**
-   Скорость сходимости снижается с квадратичной (как у Ньютона) до линейной.
-   Сходимость по-прежнему зависит от выбора начальной точки `x0`.
-   Если производная сильно меняется на интервале поиска, метод может сходиться очень медленно или вовсе расходиться.
"""
    code = """
### Код реализации:
```python
def modified_newton_method(f, df, x0, tol=1e-5, n_iterations=100):
    '''
    Реализация модифицированного метода Ньютона.

    Параметры:
        f (function): Функция, корень которой ищется.
        df (function): Производная функции f.
        x0 (float): Начальное приближение.
        tol (float): Точность.
    '''
    df_x0 = df(x0) # Вычисляем производную только один раз
    if df_x0 == 0:
        raise ValueError("Производная в начальной точке равна нулю.")

    for i in range(n_iterations):
        fx0 = f(x0)
        if abs(fx0) < tol:
            return x0

        x_new = x0 - fx0 / df_x0
        x0 = x_new

    return x0

# Пример использования:
# f = lambda x: x**2 - 2
# f_prime = lambda x: 2*x
# root = modified_newton_method(f, f_prime, 1.5, tol=1e-7)
# print(f'Найденный корень: {root}')"""
    return theory + code

### Метод дихотомии

def p6():
    theory = """
## 6. Метод дихотомии для поиска экстремума

### Теоретическое описание:
Метод дихотомии (не путать с методом половинного деления для поиска корней) используется для нахождения минимума или максимума *унимодальной* функции на заданном отрезке [a, b]. Унимодальная функция — это функция, имеющая на отрезке только один локальный экстремум.

**Основная идея:**
На каждом шаге мы сужаем интервал, содержащий экстремум. Для этого внутри текущего интервала [a, b] выбираются две точки `x1` и `x2`, расположенные симметрично относительно его середины на небольшом расстоянии `ε`.

`x1 = (a + b) / 2 - ε`
`x2 = (a + b) / 2 + ε`

Далее сравниваются значения функции в этих точках, `f(x1)` и `f(x2)`.

**Алгоритм (для поиска минимума):**
1.  **Начальный шаг:** Задается начальный интервал [a, b], где предполагается наличие минимума, и малая величина `ε`.
2.  **Итерация:** Вычисляются `x1` и `x2` по формулам выше.
3.  **Проверка:**
    -   Если `f(x1) < f(x2)`, то минимум находится левее `x2`. Новый интервал поиска: `[a, x2]`.
    -   Если `f(x1) > f(x2)`, то минимум находится правее `x1`. Новый интервал поиска: `[x1, b]`.
    -   Если `f(x1) = f(x2)`, интервал можно сузить до `[x1, x2]`.
4.  **Повторение:** Процесс повторяется, пока длина интервала `(b - a)` не станет меньше заданной точности.

**Преимущества:**
-   Надежен и всегда сходится для унимодальных функций.
-   Не требует вычисления производных.

**Недостатки:**
-   Медленная сходимость.
-   Требует, чтобы функция была унимодальной на исследуемом интервале.
"""
    code = """
### Код реализации (поиск минимума):
```python
def dichotomy_method(f, a, b, tol=1e-5, epsilon=1e-7):
    '''
    Реализация метода дихотомии для поиска минимума.

    Параметры:
        f (function): Унимодальная функция, минимум которой ищется.
        a, b (float): Границы начального интервала.
        tol (float): Точность для остановки (длина интервала).
        epsilon (float): Малое смещение для тестовых точек.
    '''
    while (b - a) > tol:
        mid = (a + b) / 2
        x1 = mid - epsilon
        x2 = mid + epsilon

        if f(x1) < f(x2):
            b = x2
        else:
            a = x1

    return (a + b) / 2

# Пример использования:
# Найдем минимум функции f(x) = (x-2)^2 на отрезке [0, 5]
# f = lambda x: (x - 2)**2
# min_point = dichotomy_method(f, 0, 5)
# print(f'Точка минимума: {min_point}')"""
    return theory + code

def p7():
    theory = """
## 5. Модифицированный метод Ньютона для систем нелинейных уравнений

### Теоретическое описание:
Это вариация классического метода Ньютона для систем, предназначенная для ситуаций, когда вычисление матрицы Якоби `J(X)` на каждой итерации является вычислительно затратным.

**Основная идея:**
Вместо того чтобы пересчитывать матрицу Якоби `J(X_k)` на каждом шаге `k`, мы вычисляем её только один раз в начальной точке `X0`. Это значительно упрощает вычисления, так как сложная операция решения СЛАУ (например, через LU-разложение) выполняется для одной и той же матрицы на всех шагах.

Итерационный процесс включает решение СЛАУ:
`J(X0) * ΔX_k = -F(X_k)`
И обновление приближения:
`X_{k+1} = X_k + ΔX_k`

**Преимущества:**
-   Значительно снижает вычислительные затраты на каждой итерации по сравнению с классическим методом Ньютона.

**Недостатки:**
-   Скорость сходимости падает с квадратичной до линейной.
-   Сходимость, как и в классическом методе, зависит от качества начального приближения.
"""
    code = """
### Код реализации:
```python
import numpy as np

def _solve_gauss_manual(A, b):
    \"\"\"
    Решает СЛАУ Ax=b методом Гаусса.
    Это вспомогательная функция, не использующая np.linalg.solve.
    \"\"\"
    n = len(b)
    # Прямой ход
    for i in range(n):
        # Поиск главного элемента для устойчивости
        max_row = i
        for k in range(i + 1, n):
            if abs(A[k, i]) > abs(A[max_row, i]):
                max_row = k
        A[[i, max_row]] = A[[max_row, i]]
        b[[i, max_row]] = b[[max_row, i]]

        # Приведение к верхнетреугольному виду
        for k in range(i + 1, n):
            c = -A[k, i] / A[i, i]
            A[k, i] = 0
            for j in range(i + 1, n):
                A[k, j] += c * A[i, j]
            b[k] += c * b[i]

    # Обратный ход
    x = np.zeros(n)
    for i in range(n - 1, -1, -1):
        x[i] = (b[i] - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]
    return x

def modified_newton_system(f_funcs, j_funcs, x0, tol=1e-5, n_iterations=100):
    \"\"\"
    Реализация модифицированного метода Ньютона для СНУ с ручным решателем.
    \"\"\"
    x = np.array(x0, dtype=float)
    J0 = j_funcs(x) # Вычисляем Якобиан один раз

    for k in range(n_iterations):
        fx = f_funcs(x)

        # Решаем систему J0 * delta_x = -fx вручную
        delta_x = _solve_gauss_manual(J0.copy(), -fx)

        x = x + delta_x

        # Проверка сходимости по норме (максимальное абсолютное значение)
        if np.max(np.abs(delta_x)) < tol:
            return x

    return x

# Пример использования:
# def f_sys(x):
#     return np.array([x[0]**2 + x[1]**2 - 1.0, 5*x[0]**2 + 21*x[1]**2 - 9.0])
#
# def jacobian_sys(x):
#     return np.array([[2*x[0], 2*x[1]], [10*x[0], 42*x[1]]])
#
# x0 = np.array([1.0, 1.0])
# root = modified_newton_system(f_sys, jacobian_sys, x0)
# print(f'Найденное решение: {root}')"""
    return theory + code

def p8():
    theory = """
## 8. Метод Гаусса-Зейделя для систем нелинейных уравнений

### Теоретическое описание:
Метод Гаусса-Зейделя (или метод Зейделя) — это модификация метода простых итераций для СНУ, которая часто обеспечивает более быструю сходимость.

**Основная идея:**
Главное отличие от метода простых итераций заключается в том, что при вычислении `(k+1)`-го приближения для переменной `x_i` используются *уже вычисленные* на `(k+1)`-й итерации значения других переменных (`x_1, ..., x_{i-1}`).

Итерационный процесс для `i`-й компоненты вектора `X`:
`x_{i}^{(k+1)} = g_i(x_{1}^{(k+1)}, ..., x_{i-1}^{(k+1)}, x_{i}^{(k)}, ..., x_{n}^{(k)})`

**Условие сходимости:**
Условия сходимости аналогичны методу простых итераций и связаны со сжимающим отображением. На практике метод Зейделя часто сходится быстрее, если система обладает свойством диагонального преобладания.

**Алгоритм:**
1.  Преобразовать систему к виду `X = G(X)`.
2.  Выбрать начальное векторное приближение `X0`.
3.  На `(k+1)`-й итерации последовательно вычислять `x_i^{(k+1)}`, сразу используя новые значения в последующих вычислениях на этом же шаге.
4.  Процесс останавливается, когда норма разности между итерациями становится достаточно малой.
"""
    code = """
### Код реализации:
```python
import numpy as np

def seidel_method_system(g_funcs, x0, tol=1e-5, n_iterations=100):
    '''
    Реализация метода Зейделя для СНУ.

    Параметры:
        g_funcs (list of functions): Список функций [g1, g2, ...].
        x0 (np.array): Вектор начальных приближений.
    '''
    x = np.array(x0, dtype=float)

    for k in range(n_iterations):
        x_old = x.copy()
        for i in range(len(g_funcs)):
            # Передаем весь вектор x, но функция gi будет использовать его компоненты
            x[i] = g_funcs[i](*x)

        if np.linalg.norm(x - x_old) < tol:
            return x

    return x

# Пример использования:
# Решим систему:
# x1 = cos(x2)/6 + 1/6
# x2 = sqrt(x1^2 + sin(x1) + 1.06)/9 - 0.1
# g1 = lambda x1, x2: np.cos(x2) / 6.0 + 1.0 / 6.0
# g2 = lambda x1, x2: np.sqrt(x1**2 + np.sin(x2) + 1.06) / 9.0 - 0.1
#
# x0 = np.array([0.1, 0.2])
# root = seidel_method_system([g1, g2], x0)
# print(f'Найденное решение: {root}')"""
    return theory + code

### Метод Ньютона для СНУ

def p9():
    theory = """
## 9. Метод Ньютона для систем нелинейных уравнений

### Теоретическое описание:
Это обобщение метода Ньютона для систем нелинейных уравнений `F(X) = 0`, где `X` и `F` — векторы.

**Основная идея:**
На каждом шаге система нелинейных уравнений `F(X) = 0` линеаризуется в окрестности текущего приближения `X_k` с помощью матрицы Якоби `J(X_k)`. Матрица Якоби — это матрица из частных производных.

Получается система линейных алгебраических уравнений (СЛАУ) для нахождения поправки `ΔX_k`:
`J(X_k) * ΔX_k = -F(X_k)`

**Алгоритм:**
1.  Выбрать начальное приближение `X0`.
2.  На каждой итерации `k`:
    a. Вычислить вектор `F(X_k)` и матрицу Якоби `J(X_k)`.
    b. Решить СЛАУ для нахождения вектора поправки `ΔX_k`.
    c. Обновить приближение: `X_{k+1} = X_k + ΔX_k`.
3.  Повторять до достижения необходимой точности.
"""
    code = """
### Код реализации:
```python
import numpy as np

def _solve_gauss_manual(A, b):
    \"\"\"
    Решает СЛАУ Ax=b методом Гаусса с частичным выбором ведущего элемента.
    Это вспомогательная функция, не использующая np.linalg.solve.
    \"\"\"
    n = len(b)
    A_copy = np.copy(A).astype(float)
    b_copy = np.copy(b).astype(float)

    # Прямой ход (Приведение к верхнетреугольному виду)
    for i in range(n):
        # Поиск главного элемента в текущем столбце для устойчивости
        pivot_row = i
        for k in range(i + 1, n):
            if abs(A_copy[k, i]) > abs(A_copy[pivot_row, i]):
                pivot_row = k
        A_copy[[i, pivot_row]] = A_copy[[pivot_row, i]]
        b_copy[[i, pivot_row]] = b_copy[[pivot_row, i]]

        # Нормализация ведущей строки
        pivot = A_copy[i, i]
        for j in range(i, n):
            A_copy[i, j] /= pivot
        b_copy[i] /= pivot

        # Вычитание из нижних строк
        for k in range(i + 1, n):
            factor = A_copy[k, i]
            for j in range(i, n):
                A_copy[k, j] -= factor * A_copy[i, j]
            b_copy[k] -= factor * b_copy[i]

    # Обратный ход
    x = np.zeros(n)
    for i in range(n - 1, -1, -1):
        sum_ax = 0
        for j in range(i + 1, n):
            sum_ax += A_copy[i, j] * x[j]
        x[i] = b_copy[i] - sum_ax

    return x

def newton_method_system(f_funcs, j_funcs, x0, tol=1e-5, n_iterations=20):
    \"\"\"
    Реализация метода Ньютона для СНУ с ручным решателем СЛАУ.
    \"\"\"
    x = np.array(x0, dtype=float)

    for k in range(n_iterations):
        fx = f_funcs(x)
        J = j_funcs(x)

        # Решаем систему J * delta_x = -fx вручную
        delta_x = _solve_gauss_manual(J, -fx)

        x = x + delta_x

        # Проверка сходимости по норме (максимальное абсолютное значение)
        if np.max(np.abs(delta_x)) < tol:
            return x

    return x

# Пример использования:
# Решим систему: f1 = x1^2 + x2^2 - 1 = 0; f2 = 5*x1^2 + 21*x2^2 - 9 = 0
# def f_sys(x):
#     return np.array([x[0]**2 + x[1]**2 - 1.0,
#                      5.0*x[0]**2 + 21.0*x[1]**2 - 9.0])
#
# def jacobian_sys(x):
#     return np.array([[2*x[0], 2*x[1]],
#                      [10*x[0], 42*x[1]]])
#
# x0 = np.array([1.0, 1.0])
# root = newton_method_system(f_sys, jacobian_sys, x0)
# print(f'Найденное решение: {root}')"""
    return theory + code

def p10():
    theory = """
## 10. Модифицированный метод Ньютона для систем нелинейных уравнений

### Теоретическое описание:
Это вариация классического метода Ньютона для систем, предназначенная для снижения вычислительной сложности.

**Основная идея:**
Матрица Якоби `J`, как и производная в одномерном случае, вычисляется не на каждой итерации, а только один раз в начальной точке `X0`. Это значительно упрощает вычисления, так как сложная операция решения СЛАУ (например, методом Гаусса) выполняется для одной и той же матрицы на всех шагах.

Итерационный процесс включает решение СЛАУ:
`J(X0) * ΔX_k = -F(X_k)`
И обновление приближения:
`X_{k+1} = X_k + ΔX_k`

**Преимущества:**
-   Значительно снижает вычислительные затраты на каждой итерации.

**Недостатки:**
-   Скорость сходимости падает с квадратичной до линейной.
"""
    code = """
### Код реализации (на чистом Python):
```python
def _solve_gauss_python(A, b):
    \"\"\"
    Решает СЛАУ Ax=b методом Гаусса на чистом Python.
    A - список списков (матрица), b - список (вектор).
    \"\"\"
    n = len(b)
    # Создаем расширенную матрицу
    M = [row[:] + [b_i] for row, b_i in zip(A, b)]

    # Прямой ход
    for i in range(n):
        # Поиск главного элемента
        pivot_row = i
        for k in range(i + 1, n):
            if abs(M[k][i]) > abs(M[pivot_row][i]):
                pivot_row = k
        M[i], M[pivot_row] = M[pivot_row], M[i]

        # Приведение к верхнетреугольному виду
        pivot = M[i][i]
        for j in range(i, n + 1):
            M[i][j] /= pivot
        for k in range(i + 1, n):
            factor = M[k][i]
            for j in range(i, n + 1):
                M[k][j] -= factor * M[i][j]

    # Обратный ход
    x = [0] * n
    for i in range(n - 1, -1, -1):
        x[i] = M[i][n]
        for j in range(i + 1, n):
            x[i] -= M[i][j] * x[j]
    return x

def modified_newton_system_python(f_funcs, j_funcs, x0, tol=1e-5, n_iter=100):
    \"\"\"
    Реализация модифицированного метода Ньютона для СНУ на чистом Python.
    \"\"\"
    x = list(x0)
    J0 = j_funcs(x) # Якобиан один раз

    for k in range(n_iter):
        fx = f_funcs(x)
        neg_fx = [-val for val in fx]

        # Решаем систему J0 * delta_x = -fx
        delta_x = _solve_gauss_python(J0, neg_fx)

        # Обновление и проверка
        max_delta = 0
        for i in range(len(x)):
            current_delta = delta_x[i]
            x[i] += current_delta
            if abs(current_delta) > max_delta:
                max_delta = abs(current_delta)

        if max_delta < tol:
            return x

    return x

# Пример использования:
# def f_sys_py(x_list):
#     x1, x2 = x_list
#     return [x1**2 + x2**2 - 1.0, 5*x1**2 + 21*x2**2 - 9.0]
#
# def jacobian_sys_py(x_list):
#     x1, x2 = x_list
#     return [[2*x1, 2*x2], [10*x1, 42*x2]]
#
# x0_py = [1.0, 1.0]
# root = modified_newton_system_python(f_sys_py, jacobian_sys_py, x0_py)
# print(f'Найденное решение: {root}')"""
    return theory + code

def p11():
    theory = """
## 11. Линейная интерполяция

### Теоретическое описание:
Линейная интерполяция — это простейший вид полиномиальной интерполяции, при котором для нахождения промежуточного значения используется линейный полином (прямая линия).

**Основная идея:**
Если известны значения функции в двух точках `(x_i, y_i)` и `(x_{i+1}, y_{i+1})`, то для любой точки `x` между `x_i` и `x_{i+1}` значение `y` можно найти, проведя прямую через эти две точки.

Формула выводится из уравнения прямой, проходящей через две точки:
`(y - y_i) / (x - x_i) = (y_{i+1} - y_i) / (x_{i+1} - x_i)`

Отсюда, формула для интерполированного значения `y`:
`y = y_i + (y_{i+1} - y_i) * (x - x_i) / (x_{i+1} - x_i)`

**Геометрическая интерпретация:**
График исходной функции на отрезке `[x_i, x_{i+1}]` заменяется отрезком прямой линии (хордой).

**Преимущества:**
-   Простота реализации и низкая вычислительная сложность.

**Недостатки:**
-   Низкая точность, особенно для сильно нелинейных функций.
-   Производная интерполирующей функции является кусочно-постоянной, что означает наличие "изломов" в узловых точках.
"""
    code = """
### Код реализации:
```python
def linear_interpolation(x_values, y_values, x_new):
    '''
    Линейная интерполяция.

    Параметры:
        x_values, y_values (list or np.array): Координаты известных точек.
        x_new (float): Точка, в которой нужно найти значение.
    '''
    # Поиск нужного интервала
    for i in range(len(x_values) - 1):
        if x_values[i] <= x_new <= x_values[i+1]:
            # Формула линейной интерполяции
            y_new = y_values[i] + (y_values[i+1] - y_values[i]) * \\
                    (x_new - x_values[i]) / (x_values[i+1] - x_values[i])
            return y_new
    return None # Если x_new вне диапазона

# Пример использования:
# x = [0, 1, 2]
# y = [1, 3, 2]
# y_hat = linear_interpolation(x, y, 1.5)
# print(f'Интерполированное значение y при x = 1.5: {y_hat}')"""
    return theory + code

def p12():
    theory = """
## 12. Интерполяционный многочлен Лагранжа

### Теоретическое описание:
Многочлен Лагранжа — это метод глобальной полиномиальной интерполяции. Для `n+1` точки он строит единственный полином степени не выше `n`, который проходит через все эти точки.

**Основная идея:**
Многочлен `L(x)` строится как сумма базисных полиномов Лагранжа `l_i(x)`, каждый из которых умножается на соответствующее значение `y_i`.
`L(x) = sum_{i=0 to n} y_i * l_i(x)`
Каждый базисный полином `l_i(x)` обладает свойством `l_i(x_j) = δ_ij` (символ Кронекера).

**Недостатки:**
-   Высокая вычислительная сложность.
-   **Феномен Рунге:** При интерполяции на равномерной сетке по краям интервала могут возникать сильные осцилляции.
-   При добавлении нового узла весь полином нужно пересчитывать заново.
"""
    code = """
### Код реализации (на чистом Python):
```python
def lagrange_polynomial_python(x_values, y_values, x_new):
    '''
    Вычисление значения интерполяционного многочлена Лагранжа.
    '''
    n = len(x_values)
    result = 0.0

    for i in range(n):
        # Вычисляем i-й базисный полином l_i(x_new)
        term = y_values[i]
        for j in range(n):
            if i != j:
                term = term * (x_new - x_values[j]) / (x_values[i] - x_values[j])
        result += term

    return result

# Пример использования:
# x = [0, 1, 2]
# y = [1, 3, 2]
# y_hat = lagrange_polynomial_python(x, y, 1.5)
# print(f'Интерполированное значение y при x = 1.5: {y_hat}')"""
    return theory + code

def p13():
    theory = """
## 13. Кубическая сплайн-интерполяция

### Теоретическое описание:
Сплайн-интерполяция — это метод кусочно-полиномиальной интерполяции. Кубический сплайн наиболее распространен. На каждом отрезке `[x_i, x_{i+1}]` функция аппроксимируется своим кубическим полиномом `S_i(x)`.

**Основная идея:**
Коэффициенты полиномов подбираются так, чтобы в узловых точках (`x_i`) были непрерывны значения, а также первая и вторая производные. Это обеспечивает гладкость.

`S_i(x) = a_i + b_i(x-x_i) + c_i(x-x_i)^2 + d_i(x-x_i)^3`

Нахождение коэффициентов `a, b, c, d` сводится к решению системы линейных уравнений с трехдиагональной матрицей.

**Преимущества:**
-   Обеспечивает гладкую интерполяцию без осцилляций.
-   Высокая точность.

**Недостатки:**
-   Сложная реализация "с нуля".
"""
    code = """
### Код реализации (на чистом Python):
```python
def _solve_tridiagonal_system(a, b, c, d):
    \"\"\"
    Решает СЛАУ с трехдиагональной матрицей методом прогонки.
    a - поддиагональ, b - главная диагональ, c - наддиагональ, d - правая часть.
    \"\"\"
    n = len(d)
    c_p = [0] * n
    d_p = [0] * n
    x = [0] * n

    # Прямой ход
    c_p[0] = c[0] / b[0]
    d_p[0] = d[0] / b[0]
    for i in range(1, n):
        m = 1.0 / (b[i] - a[i] * c_p[i-1])
        c_p[i] = c[i] * m
        d_p[i] = (d[i] - a[i] * d_p[i-1]) * m

    # Обратный ход
    x[n-1] = d_p[n-1]
    for i in range(n-2, -1, -1):
        x[i] = d_p[i] - c_p[i] * x[i+1]

    return x

def cubic_spline_coeffs_python(x, y):
    \"\"\"
    Вычисляет коэффициенты для естественного кубического сплайна.
    \"\"\"
    n = len(x) - 1
    h = [x[i+1] - x[i] for i in range(n)]

    # Составляем СЛАУ для вторых производных (c_i)
    # Матрица будет трехдиагональной
    A_diag = [2 * (h[i] + h[i+1]) for i in range(n-1)]
    A_upper = h[1:-1]
    A_lower = h[1:-1]

    B = [6 * ((y[i+2] - y[i+1]) / h[i+1] - (y[i+1] - y[i]) / h[i]) for i in range(n-1)]

    # Решаем систему для c_1, ..., c_{n-1}
    # c_0 и c_n равны 0 для естественного сплайна
    c_internal = _solve_tridiagonal_system([0]+A_lower, A_diag, A_upper+[0], B)
    c_coeffs = [0] + c_internal + [0]

    a_coeffs = list(y)
    b_coeffs = [(y[i+1] - y[i]) / h[i] - h[i] * (c_coeffs[i+1] + 2*c_coeffs[i]) / 3 for i in range(n)]
    d_coeffs = [(c_coeffs[i+1] - c_coeffs[i]) / (3 * h[i]) for i in range(n)]

    return a_coeffs, b_coeffs, c_coeffs, d_coeffs

def cubic_spline_python(x_values, y_values, x_new):
    '''
    Вычисляет значение кубического сплайна в точке x_new.
    '''
    a, b, c, d = cubic_spline_coeffs_python(x_values, y_values)

    # Поиск интервала
    for i in range(len(x_values) - 1):
        if x_values[i] <= x_new <= x_values[i+1]:
            dx = x_new - x_values[i]
            return a[i] + b[i] * dx + c[i] * dx**2 + d[i] * dx**3

    return None

# Пример использования:
# x = [0, 1, 2, 3]
# y = [0, 1, 0, 1]
# y_hat = cubic_spline_python(x, y, 1.5)
# print(f'Интерполированное значение y при x = 1.5: {y_hat}')"""
    return theory + code

def p14():
    theory = """
## 14. Наивный алгоритм перемножения матриц

### Теоретическое описание:
Это базовый алгоритм перемножения двух матриц A (размера m x n) и B (размера n x p), основанный непосредственно на определении матричного умножения.

**Основная идея:**
Каждый элемент `C_ij` результирующей матрицы `C` (размера m x p) вычисляется как скалярное произведение `i`-й строки матрицы `A` на `j`-й столбец матрицы `B`.

Формула для элемента `C_ij`:
`C_{ij} = sum_{k=0 to n-1} A_{ik} * B_{kj}`

**Алгоритм:**
Для вычисления результирующей матрицы `C` используются три вложенных цикла.

**Сложность:** O(n³) для квадратных матриц.
"""
    code = """
### Код реализации (на чистом Python):
```python
def naive_matrix_multiplication_python(A, B):
    '''
    Наивное перемножение матриц A и B.
    Матрицы представлены как списки списков.
    '''
    m = len(A)
    n = len(A[0])
    p = len(B[0])

    if n != len(B):
        raise ValueError("Несовместимые размеры матриц.")

    # Инициализация результирующей матрицы нулями
    C = [[0 for _ in range(p)] for _ in range(m)]

    for i in range(m):
        for j in range(p):
            for k in range(n):
                C[i][j] += A[i][k] * B[k][j]

    return C

# Пример использования:
# A = [[1, 2], [3, 4]]
# B = [[5, 6], [7, 8]]
# C = naive_matrix_multiplication_python(A, B)
# print("Результат перемножения:")
# print(C)"""
    return theory + code

def p15():
    theory = """
## 15. Алгоритм Штрассена для перемножения матриц

### Теоретическое описание:
Алгоритм Штрассена — это рекурсивный алгоритм для быстрого перемножения матриц, который асимптотически быстрее наивного алгоритма.

**Основная идея:**
Алгоритм основан на методе "разделяй и властвуй". Две исходные квадратные матрицы `A` и `B` размера n x n (где n - степень двойки) разбиваются на четыре подматрицы размера (n/2) x (n/2) каждая:
`A = [[A11, A12], [A21, A22]]`
`B = [[B11, B12], [B21, B22]]`

Вместо 8 рекурсивных умножений (как в наивном блочном методе) Штрассен показал, что можно обойтись всего 7 умножениями и дополнительными операциями сложения/вычитания матриц. Вычисляются 7 промежуточных матриц `P1, ..., P7`, из которых затем комбинируются блоки результирующей матрицы `C`.

**Сложность:**
-   **Временная сложность:** O(n^log₂(7)) ≈ O(n^2.807). Это лучше, чем O(n³) у наивного алгоритма.
-   На практике алгоритм становится эффективнее наивного только для матриц достаточно большого размера из-за накладных расходов на рекурсию и сложение/вычитание.

**Недостатки:**
-   Сложнее в реализации.
-   Может быть менее численно устойчивым из-за большего числа сложений и вычитаний.
-   Наиболее простая реализация требует, чтобы размер матриц был степенью двойки.
"""
    code = """
### Код реализации (с использованием NumPy для базовых операций):
```python
import numpy as np

def strassen_multiplication(A, B):
    \"\"\"
    Перемножение матриц A и B алгоритмом Штрассена.
    Предполагается, что матрицы квадратные и их размер - степень двойки.
    \"\"\"
    n = A.shape[0]

    # Базовый случай рекурсии
    if n <= 2:
        # Используем наивный метод для маленьких матриц
        m, n_a = A.shape
        n_b, p = B.shape
        C = np.zeros((m, p))
        for i in range(m):
            for j in range(p):
                for k in range(n_a):
                    C[i, j] += A[i, k] * B[k, j]
        return C

    # Разделение матриц на подблоки
    mid = n // 2
    A11, A12 = A[:mid, :mid], A[:mid, mid:]
    A21, A22 = A[mid:, :mid], A[mid:, mid:]
    B11, B12 = B[:mid, :mid], B[:mid, mid:]
    B21, B22 = B[mid:, :mid], B[mid:, mid:]

    # 7 рекурсивных вызовов по формулам Штрассена
    P1 = strassen_multiplication(A11 + A22, B11 + B22)
    P2 = strassen_multiplication(A21 + A22, B11)
    P3 = strassen_multiplication(A11, B12 - B22)
    P4 = strassen_multiplication(A22, B21 - B11)
    P5 = strassen_multiplication(A11 + A12, B22)
    P6 = strassen_multiplication(A21 - A11, B11 + B12)
    P7 = strassen_multiplication(A12 - A22, B21 + B22)

    # Комбинирование результатов
    C11 = P1 + P4 - P5 + P7
    C12 = P3 + P5
    C21 = P2 + P4
    C22 = P1 - P2 + P3 + P6

    # Сборка итоговой матрицы
    C = np.vstack((np.hstack((C11, C12)), np.hstack((C21, C22))))

    return C

# Пример использования:
# n_size = 16 # Размер должен быть степенью двойки
# A = np.random.rand(n_size, n_size)
# B = np.random.rand(n_size, n_size)
# C_strassen = strassen_multiplication(A, B)
# print("Результат (Штрассен):")
# print(C_strassen)"""
    return theory + code

def p16():
    theory = """
## 16. Вычисление собственных значений через характеристический многочлен

### Теоретическое описание:
Это классический аналитический метод нахождения собственных значений квадратной матрицы `A`. Собственные значения `λ` являются корнями **характеристического уравнения**:
`det(A - λI) = 0`
где `I` — единичная матрица, а `det` — определитель.

**Основная идея:**
1.  **Построение многочлена:** Выражение `det(A - λI)` представляет собой многочлен степени `n` относительно `λ`, который называется характеристическим многочленом.
2.  **Нахождение корней:** Собственные значения `λ_1, ..., λ_n` являются корнями этого многочлена.

**Алгоритм:**
1.  Сформировать матрицу `A - λI`.
2.  Вычислить её определитель, получив явное выражение для характеристического многочлена `p(λ)`.
3.  Найти корни многочлена `p(λ) = 0`.

**Преимущества:**
-   Дает точное аналитическое решение для малых матриц (2x2, 3x3).
-   Фундаментальный метод, лежащий в основе теории собственных значений.

**Недостатки:**
-   **Вычислительная сложность:** Нахождение коэффициентов многочлена через определитель очень затратно (O(n!)).
-   **Численная неустойчивость:** Корни многочлена очень чувствительны к малейшим изменениям его коэффициентов. Небольшие ошибки округления при вычислении коэффициентов могут привести к огромным ошибкам в найденных корнях (собственных значениях).
-   Из-за этих недостатков на практике этот метод почти не используется для матриц размером больше 4x4. Вместо него применяются итерационные методы (например, QR-алгоритм).
"""
    code = """
### Код реализации (пример для матрицы 2x2):
```python
import numpy as np
import cmath # для работы с комплексными корнями

def characteristic_poly_2x2(A):
    \"\"\"
    Находит собственные значения матрицы 2x2 через корни характ. многочлена.
    \"\"\"
    if A.shape != (2, 2):
        raise ValueError("Матрица должна быть размера 2x2")

    # Характеристический многочлен: λ^2 - tr(A)λ + det(A) = 0
    # где tr(A) - след матрицы (сумма диагональных элементов)

    tr_A = A[0, 0] + A[1, 1]
    det_A = A[0, 0] * A[1, 1] - A[0, 1] * A[1, 0]

    # Решение квадратного уравнения: a=1, b=-tr_A, c=det_A
    discriminant = tr_A**2 - 4 * det_A

    if discriminant >= 0:
        lambda1 = (tr_A + discriminant**0.5) / 2
        lambda2 = (tr_A - discriminant**0.5) / 2
    else:
        # Комплексные корни
        lambda1 = (tr_A + cmath.sqrt(discriminant)) / 2
        lambda2 = (tr_A - cmath.sqrt(discriminant)) / 2

    return lambda1, lambda2

# Пример использования:
# A = np.array([[4, 1], [2, 3]])
# eigenvalues = characteristic_poly_2x2(A)
# print(f'Собственные значения: {eigenvalues}')
# print(f'Проверка с помощью numpy: {np.linalg.eigvals(A)}')"""
    return theory + code

def p17():
    theory = """
## 17. Степенной метод

### Теоретическое описание:
Степенной метод — это итерационный алгоритм для нахождения собственного значения с наибольшим модулем (доминантного собственного значения) и соответствующего ему собственного вектора.

**Основная идея:**
Метод основан на многократном умножении матрицы `A` на некоторый начальный вектор `x0`.
`x_{k+1} = A * x_k`
При большом количестве итераций `k`, вектор `A^k * x0` будет стремиться к направлению собственного вектора, соответствующего доминантному собственному значению `λ1`.

Чтобы избежать переполнения или исчезновения компонент вектора, на каждом шаге его нормируют.

**Алгоритм:**
1.  Выбрать произвольный начальный вектор `x0` (обычно с единичной нормой).
2.  На каждой итерации `k`:
    a. Вычислить `y_{k+1} = A * x_k`.
    b. Нормализовать вектор: `x_{k+1} = y_{k+1} / ||y_{k+1}||`.
    c. Оценить собственное значение: `λ_1 ≈ (x_k^T * A * x_k) / (x_k^T * x_k)` (отношение Рэлея) или проще `λ_1 ≈ ||y_{k+1}||`.
3.  Повторять, пока изменения в векторе `x_k` или значении `λ_1` не станут пренебрежимо малы.

**Преимущества:**
-   Простота реализации.
-   Эффективен для больших разреженных матриц, так как требует только операции умножения матрицы на вектор.

**Недостатки:**
-   Находит только доминантное собственное значение.
-   Скорость сходимости зависит от отношения `|λ2/λ1|`, где `λ2` — следующее по величине собственное значение. Если `|λ1| ≈ |λ2|`, сходимость очень медленная.
-   Не сходится, если доминантных собственных значений несколько (например, комплексная пара).
"""
    code = """
### Код реализации (на чистом Python):
```python
def power_iteration_python(A, n_iter=100, tol=1e-6):
    \"\"\"
    Степенной метод для нахождения доминантного собственного значения и вектора.
    A - список списков.
    \"\"\"
    n = len(A)
    # Случайный начальный вектор
    x = [1.0] * n

    lambda_old = 0.0

    for _ in range(n_iter):
        # Умножение матрицы на вектор
        Ax = [0.0] * n
        for i in range(n):
            for j in range(n):
                Ax[i] += A[i][j] * x[j]

        # Нахождение нормы (для нормализации)
        norm_ax = 0
        for val in Ax:
            norm_ax += val**2
        norm_ax = norm_ax**0.5

        # Нормализация
        x_new = [val / norm_ax for val in Ax]

        # Оценка собственного значения (используя норму)
        lambda_new = norm_ax

        # Проверка сходимости
        if abs(lambda_new - lambda_old) < tol:
            break

        x = x_new
        lambda_old = lambda_new

    return lambda_new, x

# Пример использования:
# A_list = [[4, 1], [2, 3]]
# eigenvalue, eigenvector = power_iteration_python(A_list)
# print(f"Доминантное собственное значение: {eigenvalue}")
# print(f"Соответствующий собственный вектор: {eigenvector}")"""
    return theory + code

def p18():
    theory = """
## 18. Степенной метод со сдвигами (обратная итерация)

### Теоретическое описание:
Степенной метод со сдвигами, или обратная итерация, — это модификация степенного метода, позволяющая находить собственное значение, ближайшее к заданному числу `σ` (сдвигу).

**Основная идея:**
Если `λ_i` — собственные значения матрицы `A`, то `(λ_i - σ)` — собственные значения матрицы `(A - σI)`. Соответственно, `1 / (λ_i - σ)` являются собственными значениями обратной матрицы `(A - σI)⁻¹`.

Если выбрать сдвиг `σ` очень близко к искомому собственному значению `λ_j`, то `(λ_j - σ)` будет очень малым числом. Тогда `1 / (λ_j - σ)` станет очень большим — доминантным собственным значением для матрицы `(A - σI)⁻¹`.

Таким образом, применяя степенной метод к матрице `(A - σI)⁻¹`, мы находим её доминантное собственное значение `μ`, а искомое собственное значение `λ_j` матрицы `A` равно:
`λ_j = 1/μ + σ`

**Алгоритм:**
1.  Выбрать начальный вектор `x0` и сдвиг `σ`, близкий к искомому `λ_j`.
2.  На каждой итерации `k`:
    a. Решить СЛАУ `(A - σI) * y_{k+1} = x_k`. (Это эквивалентно умножению на `(A - σI)⁻¹`, но численно гораздо устойчивее).
    b. Нормализовать вектор: `x_{k+1} = y_{k+1} / ||y_{k+1}||`.
3.  Повторять до сходимости.

**Преимущества:**
-   Позволяет находить не только доминантное, но и любое другое собственное значение, если известно его примерное положение.
-   Очень быстрая сходимость, если сдвиг `σ` выбран удачно.

**Недостатки:**
-   Требует решения СЛАУ на каждом шаге, что является вычислительно дорогой операцией.
"""
    code = """
### Код реализации (с ручным решателем СЛАУ):
```python
# Вспомогательная функция для решения СЛАУ методом Гаусса
def _solve_gauss_python(A, b):
    # (Код этой функции представлен в p10 для модифицированного метода Ньютона)
    # ...
    n = len(b)
    # Создаем расширенную матрицу
    M = [row[:] + [b_i] for row, b_i in zip(A, b)]

    # Прямой ход
    for i in range(n):
        pivot_row = i
        for k in range(i + 1, n):
            if abs(M[k][i]) > abs(M[pivot_row][i]):
                pivot_row = k
        M[i], M[pivot_row] = M[pivot_row], M[i]
        pivot = M[i][i]
        for j in range(i, n + 1):
            M[i][j] /= pivot
        for k in range(i + 1, n):
            factor = M[k][i]
            for j in range(i, n + 1):
                M[k][j] -= factor * M[i][j]

    # Обратный ход
    x = [0] * n
    for i in range(n - 1, -1, -1):
        x[i] = M[i][n]
        for j in range(i + 1, n):
            x[i] -= M[i][j] * x[j]
    return x

def inverse_iteration_python(A, sigma, n_iter=100, tol=1e-6):
    \"\"\"
    Степенной метод со сдвигом (обратная итерация) на чистом Python.
    A - список списков, sigma - сдвиг.
    \"\"\"
    n = len(A)
    # Создаем матрицу A - sigma*I
    A_s = [[(A[i][j] - sigma) if i == j else A[i][j] for j in range(n)] for i in range(n)]

    # Случайный начальный вектор
    x = [1.0] * n
    lambda_eigen = 0

    for _ in range(n_iter):
        # Решаем СЛАУ (A - σI) * y = x
        y = _solve_gauss_python(A_s, x)

        # Находим норму
        norm_y = 0
        for val in y:
            norm_y += val**2
        norm_y = norm_y**0.5

        # Нормализация
        x = [val / norm_y for val in y]

        # Оценка собственного значения
        mu = norm_y
        lambda_new = 1.0/mu + sigma

        if abs(lambda_new - lambda_eigen) < tol:
            break
        lambda_eigen = lambda_new

    return lambda_eigen, x

# Пример использования:
# A_list = [[4, 1], [2, 3]]
# # Ищем собственное значение, близкое к 2.8 (точное значение 2.0)
# eigenvalue, eigenvector = inverse_iteration_python(A_list, sigma=2.8)
# print(f"Собственное значение, близкое к 2.8: {eigenvalue}")"""
    return theory + code

def p19():
    theory = """
## 19. Метод вращений (метод Якоби)

### Теоретическое описание:
Метод вращений Якоби — это итерационный алгоритм для нахождения всех собственных значений и собственных векторов **симметричной** матрицы.

**Основная идея:**
Метод основан на последовательном "занулении" внедиагональных элементов матрицы с помощью преобразований подобия с ортогональными матрицами. В качестве таких матриц используются **матрицы вращения Гивенса**.

На каждой итерации находится самый большой по модулю внедиагональный элемент `a_ij`. Затем строится матрица вращения `R(i, j, θ)` так, чтобы после преобразования `A_{k+1} = R^T * A_k * R`, элемент на позиции `(i, j)` (и `(j, i)`) стал равен нулю.

Этот процесс повторяется. Так как сумма квадратов внедиагональных элементов на каждом шаге уменьшается, матрица `A_k` итерационно стремится к диагональной.

**Алгоритм:**
1.  Начать с исходной симметричной матрицы `A_0 = A`.
2.  На итерации `k`:
    a. Найти внедиагональный элемент `a_ij` с максимальным абсолютным значением.
    b. Вычислить угол поворота `θ` по формуле, которая занулит этот элемент.
    c. Построить матрицу вращения `R_k`.
    d. Выполнить преобразование: `A_{k+1} = R_k^T * A_k * R_k`.
3.  Повторять до тех пор, пока все внедиагональные элементы не станут достаточно малы.
4.  Собственные значения будут находиться на диагонали итоговой матрицы, а собственные векторы — в столбцах произведения всех матриц вращения `R_1 * R_2 * ...`.

**Преимущества:**
-   Надежный и простой для понимания метод для симметричных матриц.
-   Находит сразу все собственные значения и векторы.

**Недостатки:**
-   Применим только к симметричным матрицам.
-   Медленнее, чем QR-алгоритм для больших матриц.
"""
    code = """
### Код реализации (с использованием NumPy для базовых операций):
```python
import numpy as np
import math

def jacobi_rotation_method(A, tol=1e-9, max_iter=100):
    \"\"\"
    Находит все собственные значения и векторы симметричной матрицы A
    методом вращений Якоби.
    \"\"\"
    n = A.shape[0]
    A_k = np.copy(A)
    V = np.identity(n) # Матрица собственных векторов

    for _ in range(max_iter):
        # Находим максимальный внедиагональный элемент
        max_val = 0
        p, q = 0, 1
        for i in range(n):
            for j in range(i + 1, n):
                if abs(A_k[i, j]) > max_val:
                    max_val = abs(A_k[i, j])
                    p, q = i, j

        if max_val < tol:
            break

        # Вычисляем угол вращения
        app = A_k[p, p]
        aqq = A_k[q, q]
        apq = A_k[p, q]

        if app == aqq:
            theta = np.pi / 4
        else:
            theta = 0.5 * math.atan(2 * apq / (app - aqq))

        c = math.cos(theta)
        s = math.sin(theta)

        # Создаем матрицу вращения
        R = np.identity(n)
        R[p, p], R[q, q] = c, c
        R[p, q], R[q, p] = -s, s

        # Обновляем матрицу A и матрицу собственных векторов
        A_k = R.T @ A_k @ R
        V = V @ R

    eigenvalues = np.diag(A_k)
    eigenvectors = V

    return eigenvalues, eigenvectors

# Пример использования:
# A = np.array([[4, 2, 1], [2, 5, 3], [1, 3, 6]], dtype=float)
# eigvals, eigvecs = jacobi_rotation_method(A)
# print("Собственные значения:", eigvals)
# print("Собственные векторы:\\n", eigvecs)"""
    return theory + code

def p20():
    theory = """
## 20. QR-алгоритм

### Теоретическое описание:
QR-алгоритм — это один из самых эффективных и широко используемых итерационных методов для вычисления всех собственных значений (и векторов) матрицы.

**Основная идея:**
Алгоритм генерирует последовательность матриц A_k, которые подобны исходной матрице A и сходятся к верхнетреугольной (для общих матриц) или диагональной (для симметричных) форме.

**Базовый QR-алгоритм:**
1.  **Начальный шаг:** `A_0 = A`.
2.  **Итерация k:**
    a. Выполнить QR-разложение матрицы `A_k`: `A_k = Q_k * R_k`, где `Q_k` — ортогональная матрица, а `R_k` — верхнетреугольная.
    b. Вычислить следующую матрицу: `A_{k+1} = R_k * Q_k`.

Поскольку `R_k = Q_k^T * A_k`, то `A_{k+1} = Q_k^T * A_k * Q_k`, что является преобразованием подобия. Следовательно, все матрицы `A_k` имеют те же собственные значения, что и `A`. В пределе `A_k` сходится к **форме Шура** (верхнетреугольной), и собственные значения оказываются на её диагонали.

**Преимущества:**
-   Численно устойчив.
-   Сходится для широкого класса матриц.

**Недостатки:**
-   Базовая версия сходится медленно. На практике всегда используются улучшения: **сдвиги** для ускорения сходимости и предварительное **приведение к форме Хессенберга** для удешевления QR-разложения.
"""
    code = """
### Код реализации (базовая версия с ручным QR-разложением):
```python
import numpy as np

def _gram_schmidt_qr(A):
    \"\"\"
    QR-разложение матрицы A с помощью процесса Грама-Шмидта.
    Возвращает ортогональную матрицу Q и верхнетреугольную R.
    \"\"\"
    n, m = A.shape
    Q = np.zeros((n, m))
    R = np.zeros((m, m))

    for j in range(m):
        v = A[:, j]
        for i in range(j):
            # R[i, j] = q_i^T * a_j
            R[i, j] = np.dot(Q[:, i], A[:, j])
            # v = v - (q_i^T * a_j) * q_i
            v = v - R[i, j] * Q[:, i]

        # R[j, j] = ||v||
        R[j, j] = np.linalg.norm(v)
        # q_j = v / ||v||
        Q[:, j] = v / R[j, j]

    return Q, R

def naive_matrix_multiplication(A, B):
    \"\"\"Наивное перемножение матриц.\"\"\"
    m, n = A.shape
    n_b, p = B.shape
    if n != n_b:
        raise ValueError("Несовместимые размеры матриц.")
    C = np.zeros((m, p))
    for i in range(m):
        for j in range(p):
            for k in range(n):
                C[i, j] += A[i, k] * B[k, j]
    return C

def basic_qr_algorithm_manual(A, max_iter=100, tol=1e-9):
    \"\"\"
    Базовый QR-алгоритм с ручной реализацией QR-разложения.
    \"\"\"
    A_k = np.copy(A).astype(float)
    n = A_k.shape[0]

    for _ in range(max_iter):
        Q, R = _gram_schmidt_qr(A_k)
        A_k = naive_matrix_multiplication(R, Q) # A_{k+1} = R_k * Q_k

        # Проверка сходимости: смотрим на сумму абсолютных значений под диагональю
        off_diagonal_sum = 0
        for i in range(n):
            for j in range(i):
                off_diagonal_sum += abs(A_k[i, j])

        if off_diagonal_sum < tol:
            break

    # Собственные значения находятся на диагонали A_k
    eigenvalues = [A_k[i, i] for i in range(n)]
    return eigenvalues

# Пример использования:
# A = np.array([[4, 1, 1], [1, 3, 2], [1, 2, 6]], dtype=float)
# eigvals = basic_qr_algorithm_manual(A)
# print("Приближенные собственные значения:", eigvals)"""
    return theory + code

def p21():
    theory = """
## 21. Разложение Шура и Теорема Шура

### Теоретическое описание:
Разложение (или декомпозиция) Шура утверждает, что любую квадратную матрицу `A` с комплексными элементами можно представить в виде:
`A = U * T * U*`
где:
-   `U` — унитарная матрица (`U* * U = I`, где `U*` — эрмитово-сопряженная матрица). Для вещественных матриц `U` — ортогональная матрица (`U^T * U = I`).
-   `T` — верхнетреугольная матрица, называемая **формой Шура** матрицы `A`.

**Теорема Шура:**
Для любой квадратной матрицы `A` размера n x n существует унитарная матрица `U`, такая что `T = U* * A * U` является верхнетреугольной.

**Ключевые свойства:**
1.  **Собственные значения:** Собственные значения матрицы `A` совпадают с диагональными элементами её формы Шура `T`. Это следует из того, что `A` и `T` подобны.
2.  **Численная устойчивость:** Преобразования с унитарными/ортогональными матрицами численно устойчивы.
3.  **Связь с QR-алгоритмом:** QR-алгоритм — это итерационный способ построения разложения Шура. В ходе алгоритма матрица `A_k` сходится к форме Шура `T`, а произведение всех ортогональных матриц `Q_k` сходится к матрице `U`.

`T = lim_{k->∞} A_k`
`U = lim_{k->∞} (Q_0 * Q_1 * ... * Q_k)`

**Практическая реализация:**
Нахождение разложения Шура эквивалентно выполнению полного QR-алгоритма до сходимости.
"""
    code = """
### Код реализации (через QR-алгоритм):
Ручная реализация разложения Шура — это, по сути, полная версия QR-алгоритма, где дополнительно накапливается произведение матриц Q.

```python
import numpy as np

def _gram_schmidt_qr(A):
    # (Код этой функции представлен в p20 для QR-алгоритма)
    # ...
    n, m = A.shape
    Q = np.zeros((n, m))
    R = np.zeros((m, m))
    for j in range(m):
        v = A[:, j]
        for i in range(j):
            R[i, j] = np.dot(Q[:, i], A[:, j])
            v = v - R[i, j] * Q[:, i]
        R[j, j] = np.linalg.norm(v)
        Q[:, j] = v / R[j, j]
    return Q, R

def naive_matrix_multiplication(A, B):
    # (Код этой функции представлен в p20)
    # ...
    m, n = A.shape
    n_b, p = B.shape
    C = np.zeros((m, p))
    for i in range(m):
        for j in range(p):
            for k in range(n):
                C[i, j] += A[i, k] * B[k, j]
    return C

def schur_decomposition_manual(A, max_iter=1000, tol=1e-9):
    \"\"\"
    Находит разложение Шура A = U*T*U^T с помощью QR-алгоритма.
    \"\"\"
    A_k = np.copy(A).astype(float)
    n = A_k.shape[0]
    U = np.eye(n) # Начинаем с единичной матрицы

    for _ in range(max_iter):
        Q, R = _gram_schmidt_qr(A_k)
        A_k = naive_matrix_multiplication(R, Q)
        U = naive_matrix_multiplication(U, Q) # Накапливаем преобразования

        off_diagonal_sum = 0
        for i in range(n):
            for j in range(i):
                off_diagonal_sum += abs(A_k[i, j])

        if off_diagonal_sum < tol:
            break

    T = A_k
    return T, U

# Пример использования:
# A = np.array([[4, 1, 1], [1, 3, 2], [1, 2, 6]], dtype=float)
# T, U = schur_decomposition_manual(A)
#
# print("Верхнетреугольная матрица Шура (T):")
# print(T)
# print("\\nОртогональная матрица (U):")
# print(U)
#
# # Проверка: A должно быть примерно равно U @ T @ U.T
# print("\\nПроверка A ~= U*T*U.T:")
# print(U @ T @ U.T)"""
    return theory + code

## 22. QR-разложение (QR-декомпозиция)
def p22():
    theory = """
## 22. QR-разложение (QR-декомпозиция)

### Теоретическое описание:
QR-разложение — это представление матрицы `A` в виде произведения ортогональной (или унитарной) матрицы `Q` и верхнетреугольной матрицы `R`.
`A = Q * R`

**Основная идея:**
Процесс построения разложения заключается в последовательном занулении поддиагональных элементов матрицы `A` с помощью ортогональных преобразований. Наиболее популярные методы для этого:
1.  **Процесс ортогонализации Грама-Шмидта:** Столбцы матрицы `A` рассматриваются как система векторов. Эта система преобразуется в ортонормированную систему векторов, которые становятся столбцами матрицы `Q`.
2.  **Вращения Гивенса:** Последовательное зануление отдельных поддиагональных элементов с помощью матриц вращения.
3.  **Отражения Хаусхолдера:** Зануление целых столбцов под диагональю с помощью матриц отражения. Этот метод наиболее эффективен и численно устойчив на практике.

**Свойства:**
-   Любая вещественная матрица с линейно независимыми столбцами имеет единственное QR-разложение (с положительными диагональными элементами у `R`).
-   QR-разложение является основой для QR-алгоритма поиска собственных значений.
-   Широко используется для решения СЛАУ и задач наименьших квадратов.
"""
    code = """
### Код реализации (методом Грама-Шмидта):
```python
import numpy as np

def gram_schmidt_qr(A):
    \"\"\"
    QR-разложение матрицы A методом Грама-Шмидта.
    \"\"\"
    m, n = A.shape
    Q = np.zeros((m, n))
    R = np.zeros((n, n))

    for j in range(n):
        v = A[:, j]
        for i in range(j):
            # R_ij = q_i^T * a_j
            R[i, j] = np.dot(Q[:, i], A[:, j])
            # v = v - R_ij * q_i
            v = v - R[i, j] * Q[:, i]

        # R_jj = ||v||
        R[j, j] = np.linalg.norm(v)
        # q_j = v / R_jj
        Q[:, j] = v / R[j, j]

    return Q, R

# Пример использования:
# A = np.array([[1, 1, 0], [1, 0, 1], [0, 1, 1]], dtype=float)
# Q, R = gram_schmidt_qr(A)
#
# print("Матрица Q (ортогональная):")
# print(Q)
# print("\\nМатрица R (верхнетреугольная):")
# print(R)
#
# # Проверка: A должно быть равно Q @ R
# print("\\nПроверка Q @ R:")
# print(Q @ R)"""
    return theory + code

def p23():
    theory = """
## 23. Метод Эйлера (явный)

### Теоретическое описание:
Это простейший численный метод для решения обыкновенных дифференциальных уравнений (ОДУ) с начальным условием (задачи Коши).
`y' = f(x, y)`, `y(x0) = y0`

**Основная идея:**
На каждом шаге `h` мы аппроксимируем решение, двигаясь по касательной к графику функции в текущей точке. Наклон касательной равен значению производной `f(x, y)`.

Итерационная формула:
`y_{n+1} = y_n + h * f(x_n, y_n)`

**Геометрическая интерпретация:**
Решение аппроксимируется ломаной линией, где каждый сегмент является отрезком касательной к истинному решению.

**Преимущества:**
-   Крайне прост в реализации.

**Недостатки:**
-   **Низкая точность:** Метод имеет первый порядок точности, ошибка на каждом шаге пропорциональна `h^2`, а глобальная ошибка — `h`.
-   **Условная устойчивость:** Для многих задач (например, `y' = -ky`) метод устойчив только при достаточно малом шаге `h`. При большом `h` численное решение может неограниченно расти, даже если аналитическое затухает.
"""
    code = """
### Код реализации (на чистом Python):
```python
def euler_method_python(f, y0, t0, t_end, h):
    '''
    Явный метод Эйлера для решения ОДУ y' = f(t, y).
    '''
    t_values = [t0]
    y_values = [y0]

    t = t0
    y = y0

    while t < t_end:
        y = y + h * f(t, y)
        t = t + h
        t_values.append(t)
        y_values.append(y)

    return t_values, y_values

# Пример использования:
# Решим y' = -y, y(0) = 1
# f = lambda t, y: -y
#
# t_vals, y_vals = euler_method_python(f, 1.0, 0, 5, 0.1)
#
# import matplotlib.pyplot as plt
# plt.plot(t_vals, y_vals, 'b-o', label='Метод Эйлера')
# plt.plot(t_vals, [math.exp(-t) for t in t_vals], 'r--', label='Точное решение')
# plt.legend()
# plt.show()"""
    return theory + code

def p24():
    theory = """
## 24. Метод предиктора-корректора (улучшенный метод Эйлера, метод Хойна)

### Теоретическое описание:
Это метод второго порядка точности, который улучшает простой метод Эйлера за счет двухэтапного вычисления на каждом шаге.

**Основная идея:**
1.  **Предиктор (прогноз):** Сначала делается "грубый" шаг методом Эйлера для оценки значения в следующей точке `y*_{n+1}`.
    `y*_{n+1} = y_n + h * f(x_n, y_n)`
2.  **Корректор (уточнение):** Затем вычисляется средний наклон на отрезке `[x_n, x_{n+1}]` как среднее арифметическое наклонов в начальной точке (`f_n`) и в "предсказанной" конечной точке (`f*_n+1`). Это среднее значение используется для окончательного шага.
    `y_{n+1} = y_n + (h/2) * (f(x_n, y_n) + f(x_{n+1}, y*_{n+1}))`

**Преимущества:**
-   **Выше точность:** Второй порядок точности (глобальная ошибка O(h²)), что значительно точнее простого метода Эйлера.
-   **Лучше устойчивость:** Область устойчивости метода больше, чем у явного метода Эйлера.

**Недостатки:**
-   Требует двух вычислений функции `f` на каждом шаге, что вдвое затратнее метода Эйлера.
"""
    code = """
### Код реализации (на чистом Python):
```python
def euler_predictor_corrector(f, y0, t0, t_end, h):
    \"\"\"
    Метод предиктора-корректора Эйлера (метод Хойна).
    \"\"\"
    t_values = [t0]
    y_values = [y0]

    t = t0
    y = y0

    while t < t_end:
        # Предиктор
        f_n = f(t, y)
        y_star = y + h * f_n

        # Корректор
        f_n_plus_1_star = f(t + h, y_star)
        y = y + (h / 2.0) * (f_n + f_n_plus_1_star)

        t = t + h
        t_values.append(t)
        y_values.append(y)

    return t_values, y_values

# Пример использования:
# Решим y' = -y, y(0) = 1
# f = lambda t, y: -y
#
# t_vals_pc, y_vals_pc = euler_predictor_corrector(f, 1.0, 0, 5, 0.1)
#
# import matplotlib.pyplot as plt
# plt.plot(t_vals_pc, y_vals_pc, 'b-o', label='Предиктор-корректор')
# plt.plot(t_vals_pc, [math.exp(-t) for t in t_vals_pc], 'r--', label='Точное решение')
# plt.legend()
# plt.show()"""
    return theory + code

def p25():
    theory = """
## 25. Метод Рунге-Кутты 4-го порядка (РК4)

### Теоретическое описание:
Это один из самых популярных и широко используемых численных методов для решения обыкновенных дифференциальных уравнений (ОДУ) вида `y' = f(x, y)` с начальным условием `y(x0) = y0`. Он относится к классу одношаговых явных методов.

**Основная идея:**
Вместо одного вычисления наклона (как в методе Эйлера), РК4 использует четыре "пробных" вычисления наклона внутри каждого шага `h` для получения более точной аппроксимации. Эти наклоны затем взвешиваются и усредняются.

**Алгоритм:**
Для перехода от точки `(x_n, y_n)` к `(x_{n+1}, y_{n+1})` вычисляются четыре коэффициента:
`k1 = f(x_n, y_n)`
`k2 = f(x_n + h/2, y_n + h*k1/2)`
`k3 = f(x_n + h/2, y_n + h*k2/2)`
`k4 = f(x_n + h, y_n + h*k3)`

Затем вычисляется следующее значение `y_{n+1}`:
`y_{n+1} = y_n + (h/6) * (k1 + 2*k2 + 2*k3 + k4)`

**Преимущества:**
-   Высокая точность (четвертый порядок), что позволяет использовать больший шаг `h` по сравнению с методами низшего порядка при той же точности.
-   Численная устойчивость для широкого круга задач.

**Недостатки:**
-   Требует четыре вычисления правой части `f(x, y)` на каждом шаге, что может быть затратно, если функция `f` сложная.
"""
    code = """
### Код реализации (на чистом Python):
```python
def runge_kutta_4(f, y0, t_span, h):
    '''
    Метод Рунге-Кутты 4-го порядка для решения ОДУ y' = f(t, y).

    Параметры:
        f (function): Функция правой части ОДУ, f(t, y).
        y0 (float): Начальное условие.
        t_span (tuple): Интервал времени (t_start, t_end).
        h (float): Шаг интегрирования.
    '''
    t_values = [t_span[0]]
    y_values = [y0]

    t = t_span[0]
    y = y0

    while t < t_span[1]:
        k1 = f(t, y)
        k2 = f(t + h/2, y + h*k1/2)
        k3 = f(t + h/2, y + h*k2/2)
        k4 = f(t + h, y + h*k3)

        y = y + (h/6) * (k1 + 2*k2 + 2*k3 + k4)
        t = t + h

        t_values.append(t)
        y_values.append(y)

    return t_values, y_values

# Пример использования:
# Решим y' = y, y(0) = 1. Точное решение y(t) = e^t.
# f = lambda t, y: y
# t, y = runge_kutta_4(f, 1.0, (0, 1), 0.1)
# print(f"Приближенное значение в t=1: {y[-1]}")
# print(f"Точное значение в t=1: {2.71828}")"""
    return theory + code

def p26():
    theory = """
## 26. Методы Адамса-Башфорта

### Теоретическое описание:
Это семейство явных многошаговых методов для решения ОДУ. В отличие от одношаговых методов (как Рунге-Кутта), они используют информацию из нескольких предыдущих точек для вычисления следующей.

**Основная идея:**
Метод основан на аппроксимации функции `f(x,y)` в интегральной форме уравнения `y_{n+1} = y_n + ∫[x_n, x_{n+1}] f(x, y(x)) dx` с помощью интерполяционного полинома, построенного по уже известным точкам `(x_{n-k}, f_{n-k})`.

**Формулы (для равноотстоящих узлов с шагом h):**
- **2-шаговый:** `y_{n+1} = y_n + h/2 * (3*f_n - f_{n-1})`
- **3-шаговый:** `y_{n+1} = y_n + h/12 * (23*f_n - 16*f_{n-1} + 5*f_{n-2})`
- **4-шаговый:** `y_{n+1} = y_n + h/24 * (55*f_n - 59*f_{n-1} + 37*f_{n-2} - 9*f_{n-3})`
где `f_k = f(x_k, y_k)`.

**Проблема "старта":**
Для k-шагового метода необходимо знать `k` начальных значений `y_0, y_1, ..., y_{k-1}`. Поскольку дано только `y_0`, остальные "стартовые" значения обычно вычисляются другим методом, например, Рунге-Кутты.

**Преимущества:**
-   Вычислительно эффективны: требуется только одно вычисление `f(x, y)` на шаге.

**Недостатки:**
-   Проблема "старта".
-   Сложнее изменять шаг интегрирования в процессе вычислений.
-   Меньшая область устойчивости по сравнению с неявными методами.
"""
    code = """
### Код реализации (4-шаговый метод):
```python
def adams_bashforth_4(f, y0, t_span, h):
    '''
    4-шаговый метод Адамса-Башфорта.
    '''
    num_steps = int((t_span[1] - t_span[0]) / h)
    t = [t_span[0] + i*h for i in range(num_steps + 1)]
    y = [0.0] * (num_steps + 1)
    y[0] = y0

    # "Старт" с помощью РК4 для первых 3-х шагов
    for i in range(3):
        k1 = f(t[i], y[i])
        k2 = f(t[i] + h/2, y[i] + h*k1/2)
        k3 = f(t[i] + h/2, y[i] + h*k2/2)
        k4 = f(t[i] + h, y[i] + h*k3)
        y[i+1] = y[i] + (h/6) * (k1 + 2*k2 + 2*k3 + k4)

    # Основной цикл Адамса-Башфорта
    for i in range(3, num_steps):
        f_im3 = f(t[i-3], y[i-3])
        f_im2 = f(t[i-2], y[i-2])
        f_im1 = f(t[i-1], y[i-1])
        f_i   = f(t[i], y[i])

        y[i+1] = y[i] + (h/24) * (55*f_i - 59*f_im1 + 37*f_im2 - 9*f_im3)

    return t, y

# Пример использования:
# f = lambda t, y: y
# t, y = adams_bashforth_4(f, 1.0, (0, 1), 0.01)
# print(f"Приближенное значение в t=1: {y[-1]}")"""
    return theory + code

def p27():
    theory = """
## 27. Методы Адамса-Мултона

### Теоретическое описание:
Это семейство **неявных** многошаговых методов. Они более устойчивы, чем явные методы Адамса-Башфорта, но требуют решения уравнения на каждом шаге.

**Основная идея:**
Как и в методах Башфорта, используется аппроксимация интеграла `∫f(x,y)dx`. Однако интерполяционный полином строится по точкам, включающим и будущую (еще неизвестную) точку `(x_{n+1}, f_{n+1})`. Это делает метод неявным.

**Формулы (для равноотстоящих узлов с шагом h):**
- **2-шаговый (метод трапеций):** `y_{n+1} = y_n + h/2 * (f_{n+1} + f_n)`
- **3-шаговый:** `y_{n+1} = y_n + h/12 * (5*f_{n+1} + 8*f_n - f_{n-1})`
- **4-шаговый:** `y_{n+1} = y_n + h/24 * (9*f_{n+1} + 19*f_n - 5*f_{n-1} + f_{n-2})`

**Методы "Предиктор-Корректор":**
Поскольку методы неявные, для нахождения `y_{n+1}` (которое входит в `f_{n+1}`) нужно решать уравнение. На практике это делают с помощью схемы "предиктор-корректор":
1.  **Предиктор:** Явным методом (например, Адамса-Башфорта) вычисляется предварительное значение `y*_{n+1}`.
2.  **Корректор:** Это значение `y*_{n+1}` подставляется в правую часть формулы Адамса-Мултона для уточнения (коррекции) значения `y_{n+1}`.
`y_{n+1} = y_n + h/24 * (9*f(x_{n+1}, y*_{n+1}) + 19*f_n - 5*f_{n-1} + f_{n-2})`

**Преимущества:**
-   Более высокая точность и лучшая устойчивость по сравнению с явными аналогами того же порядка.
"""
    code = """
### Код реализации (4-шаговый предиктор-корректор):
```python
def adams_predictor_corrector_4(f, y0, t_span, h):
    '''
    4-шаговый метод Адамса (Башфорт - предиктор, Мултон - корректор).
    '''
    num_steps = int((t_span[1] - t_span[0]) / h)
    t = [t_span[0] + i*h for i in range(num_steps + 1)]
    y = [0.0] * (num_steps + 1)
    y[0] = y0

    # Старт с помощью РК4
    for i in range(3):
        k1 = f(t[i], y[i])
        k2 = f(t[i] + h/2, y[i] + h*k1/2)
        k3 = f(t[i] + h/2, y[i] + h*k2/2)
        k4 = f(t[i] + h, y[i] + h*k3)
        y[i+1] = y[i] + (h/6) * (k1 + 2*k2 + 2*k3 + k4)

    for i in range(3, num_steps):
        f_im3 = f(t[i-3], y[i-3])
        f_im2 = f(t[i-2], y[i-2])
        f_im1 = f(t[i-1], y[i-1])
        f_i   = f(t[i], y[i])

        # Предиктор (Адамс-Башфорт 4)
        y_pred = y[i] + (h/24) * (55*f_i - 59*f_im1 + 37*f_im2 - 9*f_im3)

        # Корректор (Адамс-Мултон 3 - требует 3 точки)
        f_pred = f(t[i+1], y_pred)
        y[i+1] = y[i] + (h/24) * (9*f_pred + 19*f_i - 5*f_im1 + f_im2)

    return t, y

# Пример использования:
# f = lambda t, y: y
# t, y = adams_predictor_corrector_4(f, 1.0, (0, 1), 0.01)
# print(f"Приближенное значение в t=1: {y[-1]}")"""
    return theory + code

def p28():
    theory = """
## 28. Дискретное преобразование Фурье (ДПФ) и обратное ДПФ

### Теоретическое описание:
ДПФ является аналогом непрерывного преобразования Фурье для дискретных сигналов (последовательностей чисел). Оно преобразует сигнал из временной/пространственной области в частотную.

**Основная идея:**
ДПФ разлагает конечную последовательность из `N` отсчетов `x[n]` на `N` комплексных экспонент (синусоид и косинусоид) с различными частотами.

**Прямое ДПФ:**
Вычисляет `N` комплексных коэффициентов `X[k]`, которые представляют амплитуду и фазу каждой частотной компоненты:
`X[k] = sum_{n=0 to N-1} x[n] * exp(-j * 2 * pi * k * n / N)`
где `k = 0, 1, ..., N-1`, а `j` — мнимая единица.

**Обратное ДПФ (ОДПФ):**
Позволяет восстановить исходный сигнал из его частотного представления:
`x[n] = (1/N) * sum_{k=0 to N-1} X[k] * exp(j * 2 * pi * k * n / N)`

**Свойства:**
-   **Периодичность:** Спектр, полученный с помощью ДПФ, является периодическим с периодом N.
-   **Линейность, сдвиг, свёртка:** Обладает свойствами, аналогичными непрерывному преобразованию, но в дискретной форме (например, круговая свёртка вместо линейной).

**Недостатки:**
-   Прямое вычисление "в лоб" требует O(N²) комплексных умножений и сложений, что очень медленно для больших `N`.
"""
    code = """
### Код реализации (на чистом Python):
```python
import cmath

def dft(x):
    \"\"\"
    Дискретное преобразование Фурье.
    \"\"\"
    N = len(x)
    X = [0] * N
    for k in range(N):
        sum_val = 0
        for n in range(N):
            angle = -2 * cmath.pi * k * n / N
            sum_val += x[n] * cmath.exp(1j * angle)
        X[k] = sum_val
    return X

def idft(X):
    \"\"\"
    Обратное дискретное преобразование Фурье.
    \"\"\"
    N = len(X)
    x = [0] * N
    for n in range(N):
        sum_val = 0
        for k in range(N):
            angle = 2 * cmath.pi * k * n / N
            sum_val += X[k] * cmath.exp(1j * angle)
        x[n] = sum_val / N
    return x

# Пример использования:
# signal = [0, 1, 2, 3]
# dft_coeffs = dft(signal)
# reconstructed_signal = idft(dft_coeffs)
# print("Коэффициенты ДПФ:", dft_coeffs)
# print("Восстановленный сигнал:", reconstructed_signal) # будут комплексные числа
# print("Действительная часть:", [val.real for val in reconstructed_signal])"""
    return theory + code

def p29():
    theory = """
## 29. Быстрое преобразование Фурье (БПФ)

### Теоретическое описание:
Быстрое преобразование Фурье — это не новое преобразование, а набор высокоэффективных алгоритмов для вычисления ДПФ. Самый известный из них — алгоритм Кули-Тьюки.

**Основная идея (алгоритм Кули-Тьюки с прореживанием по времени):**
1.  **Разделяй и властвуй:** Если длина входного сигнала `N` является степенью двойки, его можно разделить на два подсигнала: один из четных отсчетов, другой — из нечетных.
2.  **Рекурсия:** ДПФ вычисляется рекурсивно для каждого из этих подсигналов (длиной N/2).
3.  **Объединение:** Результаты двух ДПФ меньшего размера объединяются с помощью умножения на так называемые "поворотные множители" (`exp(-j*2*pi*k/N)`) для получения итогового ДПФ.

Формула объединения:
`X[k] = E[k] + exp(-j*2*pi*k/N) * O[k]`
`X[k + N/2] = E[k] - exp(-j*2*pi*k/N) * O[k]`
где `E[k]` и `O[k]` — ДПФ четных и нечетных отсчетов соответственно.

**Сложность:**
-   **Временная сложность:** O(N log N). Это значительно быстрее, чем O(N²) у прямого ДПФ, и делает возможным спектральный анализ больших массивов данных.

**Применение:**
-   Цифровая обработка сигналов (аудио, видео, изображения).
-   Анализ временных рядов в финансах и науке.
-   Решение дифференциальных уравнений в частных производных.
"""
    code = """
### Код реализации (рекурсивный алгоритм Кули-Тьюки):
```python
import cmath

def fft(x):
    \"\"\"
    Рекурсивная реализация БПФ (алгоритм Кули-Тьюки).
    Длина входного списка должна быть степенью двойки.
    \"\"\"
    N = len(x)

    # Базовый случай рекурсии
    if N <= 1:
        return x

    # Разделение на четные и нечетные отсчеты
    even = fft(x[0::2])
    odd =  fft(x[1::2])

    # Объединение
    T = [cmath.exp(-2j*cmath.pi*k/N) * odd[k] for k in range(N//2)]

    X = [0] * N
    for k in range(N//2):
        X[k] = even[k] + T[k]
        X[k+N//2] = even[k] - T[k]

    return X

def ifft(X):
    \"\"\"
    Обратное БПФ на основе прямого БПФ.
    \"\"\"
    N = len(X)
    # Сначала найдем сопряженные значения
    X_conj = [val.conjugate() for val in X]

    # Применим прямое БПФ
    x_conj = fft(X_conj)

    # Снова сопряжем и разделим на N
    x = [val.conjugate() / N for val in x_conj]

    return x

# Пример использования:
# signal = [0, 1, 2, 3, 4, 5, 6, 7] # 8 точек (2^3)
# fft_coeffs = fft(signal)
# reconstructed_signal = ifft(fft_coeffs)
# print("Коэффициенты БПФ:", fft_coeffs)
# print("Восстановленный сигнал:", reconstructed_signal)"""
    return theory + code

def info1():
    questions = [
        "1. Метод половинного деления (бисекции) - p1",
        "2. Методы функциональной итерации - p2",
        "3. Метод хорд (секущих) - p3",
        "4. Метод Ньютона (касательных) - p4",
        "5. Модифицированный метод Ньютона - p5",
        "6. Метод дихотомии - p6",
        "7. Метод функциональной итерации для решения систем нелинейных уравнений - p7",
        "8. Метод Гаусса–Зейделя - p8",
        "9. Метод Ньютона в двумерном случае - p9",
        "10. Модифицированный метод Ньютона в двумерном случае - p10",
        "11. Линейная интерполяция - p11",
        "12. Интерполяционный многочлен Лагранжа - p12",
        "13. Кубическая сплайн-интерполяция - p13",
        "14. Наивный алгоритм перемножения матриц - p14",
        "15. Алгоритм Штрассена - p15",
        "16. Вычисление собственных значений с помощью характеристического многочлена - p16",
        "17. Степенной метод - p17",
        "18. Степенной метод со сдвигами - p18",
        "19. Метод вращений - p19",
        "20. QR алгоритм - p20",
        "21. Разложение Шура, теорема Шура - p21",
        "22. QR разложение - p22",
        "23. Метод Эйлера - p23",
        "24. Метод предиктора-корректора Эйлера - p24",
        "25. Метод Рунге-Кутты 4-го порядка - p25",
        "26. Методы Адамса-Башфорта - p26",
        "27. Методы Адамса-Мултона - p27",
        "28. Дискретное преобразование и обратное дискретное преобразование Фурье - p28",
        "29. Быстрое преобразование Фурье - p29"
    ]
    return questions

"""# Теория"""

def t1():
    """
    Ответ на вопрос:
    - Работа с числами с плавающей точкой
    """

    answer = """
    Ответ на вопрос:
    - Работа с числами с плавающей точкой

    Работа с числами с плавающей точкой в компьютере принципиально отличается от идеальных математических вычислений из-за ограниченной точности представления чисел.

    В отличие от вычислений "вручную", где можно использовать любую степень точности, в компьютере числа хранятся в ячейках памяти с фиксированной разрядностью (например, float16, float32, float64). Это накладывает ограничения как на диапазон представляемых значений, так и на их точность. Например, для стандартного 64-битного числа (double precision) диапазон значений ограничен: 10⁻³⁰⁷ < |x| < 10³⁰⁷, и оно хранит не более 15-16 значащих цифр.

    Это приводит к тому, что при выполнении арифметических операций производятся округления, вносящие так называемую вычислительную погрешность. При большом количестве операций эти малые ошибки могут накапливаться, приводя к значительным отклонениям в конечном результате. Таким образом, компьютерная арифметика, связанная с представлением чисел в CPU/GPU, отличается от обычной, что необходимо учитывать при разработке и анализе численных методов.

    ---
    *Информация для ответа взята со слайдов 15, 18, 23, 118.*
    """
    return answer


def t2():
    """
    Ответ на вопрос:
    - Архитектура памяти.
    """

    answer = """
    Современные вычислительные системы используют иерархическую архитектуру памяти для баланса между скоростью, стоимостью и объёмом. Эта иерархия включает в себя несколько уровней: от самых быстрых и малых (регистры, кэш-память L1, L2, L3) до более медленных и объёмных (оперативная память RAM) и, наконец, до самого медленного, но и самого большого по объёму хранилища (SSD/HDD). Вычисления могут производиться только с данными, находящимися на вершине иерархии (в регистрах и кэше).

    Производительность численных методов напрямую зависит от того, насколько эффективно используется эта иерархия. Ключевым фактором является соотношение числа вычислительных операций (flops) к числу обращений к памяти. Библиотеки, такие как BLAS, оптимизированы для максимизации этого соотношения. Например, операции BLAS Уровня 3 (матрично-матричные) выполняют O(n³) вычислений при O(n²) обращений к памяти, что позволяет эффективно использовать быстрый кэш и минимизировать дорогостоящие обращения к медленной оперативной памяти. Этот принцип, известный как "принцип локальности", критически важен для достижения высокой производительности в численных расчетах.

    ---
    *Информация для ответа взята со слайдов 237-240. На слайде 239 приведена наглядная диаграмма иерархии памяти.*
    """
    return answer

def t3():
    """
    Ответ на вопрос:
    - Переполнение (overflow), потеря точности (underflow).
    """

    answer = """
    Переполнение (overflow) и потеря точности (underflow) — это ошибки, возникающие, когда результат арифметической операции выходит за пределы диапазона, представимого типом данных с плавающей точкой.

    **Переполнение (Overflow)** происходит, когда абсолютное значение результата превышает максимально возможное число для данного формата. Например, для 64-битного числа это происходит, если |x| > 10³⁰⁷. Такая операция обычно возвращает специальное значение, такое как бесконечность (Inf), что приводит к некорректной работе алгоритма.

    **Потеря точности (Underflow)** происходит, когда результат операции по абсолютной величине оказывается меньше минимального положительного числа, которое можно представить в данном формате (например, |x| < 10⁻³⁰⁷). В этом случае результат обычно округляется до нуля. Это может привести к потере значимой информации и последующим ошибкам, например, к делению на ноль. Оба этих явления являются следствием ограниченной разрядности компьютерной арифметики.

    ---
    *Информация для ответа взята со слайда 15.*
    """
    return answer

def t4():
    """
    Ответ на вопрос:
    - Ошибка округления в арифметике с плавающей точкой, накопление ошибок округления, потеря значимости.
    """

    answer = """
    **Ошибка округления** — это погрешность, возникающая из-за того, что результат арифметической операции не может быть точно представлен в памяти компьютера с фиксированной разрядностью и округляется до ближайшего доступного значения.

    **Накопление ошибок округления** происходит в алгоритмах, содержащих миллионы операций. Даже очень малые ошибки округления на каждом шаге могут суммироваться и приводить к значительной погрешности в конечном результате. Этот эффект особенно опасен в итерационных методах.

    **Потеря значимости** — это катастрофическое увеличение относительной погрешности при вычитании двух близких по значению чисел. При такой операции старшие разряды чисел взаимно уничтожаются, и результат определяется младшими, менее точными разрядами, которые могут содержать накопленную ошибку. Из-за этого относительная погрешность результата может стать очень большой, что делает его недостоверным. Эта проблема особенно видна из формулы предельной относительной погрешности разности: Δu = (Δx - Δy) / |x - y|, где знаменатель стремится к нулю.

    ---
    *Информация для ответа взята со слайдов 15, 18, 23, 33.*
    """
    return answer

def t5():
    """
    Ответ на вопрос:
    - Суммирование по Кахану.
    """

    answer = """
    Тема "Суммирование по Кахану" (Kahan summation) не рассматривается в предоставленном материале.

    Это специальный алгоритм для вычисления суммы последовательности чисел с плавающей точкой, который позволяет значительно уменьшить накопление вычислительной погрешности по сравнению с простым последовательным суммированием.
    """
    return answer

def t6():
    """
    Ответ на вопрос:
    - Абсолютная и относительная погрешности.
    """

    answer = """
    Абсолютная и относительная погрешности — это две основные меры для оценки точности приближённых вычислений.

    **Абсолютной погрешностью (Δ)** приближенного значения `ap` называется модуль разности между точным значением `a` и приближенным:
    Δ = |a - ap|
    Она показывает величину ошибки в тех же единицах, что и измеряемая величина, но не даёт представления о её значимости.

    **Относительной погрешностью (δ)** называется отношение абсолютной погрешности к модулю точного значения:
    δ = Δ / |a| = |a - ap| / |a|
    Эта безразмерная величина (часто выражается в процентах) лучше характеризует качество измерения, так как показывает ошибку относительно самой величины. На практике, так как точное значение `a` обычно неизвестно, используют предельную абсолютную (Δa) и относительную (δa) погрешности, которые являются гарантированной верхней оценкой для Δ и δ соответственно.

    ---
    *Информация для ответа взята со слайдов 20-22, 26.*
    """
    return answer

def t7():
    """
    Ответ на вопрос:
    - Округление и значащие цифры в записи приближенного числа.
    """

    answer = """
    **Значащими цифрами** в записи приближенного числа называются все ненулевые цифры, а также нули, содержащиеся между ними или являющиеся представителями сохраненных разрядов при округлении. Нули в начале числа (например, в `0.0357`) не являются значащими. Так, в числе `2.305` — 4 значащие цифры, в `0.0357` — 3 значащие цифры, а в `0.035300` (если это результат округления) — 5 значащих цифр.

    **Округление** числа до `n` значащих цифр — это замена его близким числом с меньшим количеством значащих цифр. Правила округления зависят от первой отбрасываемой цифры:
    1.  Если она меньше 5, последняя сохраняемая цифра не меняется.
    2.  Если она больше 5, последняя сохраняемая цифра увеличивается на единицу.
    3.  Если она равна 5 и за ней следуют ненулевые цифры, последняя цифра также увеличивается.
    4.  Если она равна 5, а за ней нули, используется **правило четной цифры**: последняя сохраняемая цифра округляется до ближайшего четного числа (например, 2.5 -> 2, 3.5 -> 4). Это делается для компенсации знаков ошибок при массовых вычислениях.

    ---
    *Информация для ответа взята со слайдов 24, 25.*
    """
    return answer

def t8():
    """
    Ответ на вопрос:
    - Верные в строгом (узком) смысле цифры числа, верные в широком смысле цифры числа.
    """

    answer = """
    Понятие "верной цифры" позволяет оценить точность приближенного числа, связывая его абсолютную погрешность с разрядом цифр.

    **Верная в строгом (узком) смысле** цифра — это цифра, для которой абсолютная погрешность числа не превосходит половины единицы разряда, в котором стоит эта цифра. Например, для числа `46.852 ± 0.007`, цифра 8 (разряд десятых) верна в строгом смысле, так как погрешность `0.007` больше, чем `0.005` (половина от разряда сотых `0.01/2`), но меньше, чем `0.05` (половина от разряда десятых `0.1/2`). Цифра 5 (разряд сотых) уже не будет верной, так как `0.007 > 0.005`.

    **Верная в широком смысле** цифра — это цифра, для которой абсолютная погрешность не превышает единицы разряда, в котором она стоит. Это менее строгое требование. Например, в числе `а = 2.91385` с погрешностью `Δa = 0.0097`, верными в широком смысле будут цифры 2, 9 и 1, так как погрешность меньше единицы разряда тысячных (0.01).

    Запись числа считается **правильной**, если все его цифры верны.

    ---
    *Информация для ответа взята со слайдов 27-29.*
    """
    return answer

def t9():
    """
    Ответ на вопрос:
    - Сложность алгоритмов и нотация big-O.
    """

    answer = """
    **Сложность алгоритма** — это зависимость количества ресурсов, требуемых алгоритмом (обычно времени выполнения или памяти), от размера входных данных `n`. Время выполнения оценивается путем подсчета "базовых операций" (сложение, умножение, сравнение и т.д.).

    **Нотация Big-O (O-нотация)** используется для асимптотического анализа сложности, то есть для описания поведения алгоритма при больших `n` (когда `n → ∞`). Она позволяет абстрагироваться от конкретного "железа" и деталей реализации. Big-O описывает верхнюю границу роста сложности, отбрасывая константные множители и члены низшего порядка. Например, если точное число операций T(n) = 4n² + 2n + 2, то в нотации Big-O сложность записывается как O(n²), так как при больших `n` член n² доминирует.

    Распространенные классы сложности: O(1) – константная, O(log n) – логарифмическая, O(n) – линейная, O(n log n), O(n²) – квадратичная, O(2ⁿ) – экспоненциальная. Алгоритмы с полиномиальной сложностью (например, O(n^c)) считаются эффективными, в то время как экспоненциальная сложность делает алгоритм непрактичным для больших данных.

    ---
    *Информация для ответа взята со слайдов 37, 41-55.*


    # Код для графика со слайда 52
    n = np.linspace(1, 10, 400)
    log_n = np.log(n)
    lin_n = n
    quad_n = n**2

    plt.figure(figsize=(8, 6))
    plt.plot(n, quad_n, 'r-', label='O(n²)', linewidth=3)
    plt.plot(n, lin_n, 'g-', label='O(n)', linewidth=3)
    plt.plot(n, log_n, 'b-', label='O(log n)', linewidth=3)

    plt.xlabel('Input size', fontsize=16)
    plt.ylabel('Time', fontsize=16)
    plt.title('Algorithm Complexity Comparison', fontsize=18)
    plt.legend(fontsize=16, frameon=False)
    plt.ylim(0, 20)
    plt.xlim(0, 10)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)

    # Добавляем текст прямо на график
    plt.text(5, 18, 'O(n²)', fontsize=16, color='red')
    plt.text(8, 9, 'O(n)', fontsize=16, color='green')
    plt.text(7, 3, 'O(log n)', fontsize=16, color='blue')

    plt.show()"""

    return answer

def t10():
    """
    Ответ на вопрос:
    - Профилирование кода в Python.
    """

    answer = """
    Тема "Профилирование кода в Python" напрямую не раскрывается в теоретической части презентации, но упоминается в контексте домашнего задания.

    **Профилирование кода** — это процесс анализа программы для определения времени выполнения и частоты вызовов её отдельных частей (функций, строк кода). Цель профилирования — найти "узкие места" (bottlenecks), то есть участки кода, которые потребляют больше всего времени или других ресурсов, и оптимизировать их.

    В Python для этого существуют различные инструменты. В домашнем задании №1 упоминается **Python Line Profiler** (`line_profiler`). Этот инструмент позволяет получить детализированный отчет о времени, затраченном на выполнение каждой строки внутри функции. Это помогает точно определить, какие именно операции являются самыми затратными, и сосредоточить усилия по оптимизации на них, что критически важно для повышения производительности численных методов.

    ---
    *Информация для ответа взята из постановки домашнего задания на слайде 118.*
    """
    return answer

def t11():
    """
    Ответ на вопрос:
    - Представление чисел с плавающей точкой (стандарт IEEE 754), ошибки представления.
    """

    answer = """
    Тема "Стандарт IEEE 754" напрямую не раскрывается в предоставленном материале, но его концепции лежат в основе обсуждаемых проблем.

    **Стандарт IEEE 754** — это наиболее распространенный технический стандарт для представления чисел с плавающей точкой в компьютерах. Число представляется в виде трёх частей: **знак**, **порядок (экспонента)** и **мантисса**. Такая структура позволяет представлять очень большие и очень маленькие числа в компактной форме.

    **Ошибки представления** возникают из-за того, что не все вещественные числа могут быть точно представлены в этом формате. Это связано с двумя основными причинами:
    1.  **Ограниченная разрядность**: Мантисса и экспонента имеют конечное число бит, что ограничивает точность и диапазон чисел. Это приводит к **ошибкам округления**, когда число округляется до ближайшего представимого значения.
    2.  **Двоичная система**: Некоторые конечные десятичные дроби (например, 0.1) становятся бесконечными периодическими дробями в двоичной системе, и их точное представление невозможно.

    Эти ошибки представления являются источником вычислительных погрешностей, переполнения (overflow) и потери значимости (underflow), которые необходимо учитывать при работе с численными методами.

    ---
    *Ответ построен на основе общей информации и данных со слайдов 15, 18.*
    """
    return answer

def t12():
    """
    Ответ на вопрос:
    - Способы изолирования корней нелинейных функций.
    """

    answer = """
    Изолирование (или отделение) корня нелинейного уравнения `f(x) = 0` — это первый этап его численного решения, заключающийся в нахождении отрезка `[a, b]`, на котором содержится ровно один корень. Существует два основных способа для этого.

    1.  **Аналитический способ**: Основан на теореме о промежуточном значении. Если функция `f(x)` непрерывна на отрезке `[a, b]` и принимает на его концах значения разных знаков (т.е. `f(a) * f(b) < 0`), то на этом отрезке существует хотя бы один корень. Если дополнительно функция `f(x)` монотонна на `[a, b]` (т.е. ее производная `f'(x)` сохраняет знак), то корень на этом отрезке единственный.

    2.  **Графический способ**: Этот метод часто более нагляден. Можно построить график функции `y = f(x)` и визуально определить отрезки, где график пересекает ось Ox. Для упрощения задачи исходное уравнение `f(x) = 0` часто преобразуют к виду `f1(x) = f2(x)`, где `f1` и `f2` — более простые функции. Тогда абсциссы точек пересечения их графиков и будут искомыми корнями.

    ---
    *Информация для ответа взята со слайдов 60-63, 67, 69-71.*


    # Код для графика со слайда 67
    x = np.linspace(0.1, 5, 400)
    y1 = 1/x
    y2 = np.log10(x)

    plt.figure(figsize=(7, 5))
    plt.plot(x, y1, label='y = 1/x')
    plt.plot(x, y2, label='y = lg x')
    plt.axhline(0, color='black', linewidth=0.5)
    plt.axvline(0, color='black', linewidth=0.5)
    plt.title("Графическое отделение корня уравнения 1/x = lg(x)")
    plt.xlabel('x')
    plt.ylabel('y')
    plt.ylim(-1, 2)
    plt.grid(True)
    plt.legend()
    plt.show()"""

    return answer

def t13():
    """
    Ответ на вопрос:
    - Сжимающие отображения.
    """

    answer = """
    Тема "Сжимающие отображения" напрямую не названа в презентации, но ее концепция является теоретической основой метода простой итерации для решения уравнений вида `x = φ(x)`.

    Отображение `φ(x)` называется **сжимающим** на отрезке `[a, b]`, если для любых двух точек `x1` и `x2` из этого отрезка выполняется условие:
    `|φ(x1) - φ(x2)| ≤ q * |x1 - x2|`, где `0 ≤ q < 1`.
    Константа `q` называется коэффициентом сжатия.

    Если функция `φ(x)` дифференцируема, то это условие эквивалентно требованию `|φ'(x)| ≤ q < 1` для всех `x` на отрезке. Именно это условие `|φ'(x)| < 1` является ключевым для сходимости метода простой итерации. Оно гарантирует, что на каждом шаге итерации `xk+1 = φ(xk)` расстояние между последовательными приближениями будет уменьшаться, и последовательность `xk` будет сходиться к единственной неподвижной точке (корню уравнения) внутри отрезка.

    ---
    *Информация для ответа взята со слайдов 79, 81, 82, 87.*
    """
    return answer

def t14():
    """
    Ответ на вопрос:
    - Погрешность и критерии сходимости, константа Липшица.
    """

    answer = """
    При решении нелинейных уравнений итерационными методами ключевыми являются понятия погрешности и критериев сходимости.

    **Погрешность** — это отклонение текущего приближения от истинного корня. Итерационный процесс считается **сходящимся**, если эта погрешность стремится к нулю. На практике, поскольку истинный корень неизвестен, сходимость оценивают по разности между двумя последовательными приближениями. Критерием остановки (сходимости) обычно служит условие `|xk - xk-1| < ε`, где `ε` — заданная точность.

    Для метода простой итерации `x = φ(x)` основной **критерий сходимости** связан с **константой Липшица**. Функция `φ(x)` удовлетворяет условию Липшица с константой `L`, если `|φ(x1) - φ(x2)| ≤ L * |x1 - x2|`. Если `L < 1`, то отображение является сжимающим, и метод сходится. Для дифференцируемой функции `φ(x)` роль константы Липшица играет максимум модуля ее производной, и условие сходимости упрощается до `max|φ'(x)| < 1` на интервале изоляции корня.

    ---
    *Информация для ответа взята со слайдов 79-82.*
    """
    return answer

def t15():
    """
    Ответ на вопрос:
    - Скорость сходимости итерационного алгоритма.
    """

    answer = """
    Скорость сходимости итерационного алгоритма показывает, насколько быстро последовательность приближений `xk` стремится к точному решению `x*`. Формально, если ошибка на k-м шаге равна `εk = |xk - x*|`, то скорость сходимости определяется соотношением `εk+1 ≈ C * (εk)^p`, где `p` — порядок сходимости.

    *   **Линейная сходимость (p=1)**: Ошибка на каждом шаге уменьшается примерно в постоянное число раз (`C < 1`). Это относительно медленная сходимость. Примером является метод простой итерации, где `C` равно `|φ'(x*)|`.
    *   **Квадратичная сходимость (p=2)**: Количество верных значащих цифр на каждом шаге примерно удваивается. Это очень быстрая сходимость. Классическим примером является метод Ньютона.
    *   **Сверхлинейная сходимость**: Скорость выше линейной, но ниже квадратичной. Примером является метод секущих.

    Чем выше порядок сходимости `p`, тем меньше итераций требуется для достижения заданной точности. Например, метод Ньютона (квадратичная сходимость) обычно значительно быстрее метода половинного деления (линейная сходимость).

    ---
    *Информация для ответа взята со слайдов 77 (упоминание медленной сходимости), 93 (модификация Ньютона), 105 (связь с итерациями), 108 (условие сходимости Ньютона), 261 (скорость степенного метода).*
    """
    return answer

def t16():
    """
    Ответ на вопрос:
    - Стабильность и распространение ошибки в методах численного решения нелинейных уравнений.
    """

    answer = """
    Тема "Стабильность и распространение ошибки" в контексте решения нелинейных уравнений напрямую не освещается в презентации так, как для дифференциальных уравнений. Однако можно провести аналогии.

    **Стабильность** итерационного метода означает, что малые ошибки (например, ошибки округления или неточность начального приближения) не приводят к катастрофическому росту общей погрешности и расходимости процесса. Стабильность тесно связана с условиями сходимости.

    **Распространение ошибки** определяется свойствами итерационной функции. В методе простой итерации `xk+1 = φ(xk)` ошибка распространяется в соответствии с множителем `q = |φ'(x*)|`.
    *   Если `|q| < 1` (сжимающее отображение), метод является стабильным: любая ошибка на k-м шаге будет уменьшена на (k+1)-м.
    *   Если `|q| > 1`, метод нестабилен: малая ошибка будет экспоненциально нарастать с каждой итерацией, и алгоритм быстро разойдется.
    *   Случай `|q| = 1` требует дополнительного анализа.

    Таким образом, для итерационных методов решения нелинейных уравнений стабильность по сути эквивалентна выполнению условия сходимости.

    ---
    *Ответ построен на основе анализа слайдов 79, 81, 82, 87.*
    """
    return answer


def t17():
    """
    Ответ на вопрос:
    - Теория сжимающих отображений.
    """

    answer = """
    Теория сжимающих отображений, также известная как принцип Банаха о неподвижной точке, является фундаментальной для доказательства сходимости многих итерационных методов. Отображение (функция) `φ` называется **сжимающим** на некотором множестве, если оно "сближает" любые две точки из этого множества. Формально, для любых `x1` и `x2` должно выполняться `|φ(x1) - φ(x2)| ≤ q * |x1 - x2|`, где `0 ≤ q < 1` — коэффициент сжатия.

    **Принцип сжимающих отображений** гласит, что если `φ` является сжимающим отображением на полном метрическом пространстве (например, на замкнутом отрезке `[a, b]`), которое отображает это пространство в себя, то существует единственная неподвижная точка `x*` такая, что `x* = φ(x*)`. Более того, итерационная последовательность `xk+1 = φ(xk)` сходится к этой точке `x*` при любом начальном приближении `x0`.

    Этот принцип напрямую применяется в методе простой итерации для решения нелинейных уравнений, где условие `|φ'(x)| < 1` на отрезке гарантирует, что отображение является сжимающим и, следовательно, итерационный процесс сойдется.

    ---
    *Информация для ответа взята со слайдов 79, 81, 82, 87.*
    """
    return answer

def t18():
    """
    Ответ на вопрос:
    - Интерполяция, экстраполяция, аппроксимация.
    """

    answer = """
    Эти три термина описывают задачу построения функции на основе набора известных данных, но различаются целями и условиями.

    **Аппроксимация** — это самый общий термин, означающий замену одного математического объекта (например, сложной или таблично заданной функции `f(x)`) другим, более простым (аппроксимирующей функцией `F(x)`).

    **Интерполяция** — это частный случай аппроксимации, при котором требуется, чтобы аппроксимирующая функция `F(x)` точно проходила через все заданные точки (узлы). То есть, `F(xi) = f(xi)` для всех узлов `xi`. Цель интерполяции — найти значение функции *между* известными точками.

    **Экстраполяция** — это также вид аппроксимации, но в этом случае задача состоит в оценке значения функции *вне* интервала, на котором заданы исходные точки. Экстраполяция, как правило, менее точна, чем интерполяция, так как поведение функции за пределами известного диапазона может быть непредсказуемым.

    ---
    *Информация для ответа взята со слайдов 153-159, 166.*
    """

    code = """
import numpy as np
import matplotlib.pyplot as plt

# Данные для примера
x_known = np.array([2, 3, 5, 6])
y_known = np.array([12, 15, 14, 15])
x_full_range = np.linspace(1, 8, 100)
y_true_func = lambda x: 10 + 2*x + 5*np.sin(1.5*x) # Пример истинной функции

# Интерполяция (линейная)
from scipy.interpolate import interp1d
interp_func = interp1d(x_known, y_known, kind='linear', fill_value="extrapolate")
y_interp_extrap = interp_func(x_full_range)

# Аппроксимация (полином 2-й степени)
approx_poly = np.poly1d(np.polyfit(x_known, y_known, 2))
y_approx = approx_poly(x_full_range)

plt.figure(figsize=(12, 7))

# График
plt.plot(x_full_range, y_true_func(x_full_range), 'k--', label='Истинная функция (неизвестна)', alpha=0.5)
plt.plot(x_known, y_known, 'ro', label='Узлы (известные данные)', markersize=8)
plt.plot(x_full_range, y_interp_extrap, 'g-', label='Интерполяция/Экстраполяция')
plt.plot(x_full_range, y_approx, 'b-.', label='Аппроксимация (полином)')

# Обозначение областей
plt.axvspan(x_known.min(), x_known.max(), color='yellow', alpha=0.2, label='Область интерполяции')
plt.axvspan(x_full_range.min(), x_known.min(), color='cyan', alpha=0.2, label='Область экстраполяции')
plt.axvspan(x_known.max(), x_full_range.max(), color='cyan', alpha=0.2)

plt.title('Интерполяция, Экстраполяция и Аппроксимация', fontsize=16)
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.grid(True)
plt.show()
    """
    return answer + "\n\nКод для построения графика:\n" + code

def t19():
    """
    Ответ на вопрос:
    - Глобальная и локальная интерполяция.
    """

    answer = """
    Различие между глобальной и локальной интерполяцией заключается в том, как используются известные точки (узлы) для построения интерполирующей функции.

    **Глобальная интерполяция** использует *все* доступные узлы `(x0, y0), ..., (xn, yn)` для построения *единой* функции `F(x)`, которая проходит через все эти точки. Классическим примером является полином Лагранжа. Недостатком глобальной полиномиальной интерполяции является то, что при большом числе узлов (обычно `n > 7`) она становится неустойчивой и может приводить к сильным осцилляциям между узлами (феномен Рунге).

    **Локальная (кусочная) интерполяция** строит итоговую функцию из нескольких "кусочков". Весь интервал разбивается на более мелкие подынтервалы, и на каждом из них строится своя, обычно простая, интерполирующая функция (например, линейная или кубическая), используя только несколько ближайших узлов. Примерами являются линейная интерполяция и сплайн-интерполяция. Такой подход более устойчив и не страдает от сильных осцилляций.

    ---
    *Информация для ответа взята со слайдов 169, 206, 207.*
    """
    return answer

def t20():
    """
    Ответ на вопрос:
    - Ступенчатая и линейная интерполяция.
    """

    answer = """
    Ступенчатая и линейная интерполяция — это простейшие виды локальной интерполяции, использующие соседние узлы для оценки значения функции.

    **Ступенчатая интерполяция (интерполяция нулевого порядка)** — самый простой способ. Для любой точки `x` на интервале `[xi, xi+1]` значение функции принимается равным значению в одном из узлов, например, в левом: `F(x) = f(xi)`. График такой функции представляет собой набор горизонтальных отрезков, "ступенек". Этот метод имеет низкую точность, и его график является разрывным.

    **Линейная интерполяция (интерполяция первого порядка)** является более точным методом. На каждом интервале `[xi, xi+1]` функция аппроксимируется прямой линией, соединяющей два соседних узла `(xi, yi)` и `(xi+1, yi+1)`. Формула для вычисления значения в точке `x` имеет вид:
    `y(x) = yi + (y_{i+1} - yi) * (x - xi) / (x_{i+1} - xi)`
    График представляет собой ломаную линию. Этот метод обеспечивает непрерывность функции, но её производная разрывна в узлах.

    ---
    *Информация для ответа взята со слайдов 169, 172-174.*
    """
    code = """
import numpy as np
import matplotlib.pyplot as plt

x_nodes = np.array([0, 1, 2, 3, 4, 5])
y_nodes = np.array([1, 2, 1.5, 3, 2.5, 4])
x_dense = np.linspace(0, 5, 500)

# Ступенчатая интерполяция
from scipy.interpolate import interp1d
step_func = interp1d(x_nodes, y_nodes, kind='previous')
y_step = step_func(x_dense)

# Линейная интерполяция
linear_func = interp1d(x_nodes, y_nodes, kind='linear')
y_linear = linear_func(x_dense)

fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8), sharex=True)

# График ступенчатой
ax1.plot(x_dense, y_step, 'b-', label='Ступенчатая интерполяция')
ax1.plot(x_nodes, y_nodes, 'ro', label='Узлы')
ax1.set_title('Ступенчатая интерполяция')
ax1.set_ylabel('Y')
ax1.legend()
ax1.grid(True)

# График линейной
ax2.plot(x_dense, y_linear, 'g-', label='Линейная интерполяция')
ax2.plot(x_nodes, y_nodes, 'ro', label='Узлы')
ax2.set_title('Линейная интерполяция')
ax2.set_xlabel('X')
ax2.set_ylabel('Y')
ax2.legend()
ax2.grid(True)

plt.tight_layout()
# Чтобы показать график, раскомментируйте plt.show()
# plt.show()

print("Код для построения графика сравнения ступенчатой и линейной интерполяции")
    """
    return answer + "\n\n" + code


def t21():
    """
    Ответ на вопрос:
    - Интерполяционные полиномы.
    """

    answer = """
    **Интерполяционный полином** — это полином `P(x)`, который используется для аппроксимации некоторой функции `f(x)` с условием точного прохождения через заданный набор узловых точек `(xi, yi)`. То есть, для всех узлов `i` должно выполняться равенство `P(xi) = yi`. Основная теорема утверждает, что для `n+1` различных узлов существует единственный полином степени не выше `n`, удовлетворяющий этим условиям.

    Главное преимущество использования полиномов для интерполяции заключается в их простоте: они легко вычисляются, дифференцируются и интегрируются. Однако глобальная полиномиальная интерполяция (когда один полином строится по всем точкам) при большом количестве узлов может приводить к сильным осцилляциям между ними (феномен Рунге). Поэтому на практике часто используют кусочно-полиномиальную интерполяцию, например, сплайны.

    ---
    *Информация для ответа взята со слайдов 176, 177, 196.*
    """
    return answer

def t22():
    """
    Ответ на вопрос:
    - Квадратичная интерполяция.
    """

    answer = """
    **Квадратичная интерполяция** — это локальный метод интерполяции, при котором для оценки значения функции в некоторой точке `x*` используется парабола (полином второй степени), построенная по трем соседним узлам `(x_{i-1}, y_{i-1})`, `(xi, yi)` и `(x_{i+1}, y_{i+1})`. Интерполяционный полином имеет вид `φ(x) = a0 + a1*x + a2*x²`.

    Для нахождения неизвестных коэффициентов `a0, a1, a2` решается система из трех линейных уравнений, которая составляется из условия прохождения параболы через три узла:
    `φ(x_{i-1}) = y_{i-1}`
    `φ(xi) = yi`
    `φ(x_{i+1}) = y_{i+1}`

    Эта система всегда имеет единственное решение, так как ее определитель (определитель Вандермонда для различных узлов) отличен от нуля. Квадратичная интерполяция обеспечивает не только непрерывность самой функции, но и непрерывность ее первой производной, что делает график более гладким по сравнению с линейной интерполяцией.

    ---
    *Информация для ответа взята со слайдов 197-200.*
    """
    code = """
import numpy as np
import matplotlib.pyplot as plt

# Узлы для примера со слайда 199
x_nodes = np.array([1, 3, 4])
y_nodes = np.array([12, 4, 6])

# Находим коэффициенты полинома
# A * c = y  => c = A_inv * y
A = np.vander(x_nodes, increasing=True)
coeffs = np.linalg.solve(A, y_nodes) # [a0, a1, a2] -> a0 + a1*x + a2*x^2
# В презентации коэффициенты расположены наоборот [a2, a1, a0]
# coeffs_презентация = np.polyfit(x_nodes, y_nodes, 2)
# print(f"Коэффициенты из np.polyfit (a2, a1, a0): {coeffs_презентация}") # [ 2. -12.  22.]
# print(f"Коэффициенты из np.linalg.solve (a0, a1, a2): {coeffs}")     # [ 22. -12.   2.]

# Создаем полиномиальную функцию
p = np.poly1d(np.flip(coeffs)) # np.poly1d ожидает [a2, a1, a0]
print(f"Интерполяционный полином: \n{p}")

# Генерируем точки для гладкого графика
x_smooth = np.linspace(0.5, 4.5, 400)
y_smooth = p(x_smooth)

plt.figure(figsize=(8, 6))
plt.plot(x_smooth, y_smooth, label='Квадратичный полином P(x) = 2x² - 12x + 22')
plt.plot(x_nodes, y_nodes, 'ro', label='Узловые точки')
plt.title('Квадратичная интерполяция')
plt.xlabel('x')
plt.ylabel('y')
plt.grid(True)
plt.legend()
# plt.show()
print("\\nКод для построения графика квадратичной интерполяции:")
    """
    return answer + "\n\n" + code

def t23():
    """
    Ответ на вопрос:
    - Интерполяция сплайнами.
    """

    answer = """
    **Интерполяция сплайнами** — это метод кусочно-полиномиальной интерполяции, который обеспечивает не только прохождение через узловые точки, но и гладкость итоговой функции в этих узлах. Сплайн — это функция, состоящая из нескольких полиномов (обычно невысокой степени), соединенных друг с другом так, чтобы в точках их соединения (узлах) были непрерывны не только значения функции, но и ее производные до определенного порядка.

    Наиболее распространен **кубический сплайн**, где на каждом сегменте `[xi, xi+1]` функция является полиномом третьей степени. Условия "сшивки" в узлах требуют непрерывности первой и второй производных. Это позволяет избежать изломов (как в линейной интерполяции) и чрезмерных осцилляций (как в глобальной полиномиальной), создавая гладкую и предсказуемую кривую. В отличие от полинома Лагранжа, сплайн требует только значения функции в узлах, но не ее производных, что делает его практичным.

    ---
    *Информация для ответа взята со слайдов 189, 208-213.*
    """
    return answer

def t24():
    """
    Ответ на вопрос:
    - Интерполяционный полином Лагранжа.
    """

    answer = """
    **Интерполяционный многочлен Лагранжа** — это метод глобальной полиномиальной интерполяции, который позволяет построить единственный полином степени не выше `n`, проходящий через `n+1` заданных узловых точек `(xi, yi)`. Его ключевая особенность — удобная формула, не требующая решения систем уравнений.

    Полином `L(x)` строится как взвешенная сумма базисных полиномов `li(x)`:
    `L(x) = Σ [yi * li(x)]` (сумма по i от 0 до n),
    где каждый базисный полином `li(x)` обладает свойством: он равен 1 в узле `xi` и 0 во всех остальных узлах `xj` (при `j ≠ i`).

    Формула для базисного полинома `li(x)`:
    `li(x) = Π [(x - xj) / (xi - xj)]` (произведение по j от 0 до n, где `j ≠ i`).

    Хотя формула элегантна, на практике полином Лагранжа редко используется для большого числа узлов из-за вычислительной сложности и склонности к осцилляциям (феномену Рунге).

    ---
    *Информация для ответа взята со слайдов 176-179, 191-195.*
    """
    return answer

def t25():
    """
    Ответ на вопрос:
    - Метод кубической сплайн-интерполяции.
    """

    answer = """
    **Метод кубической сплайн-интерполяции** является одним из самых популярных и эффективных методов интерполяции. Он заключается в построении функции, которая на каждом отрезке `[xi, xi+1]` является кубическим полиномом `Si(x)`, и при этом в узлах `xi` обеспечивается непрерывность не только самой функции, но и её первой и второй производных. Это создает очень гладкую кривую, проходящую через все заданные точки.

    На каждом i-м интервале полином имеет вид:
    `Si(x) = ai + bi*(x - xi) + ci*(x - xi)² + di*(x - xi)³`

    Коэффициенты полиномов находятся из системы линейных уравнений, составленной из следующих условий:
    1.  Прохождение через узлы: `Si(xi) = yi`.
    2.  Непрерывность функции, первой и второй производных в узлах: `S_{i-1}(xi) = Si(xi)`, `S'_{i-1}(xi) = S'i(xi)`, `S''_{i-1}(xi) = S''i(xi)`.
    3.  Дополнительные граничные условия на концах всего интервала (например, `S''(x0) = 0` и `S''(xn) = 0` для "естественного" сплайна).

    Этот метод обеспечивает высокую точность и гладкость, избегая при этом осцилляций, характерных для полиномов высокой степени.

    ---
    *Информация для ответа взята со слайдов 189-190, 210-219.*
    """
    return answer

def t26():
    """
    Ответ на вопрос:
    - Основные операции в вычислительной линейной алгебре.
    """

    answer = """
    Основные операции в вычислительной (плотной) линейной алгебре (DLA) играют ключевую роль во множестве научных и инженерных приложений. Их можно сгруппировать по задачам:

    1.  **Решение систем линейных уравнений (СЛУ): Ax = b.** Это фундаментальная задача, возникающая в материаловедении, электродинамике, статистике и многих других областях.

    2.  **Задачи на собственные значения и векторы: Ax = λx.** Эти задачи необходимы для анализа стабильности систем, в квантовой механике, для алгоритмов ранжирования (Google PageRank) и методов снижения размерности, таких как метод главных компонент (PCA).

    3.  **Метод наименьших квадратов: найти x, минимизирующий ||Ax - b||.** Применяется для аппроксимации данных, в эконометрике, обработке сигналов и машинном обучении.

    4.  **Сингулярное разложение (SVD): A = UΣV*.** Это мощный инструмент для анализа матриц, используемый в поиске информации, сжатии данных, вычислении псевдообратной матрицы и решении задач методом наименьших квадратов.

    Для эффективной реализации этих операций созданы стандартизированные библиотеки, такие как BLAS и LAPACK.

    ---
    *Информация для ответа взята со слайда 235.*
    """
    return answer

def t27():
    """
    Ответ на вопрос:
    - Эффективная реализация алгоритмов вычисления произведения матриц.
    """

    answer = """
    Эффективная реализация умножения матриц критически важна, так как это одна из самых вычислительно-интенсивных операций. Наивный алгоритм "строка на столбец" имеет сложность O(n³), но его практическая производительность сильно зависит от организации доступа к данным в иерархической памяти компьютера.

    Ключевой принцип оптимизации — **принцип локальности**: максимизация повторного использования данных, уже загруженных в быстрый кэш. Это достигается с помощью **блочного умножения матриц**. Матрицы-сомножители разбиваются на небольшие подматрицы (блоки), размер которых подбирается так, чтобы они помещались в кэш-память. Вычисления проводятся на уровне этих блоков, что значительно снижает количество дорогостоящих обращений к медленной оперативной памяти.

    Именно такие блочные алгоритмы реализованы в оптимизированных библиотеках, таких как BLAS (Basic Linear Algebra Subroutines), которые лежат в основе быстрых вычислений в NumPy и других научных пакетах. Это позволяет достичь производительности, близкой к пиковой для конкретного процессора, что на порядки быстрее наивной реализации.

    ---
    *Информация для ответа взята со слайдов 237, 238, 240.*
    """
    return answer

def t28():
    """
    Ответ на вопрос:
    - Алгоритм Штрассена, сложность метода Штрассена.
    """

    answer = """
    **Алгоритм Штрассена** — это рекурсивный алгоритм для умножения двух матриц размером n×n, который асимптотически быстрее наивного метода. Идея алгоритма заключается в разбиении исходных матриц на четыре подматрицы (блока) размера n/2 × n/2 и вычислении произведения с помощью 7 умножений этих подматриц вместо 8, как в стандартном блочном методе, но ценой увеличения числа сложений (18 вместо 4).

    Эта экономия на одном умножении на каждом шаге рекурсии приводит к снижению общей вычислительной сложности.

    **Сложность метода Штрассена** составляет O(n^log₂(7)) ≈ **O(n^2.807)**. Это лучше, чем O(n³) у наивного и блочного методов. Несмотря на лучшую асимптотическую сложность, на практике алгоритм Штрассена становится эффективнее стандартных методов только для достаточно больших матриц из-за больших накладных расходов на рекурсию и большего числа сложений. Существуют и более сложные алгоритмы (например, Копперсмита-Винограда со сложностью ~O(n^2.37)), но они еще менее практичны.

    ---
    *Информация для ответа взята со слайдов 240, 287, 292.*
    """
    return answer

def t29():
    """
    Ответ на вопрос:
    - Вычисление SVD.
    """

    answer = """
    Вычисление сингулярного разложения (SVD) матрицы A (A = UΣVᵀ) является одной из фундаментальных задач вычислительной линейной алгебры. Прямое вычисление через определение обычно не используется. Вместо этого SVD тесно связывают с задачей нахождения собственных значений.

    Сингулярные числа `σi` (диагональные элементы матрицы Σ) являются квадратными корнями из собственных значений `λi` симметричной положительно полуопределенной матрицы `AᵀA` (или `AAᵀ`). Столбцы матрицы `V` (правые сингулярные векторы) — это собственные векторы матрицы `AᵀA`, а столбцы `U` (левые сингулярные векторы) — собственные векторы `AAᵀ`.

    Практические алгоритмы вычисления SVD обычно двухэтапные:
    1.  Сначала матрица `A` приводится к более простой **бидиагональной форме** `B` с помощью ортогональных преобразований (например, отражений Хаусхолдера).
    2.  Затем для бидиагональной матрицы `B` итерационно вычисляются её сингулярные числа и векторы, например, с помощью модифицированного QR-алгоритма. Этот подход численно более устойчив и эффективен, чем прямое формирование и решение задачи на собственные значения для `AᵀA`.

    ---
    *Информация для ответа взята со слайдов 235, 314, 316, 332-340.*
    """
    return answer

def t30():
    """
    Ответ на вопрос:
    - Собственные векторы, собственные значения.
    """

    answer = """
    Для квадратной матрицы `A` размером n×n, **собственным вектором** называется ненулевой вектор `x`, который при умножении на матрицу `A` не меняет своего направления, а только масштабируется на некоторое число `λ`. Это число `λ` называется **собственным значением**, соответствующим данному вектору.

    Математически это определение записывается как уравнение:
    `Ax = λx`

    Собственные векторы определяют инвариантные направления для линейного преобразования, задаваемого матрицей `A`. Они показывают "оси", вдоль которых действие матрицы сводится к простому растяжению или сжатию. Собственные значения `λ` являются коэффициентами этого масштабирования. Нахождение собственных значений и векторов является одной из ключевых задач линейной алгебры, имеющей широкое применение в анализе динамических систем, вибрационном анализе, квантовой механике и алгоритмах машинного обучения.

    ---
    *Информация для ответа взята со слайдов 244, 245.*
    """

    code = """
import numpy as np
import matplotlib.pyplot as plt

A = np.array([[3, 1], [0, 2]])

# Собственные значения и векторы
eigenvalues, eigenvectors = np.linalg.eig(A)
v1 = eigenvectors[:, 0]
v2 = eigenvectors[:, 1]

# Несколько случайных векторов для сравнения
x1 = np.array([1, 1])
x2 = np.array([-1, 1])
x3 = np.array([0.5, -1])

# Применяем преобразование
Ax1 = A @ x1
Ax2 = A @ x2
Ax3 = A @ x3
Av1 = A @ v1
Av2 = A @ v2

plt.figure(figsize=(8, 8))
# Исходные векторы
plt.quiver(0, 0, x1[0], x1[1], angles='xy', scale_units='xy', scale=1, color='gray', label='Original Vectors')
plt.quiver(0, 0, x2[0], x2[1], angles='xy', scale_units='xy', scale=1, color='gray')
plt.quiver(0, 0, x3[0], x3[1], angles='xy', scale_units='xy', scale=1, color='gray')

# Преобразованные векторы
plt.quiver(0, 0, Ax1[0], Ax1[1], angles='xy', scale_units='xy', scale=1, color='blue', label='Transformed Vectors')
plt.quiver(0, 0, Ax2[0], Ax2[1], angles='xy', scale_units='xy', scale=1, color='blue')
plt.quiver(0, 0, Ax3[0], Ax3[1], angles='xy', scale_units='xy', scale=1, color='blue')

# Собственные векторы (исходный и преобразованный)
plt.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='red', label='Eigenvector 1')
plt.quiver(0, 0, Av1[0], Av1[1], angles='xy', scale_units='xy', scale=1, color='pink', linestyle='--')
plt.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='green', label='Eigenvector 2')
plt.quiver(0, 0, Av2[0], Av2[1], angles='xy', scale_units='xy', scale=1, color='lightgreen', linestyle='--')

plt.xlim(-2, 4)
plt.ylim(-2, 4)
plt.axhline(0, color='black',linewidth=0.5)
plt.axvline(0, color='black',linewidth=0.5)
plt.grid(True)
plt.legend()
plt.title('Действие матрицы на векторы и собственные векторы')
# plt.show()
print("Код для визуализации собственных векторов:")
    """
    return answer + "\n\n" + code

def t31():
    """
    Ответ на вопрос:
    - Разложение по собственным векторам.
    """

    answer = """
    **Разложение по собственным векторам**, или спектральное разложение, применимо к диагонализуемым матрицам (например, к симметричным или имеющим `n` линейно независимых собственных векторов). Оно представляет матрицу `A` в виде произведения трех матриц:
    `A = VΛV⁻¹`

    Здесь:
    - `V` — это матрица, столбцами которой являются собственные векторы матрицы `A`.
    - `Λ` (Лямбда) — это диагональная матрица, на главной диагонали которой расположены соответствующие собственные значения `λi`.
    - `V⁻¹` — матрица, обратная к `V`.

    Это разложение очень полезно, так как оно сводит действие сложной матрицы `A` к трем более простым операциям: смена базиса (`V⁻¹`), простое масштабирование вдоль осей нового базиса (`Λ`) и возврат в исходный базис (`V`). Это упрощает многие вычисления, например, возведение матрицы в степень: `A^k = VΛ^k V⁻¹`, где `Λ^k` вычисляется тривиальным возведением в степень диагональных элементов.

    ---
    *Информация для ответа взята со слайдов 250, 262, 283.*
    """
    return answer

def t32():
    """
    Ответ на вопрос:
    - Задача Google PageRank.
    """

    answer = """
    **Задача Google PageRank** заключается в ранжировании веб-страниц для определения их важности. Алгоритм PageRank рассматривает Интернет как граф, где страницы — это вершины, а гиперссылки — направленные ребра. Важность страницы определяется рекурсивно: чем больше важных страниц ссылается на данную страницу, тем она важнее.

    Пусть `pi` — это важность (PageRank) i-ой страницы. Тогда она вычисляется как сумма долей PageRank'ов всех страниц `j`, которые ссылаются на страницу `i`:
    `pi = Σ [pj / L(j)]` (сумма по всем `j`, ссылающимся на `i`),
    где `L(j)` — общее число исходящих ссылок со страницы `j`.

    Эта система уравнений может быть записана в матричном виде `p = Gp`, где `p` — вектор PageRank'ов, а `G` — модифицированная матрица смежности графа. Таким образом, задача сводится к нахождению собственного вектора матрицы `G`, соответствующего собственному значению `λ=1`. Этот собственный вектор и является искомым вектором важности страниц.

    ---
    *Информация для ответа взята со слайдов 253, 254.*
    """
    return answer

def t33():
    """
    Ответ на вопрос:
    - Вычисление собственных значений с помощью характеристического многочлена.
    """

    answer = """
    Классический метод нахождения собственных значений матрицы `A` основан на решении её **характеристического уравнения**. Исходное уравнение `Ax = λx` преобразуется к виду `(A - λE)x = 0`, где `E` — единичная матрица. Эта система однородных линейных уравнений имеет ненулевое решение `x` только в том случае, если ее определитель равен нулю.

    Таким образом, мы получаем характеристическое уравнение:
    `det(A - λE) = 0`

    Раскрытие этого определителя дает полином `Pn(λ)` n-й степени относительно `λ`, который называется **характеристическим многочленом**. Корни этого полинома `λ₁, λ₂, ..., λn` и являются собственными значениями матрицы `A`.

    Хотя этот метод является теоретически фундаментальным, на практике он используется только для матриц очень малого порядка (n ≤ 4), так как аналитическое нахождение корней полиномов степени 5 и выше невозможно (теорема Абеля-Руффини), а численное решение является плохо обусловленной задачей.

    ---
    *Информация для ответа взята со слайдов 244, 245, 256, 257.*
    """
    return answer

def t34():
    """
    Ответ на вопрос:
    - Особенности степенного метода. Скорость сходимости.
    """

    answer = """
    **Степенной метод** — это итерационный алгоритм для нахождения одного, максимального по модулю, собственного значения (доминантного) и соответствующего ему собственного вектора. Его основная идея заключается в многократном умножении матрицы `A` на некоторый начальный вектор `x₀`. После каждой итерации `x_{k+1} = Ax_k` вектор `x_k` будет все больше "вытягиваться" вдоль направления собственного вектора, соответствующего доминантному собственному значению. Для предотвращения переполнения вектор на каждом шаге нормируется.

    **Особенности метода**:
    1.  Простота реализации.
    2.  Находит только одно, максимальное по модулю, собственное значение.
    3.  Требует, чтобы доминантное собственное значение было строго больше остальных по модулю.

    **Скорость сходимости** метода является линейной и определяется отношением модулей двух наибольших собственных значений: `q = |λ₂ / λ₁|`. Чем это отношение ближе к 1, тем медленнее сходится метод. Если `|λ₂| = |λ₁|`, метод не сходится. Из-за потенциально медленной сходимости он используется для грубой оценки или как часть более сложных алгоритмов.

    ---
    *Информация для ответа взята со слайдов 258-264.*
    """
    return answer

def t35():
    """
    Ответ на вопрос:
    - Круги Гершгорина, теорема Гершгорина.
    """

    answer = """
    **Теорема Гершгорина** предоставляет простой способ локализации собственных значений матрицы на комплексной плоскости, не вычисляя их. Она утверждает, что все собственные значения матрицы `A` лежат в объединении кругов, называемых **кругами Гершгорина**.

    Для квадратной матрицы `A` n×n, i-й круг Гершгорина `Ci` — это круг на комплексной плоскости с центром в диагональном элементе `aii` и радиусом `ri`, равным сумме модулей всех остальных элементов в i-й строке:
    `ri = Σ |aij|` (сумма по `j`, где `j ≠ i`).

    **Свойства**:
    1.  Все собственные значения матрицы находятся внутри хотя бы одного из этих `n` кругов.
    2.  Если объединение `k` кругов не пересекается с объединением остальных `n-k` кругов, то в первом объединении находится ровно `k` собственных значений.

    Этот метод особенно полезен для диагонально-доминантных матриц, так как позволяет легко оценить, что собственные значения близки к диагональным элементам и матрица невырождена.

    ---
    *Информация для ответа взята со слайдов 267, 268.*
    """

    code = """
import numpy as np
import matplotlib.pyplot as plt

def plot_gershgorin_circles(A):
    n = A.shape[0]
    plt.figure(figsize=(8, 8))
    ax = plt.gca()

    eigenvalues = np.linalg.eigvals(A)

    for i in range(n):
        center = A[i, i]
        radius = np.sum(np.abs(A[i, :])) - np.abs(center)

        circle = plt.Circle((center.real, center.imag), radius, color='gray', fill=False, linestyle='-', linewidth=2)
        ax.add_patch(circle)
        plt.plot(center.real, center.imag, 'ro') # Центры кругов

    plt.plot(eigenvalues.real, eigenvalues.imag, 'b*', markersize=10, label='Собственные значения')

    plt.title('Круги Гершгорина')
    plt.xlabel('Реальная часть')
    plt.ylabel('Мнимая часть')
    plt.grid(True)
    plt.axis('equal')
    plt.legend()
    # plt.show()

# Пример матрицы
A = np.array([[10, -1, 0, 1],
              [0.2, 8, 0.2, 0.2],
              [1, 1, 2, 1],
              [-1, -1, -1, -4]])

# plot_gershgorin_circles(A)
print("Код для построения кругов Гершгорина:")
    """
    return answer + "\n\n" + code

def t36():
    """
    Ответ на вопрос:
    - Теорема Шура.
    """

    answer = """
    **Теорема Шура** (или разложение Шура) является одним из центральных результатов в вычислительной линейной алгебре. Она утверждает, что для любой квадратной матрицы `A` с комплексными элементами существует унитарная матрица `U` (такая что `U*U = I`) и верхнетреугольная матрица `T` такие, что:
    `A = UTU*` или `U*AU = T`

    Ключевые моменты этой теоремы:
    1.  **Существование**: В отличие от диагонализации, разложение Шура существует для *любой* квадратной матрицы.
    2.  **Собственные значения**: Диагональные элементы верхнетреугольной матрицы `T` являются собственными значениями исходной матрицы `A`.
    3.  **Численная устойчивость**: Использование унитарных матриц в преобразованиях делает алгоритмы, основанные на разложении Шура (например, QR-алгоритм), численно устойчивыми.

    Эта теорема является теоретической основой для многих современных алгоритмов нахождения собственных значений, поскольку она сводит задачу для произвольной матрицы к более простой задаче для треугольной матрицы.

    ---
    *Информация для ответа взята со слайдов 294, 295.*
    """
    return answer

def t37():
    """
    Ответ на вопрос:
    - Нормальные матрицы, унитарно диагонализуемые матрицы, унитарные матрицы, эрмитовы матрицы.
    """

    answer = """
    Эти термины описывают важные классы матриц со специальными свойствами, которые упрощают задачу нахождения собственных значений.

    - **Эрмитова матрица**: Квадратная матрица `A`, которая равна своему эрмитово-сопряжённому (транспонированному и комплексно-сопряжённому) аналогу: `A = A*`. Для вещественных матриц это эквивалентно понятию **симметричной матрицы** (`A = Aᵀ`). Эрмитовы матрицы всегда имеют вещественные собственные значения.

    - **Унитарная матрица**: Квадратная матрица `U`, для которой обратная матрица совпадает с её эрмитово-сопряжённой: `U*U = UU* = I`. Для вещественных матриц это **ортогональная матрица** (`UᵀU = I`). Модули всех собственных значений унитарной матрицы равны 1.

    - **Нормальная матрица**: Это наиболее общий класс. Матрица `A` называется нормальной, если она коммутирует со своей эрмитово-сопряжённой: `AA* = A*A`. Эрмитовы и унитарные матрицы являются частными случаями нормальных матриц.

    - **Унитарно диагонализуемая матрица**: Матрица `A`, которую можно представить в виде `A = UΛU*`, где `U` — унитарная, а `Λ` — диагональная. **Ключевая теорема гласит, что матрица унитарно диагонализуема тогда и только тогда, когда она нормальна.** Это означает, что для любой нормальной матрицы существует ортонормированный базис из её собственных векторов.

    ---
    *Информация для ответа взята со слайдов 250, 262, 296.*
    """
    return answer

def t38():
    """
    Ответ на вопрос:
    - Верхне-гессенбергова форма матрицы.
    """

    answer = """
    **Верхне-гессенбергова форма** — это специальная структура квадратной матрицы, близкая к треугольной. Матрица `H` находится в верхне-гессенберговой форме, если все её элементы ниже первой поддиагонали равны нулю. То есть, `hij = 0` для всех `i > j + 1`.

    Визуально это выглядит так:
    `H = [[*, *, *, *], [*, *, *, *], [0, *, *, *], [0, 0, *, *]]`

    Эта форма является важным промежуточным шагом во многих алгоритмах вычисления собственных значений, в частности, в QR-алгоритме. Приведение произвольной матрицы к гессенберговой форме (что всегда возможно с помощью унитарных преобразований) значительно снижает вычислительную сложность последующих итераций QR-алгоритма с O(n³) до O(n²) за шаг. Для симметричных матриц гессенбергова форма становится трёхдиагональной, что ещё больше упрощает вычисления.

    ---
    *Информация для ответа взята со слайдов 311, 312.*
    """
    return answer

def t39():
    """
    Ответ на вопрос:
    - Приведение произвольной матрицы к верхне-гессенберговой форме
    """

    answer = """
    Приведение произвольной квадратной матрицы `A` к верхне-гессенберговой форме `H` является первым шагом в современных QR-алгоритмах для нахождения собственных значений. Цель этого преобразования — "обнулить" элементы под первой поддиагональю. Это достигается с помощью последовательности унитарных преобразований подобия, которые сохраняют собственные значения исходной матрицы.

    Наиболее распространённым методом для этого является использование **отражений Хаусхолдера**. Процесс происходит пошагово, для каждого столбца `j` от 1 до `n-2`:
    1.  Для j-го столбца строится матрица отражения Хаусхолдера `Hj`, которая обнуляет все элементы под первой поддиагональю (с `j+2` по `n`).
    2.  К исходной матрице применяется преобразование подобия: `A := Hj A Hj*`.

    Это преобразование не "портит" нули, полученные на предыдущих шагах. В отличие от приведения к форме Шура, здесь на каждом шаге обнуляются не все поддиагональные элементы, а только те, что лежат ниже первой поддиагонали. Итоговая сложность этого приведения составляет O(n³) операций.

    ---
    *Информация для ответа взята со слайдов 311, 312, 313, 318-319.*
    """
    return answer

def t40():
    """
    Ответ на вопрос:
    - Отношение Релея.
    """

    answer = """
    **Отношение Релея** (или частное Релея) — это скалярная величина, определяемая для эрмитовой (или симметричной) матрицы `A` и ненулевого вектора `x` по формуле:
    `RA(x) = (x*Ax) / (x*x)`

    Это отношение имеет несколько важных свойств:
    1.  Его значение всегда лежит между минимальным и максимальным собственными значениями матрицы `A`: `λ_min ≤ RA(x) ≤ λ_max`.
    2.  Оно достигает своих экстремальных значений (минимума и максимума) тогда и только тогда, когда `x` является собственным вектором, соответствующим `λ_min` или `λ_max`.

    Благодаря этим свойствам, отношение Релея является мощным инструментом в вариационных методах поиска экстремальных собственных значений. Оно позволяет свести задачу нахождения собственных значений к задаче оптимизации — поиску максимума или минимума функции `RA(x)`. Также оно используется в **итерации Релея** для ускорения сходимости алгоритмов поиска собственных значений.

    ---
    *Информация для ответа взята со слайда 297.*
    """
    return answer

def t41():
    """
    Ответ на вопрос:
    - Зазор между собственными значениями в матрице, алгоритмы со сдвигами.
    """

    answer = """
    **Зазор (gap)** между собственными значениями — это разница между их модулями, например, `|λi| - |λj|`. Величина зазора напрямую влияет на скорость сходимости итерационных методов, таких как степенной метод или QR-алгоритм. Большой зазор между доминантным собственным значением и остальными ускоряет сходимость, в то время как маленький зазор (близкие по модулю собственные значения) её замедляет.

    **Алгоритмы со сдвигами** — это техника для искусственного увеличения зазора и, как следствие, ускорения сходимости. Идея состоит в том, чтобы на каждой итерации QR-алгоритма применять его не к матрице `Ak`, а к сдвинутой матрице `Ak - sk*I`, где `sk` — это "сдвиг", некоторое число, близкое к искомому собственному значению.

    Формула итерации выглядит так:
    1. `Ak - sk*I = Qk*Rk` (QR-разложение сдвинутой матрицы)
    2. `A_{k+1} = Rk*Qk + sk*I` (обратный сдвиг)

    Если сдвиг `sk` выбрать удачно (например, используя **смещение Уилкинсона**), то одно из собственных значений сдвинутой матрицы становится очень малым. Это создает большой относительный зазор, что приводит к очень быстрой (квадратичной или даже кубической) сходимости к одному из собственных значений матрицы `A`.

    ---
    *Информация для ответа взята со слайдов 304, 307, 308, 310.*
    """
    return answer

def t42():
    """
    Ответ на вопрос:
    - Отражения Хаусхолдера.
    """

    answer = """
    **Отражение Хаусхолдера** (или преобразование Хаусхолдера) — это линейное ортогональное преобразование, которое "отражает" вектор относительно гиперплоскости, перпендикулярной некоторому вектору `v` (называемому вектором Хаусхолдера). Матрица этого преобразования `H` задается формулой:
    `H = I - 2 * (v vᵀ) / (vᵀv)`

    Ключевые свойства матриц Хаусхолдера:
    1.  **Ортогональность**: `Hᵀ = H⁻¹`, что обеспечивает численную устойчивость преобразований.
    2.  **Симметричность**: `H = Hᵀ`.
    3.  **Инволютивность**: `H² = I`.

    Основное применение отражений Хаусхолдера в вычислительной линейной алгебре — это управляемое "обнуление" выбранных элементов в векторе или столбце матрицы. Путем подбора вектора `v` можно отразить любой вектор `x` так, чтобы его преобразованная версия `Hx` была коллинеарна первому базисному вектору, то есть все её компоненты, кроме первой, стали равны нулю. Это свойство широко используется для приведения матриц к верхне-гессенберговой или бидиагональной форме.

    ---
    *Информация для ответа взята со слайда 313.*
    """

    code = """
import numpy as np
import matplotlib.pyplot as plt

# Исходный вектор
x = np.array([3, 4])

# Вектор, вдоль которого хотим отразить x, чтобы получить ||x||*e1
e1 = np.array([1, 0])
norm_x = np.linalg.norm(x)
sign_x1 = np.sign(x[0]) if x[0] != 0 else 1

# Вектор Хаусхолдера
v = x + sign_x1 * norm_x * e1

# Матрица Хаусхолдера
H = np.eye(2) - 2 * np.outer(v, v) / np.dot(v, v)

# Отраженный вектор
Hx = H @ x

print(f"Исходный вектор x: {x}")
print(f"Вектор Хаусхолдера v: {v}")
print(f"Матрица Хаусхолдера H: \n{H}")
print(f"Отраженный вектор Hx: {Hx}") # Должен быть [-5, 0] или [5,0]

plt.figure(figsize=(7, 7))
plt.quiver(0, 0, x[0], x[1], angles='xy', scale_units='xy', scale=1, color='b', label='Исходный вектор x')
plt.quiver(0, 0, Hx[0], Hx[1], angles='xy', scale_units='xy', scale=1, color='r', label='Отраженный вектор Hx')

# Гиперплоскость (в 2D это линия, перпендикулярная v)
line_x = np.linspace(-5, 5, 100)
line_y = (-v[0] / v[1]) * line_x
plt.plot(line_x, line_y, 'k--', label='Гиперплоскость отражения')

plt.xlim(-6, 6)
plt.ylim(-6, 6)
plt.axhline(0, color='gray',linewidth=0.5)
plt.axvline(0, color='gray',linewidth=0.5)
plt.grid(True)
plt.legend()
plt.axis('equal')
plt.title('Отражение Хаусхолдера')
# plt.show()
print("\\nКод для визуализации отражения Хаусхолдера:")
    """
    return answer + "\n\n" + code

def t43():
    """
    Ответ на вопрос:
    - Сходимость и сложность QR алгоритма.
    """

    answer = """
    **Сложность** базового QR-алгоритма для плотной матрицы `A` размером n×n складывается из двух этапов. Первый этап — приведение матрицы к верхне-гессенберговой форме, что требует O(n³) операций. Второй этап — итерационный процесс. Каждая QR-итерация для гессенберговой матрицы требует O(n²) операций. Поскольку для нахождения всех `n` собственных значений требуется O(n) итераций, общая сложность этого этапа также составляет O(n³). Таким образом, полная сложность QR-алгоритма для плотной несимметричной матрицы оценивается как O(n³). Для симметричных матриц сложность меньше, так как гессенбергова форма является трёхдиагональной.

    **Сходимость** QR-алгоритма линейная, но её можно значительно ускорить с помощью техники **сдвигов**. Сходимость зависит от **зазоров** между собственными значениями: чем они дальше друг от друга, тем быстрее сходимость. Использование сдвигов (например, смещения Уилкинсона) позволяет искусственно увеличить эти зазоры, достигая квадратичной или даже кубической сходимости, что делает QR-алгоритм очень эффективным на практике.

    ---
    *Информация для ответа взята со слайдов 300, 302, 307, 315.*
    """
    return answer

def t44():
    """
    Ответ на вопрос:
    - Метод главных компонент и поиск сингулярных значений, прикладные аспекты.
    """

    answer = """
    **Метод главных компонент (PCA)** — это статистический метод для снижения размерности данных при сохранении максимального объема информации (вариации). Он преобразует набор исходных, возможно, коррелирующих признаков в новый набор линейно независимых признаков, называемых **главными компонентами**. Первая главная компонента соответствует направлению наибольшей изменчивости данных, вторая — направлению наибольшей изменчивости в ортогональном подпространстве, и так далее.

    Математически главные компоненты являются собственными векторами ковариационной матрицы данных. Важность каждой компоненты определяется соответствующим ей собственным значением, которое показывает долю дисперсии данных, объясняемую этой компонентой.

    PCA тесно связан с **сингулярным разложением (SVD)**. Применение SVD к центрированной матрице данных `X = UΣVᵀ` напрямую дает главные компоненты (столбцы `V` — это направления, а `UΣ` — проекции данных на эти направления).

    **Прикладные аспекты**:
    - **Сжатие изображений**: Отбрасывая компоненты с малыми сингулярными числами, можно значительно уменьшить объем данных для хранения изображения с минимальной потерей качества.
    - **Визуализация многомерных данных**: Проецирование данных на первые 2-3 главные компоненты позволяет визуализировать их структуру.
    - **Ускорение машинного обучения**: Работа с данными меньшей размерности снижает вычислительные затраты и помогает бороться с "проклятием размерности".
    - **Шумоподавление**: Компоненты с малыми сингулярными значениями часто соответствуют шуму, и их удаление позволяет "очистить" данные.

    ---
    *Информация для ответа взята со слайдов 277-286, 288, 337.*
    """
    return answer

def t45():
    """
    Ответ на вопрос:
    - Сингулярное разложение (SVD).
    """

    answer = """
    **Сингулярное разложение (SVD)** — это одно из важнейших разложений матрицы в линейной алгебре, существующее для *любой* прямоугольной или квадратной матрицы `A`. Оно представляет матрицу `A` в виде произведения трех матриц:
    `A = UΣVᵀ` (или `UΣV*` для комплексных матриц)

    Здесь:
    - `U` — ортогональная (или унитарная) матрица, столбцы которой называются **левыми сингулярными векторами**. Они являются собственными векторами матрицы `AAᵀ`.
    - `Σ` (Сигма) — диагональная матрица того же размера, что и `A`. На её диагонали расположены неотрицательные числа `σi`, называемые **сингулярными числами**. Они являются квадратными корнями из собственных значений `AᵀA`.
    - `V` — ортогональная (или унитарная) матрица, столбцы которой называются **правыми сингулярными векторами**. Они являются собственными векторами `AᵀA`.

    SVD предоставляет глубокое понимание структуры матрицы, включая ее ранг, норму и четыре фундаментальных подпространства. Оно имеет широчайшее применение в статистике (метод главных компонент), обработке сигналов, сжатии данных и решении систем линейных уравнений.

    ---
    *Информация для ответа взята со слайдов 235, 314, 332-340.*
    """
    return answer

def t46():
    """
    Ответ на вопрос:
    - Плотные и разреженные матрицы, способы хранения разреженных матриц.
    """

    answer = """
    Тема "Способы хранения разреженных матриц" напрямую не рассматривается в предоставленном материале, но различие между типами матриц подразумевается.

    **Плотная матрица (Dense Matrix)** — это матрица, у которой большинство элементов не равны нулю. Для хранения такой матрицы размера m×n требуется память для всех `m*n` элементов. Большинство стандартных алгоритмов линейной алгебры, таких как в библиотеке LAPACK, разработаны именно для плотных матриц.

    **Разреженная матрица (Sparse Matrix)** — это матрица, у которой подавляющее большинство элементов равны нулю. Хранить такую матрицу как плотную (со всеми нулями) крайне неэффективно с точки зрения памяти и вычислений.

    **Способы хранения разреженных матриц** нацелены на хранение только ненулевых элементов и их позиций. Основные форматы:
    - **Coordinate (COO)**: Хранятся три массива — значения ненулевых элементов, их строковые индексы и их столбцовые индексы.
    - **Compressed Sparse Row (CSR)**: Хранятся значения, столбцовые индексы и указатели на начало каждой строки в массиве значений. Этот формат эффективен для умножения матрицы на вектор.
    - **Compressed Sparse Column (CSC)**: Аналогичен CSR, но сжатие идет по столбцам.

    Использование специализированных форматов и алгоритмов для разреженных матриц позволяет решать задачи огромной размерности, которые были бы невозможны при использовании методов для плотных матриц.

    ---
    *Ответ построен на основе общих знаний и контекста слайдов 235, 237, где упоминаются плотные матрицы.*
    """
    return answer

def t47():
    """
    Ответ на вопрос:
    - Обыкновенные дифференциальные уравнения, численное дифференцирование. Типы ОДУ.
    """

    answer = """
    **Обыкновенное дифференциальное уравнение (ОДУ)** — это уравнение, связывающее неизвестную функцию одной переменной `y(x)` с её производными: `F(x, y, y', y'', ..., y^(n)) = 0`. Порядок ОДУ определяется наивысшей производной.

    **Численное дифференцирование** — это процесс приближённого вычисления производной функции, когда она задана дискретно (в виде набора точек) или её аналитическое выражение слишком сложно. Вместо точной производной вычисляется её аппроксимация, например, через конечные разности.

    **Типы ОДУ** классифицируются по нескольким признакам:
    1.  **По порядку**: Первого порядка (содержит `y'`), второго порядка (содержит `y''`) и т.д.
    2.  **По линейности**: **Линейные**, если неизвестная функция и её производные входят в уравнение линейно. В противном случае — **нелинейные**.
    3.  **По типу зависимости от независимой переменной**: **Автономные**, если независимая переменная `x` не входит в уравнение в явном виде (например, `y' = y²`). В противном случае — **неавтономные** (например, `y' = x*y`).

    ---
    *Информация для ответа взята со слайдов 364, 368, 371.*
    """
    return answer

def t48():
    """
    Ответ на вопрос:
    - Метод прямой разности, метод обратной разности, метод центральной разности.
    """

    answer = """
    Это три основных способа аппроксимации первой производной функции `f'(x)` в точке `xi` с использованием значений в узлах дискретной сетки.

    1.  **Прямая разность (или правая разность)** использует значение в текущей и следующей точке. Она имеет первый порядок точности O(h).
        `f'(xi) ≈ (f(x_{i+1}) - f(xi)) / h`

    2.  **Обратная разность (или левая разность)** использует значение в текущей и предыдущей точке. Она также имеет первый порядок точности O(h).
        `f'(xi) ≈ (f(xi) - f(x_{i-1})) / h`

    3.  **Центральная разность** использует значения в симметричных точках вокруг `xi`. Этот метод является более точным и имеет второй порядок точности O(h²), так как ошибки, связанные с кривизной функции, взаимно компенсируются.
        `f'(xi) ≈ (f(x_{i+1}) - f(x_{i-1})) / (2*h)`

    Выбор метода зависит от задачи: центральная разность предпочтительнее по точности, но требует знания функции с обеих сторон от точки, что не всегда возможно, например, на границе интервала.

    ---
    *Информация для ответа взята со слайдов 378-381.*
    """
    return answer

def t49():
    """
    Ответ на вопрос:
    - Локальная и глобальная ошибки, правило Симпсона, ошибка сокращения и ошибка округления, накопление ошибок.
    """

    answer = """
    Эта группа понятий описывает различные источники и типы погрешностей в численных методах.

    - **Локальная ошибка** (или ошибка усечения/аппроксимации) — это погрешность, вносимая на *одном* шаге алгоритма из-за замены точной математической операции (например, производной) её дискретным аналогом (конечной разностью).

    - **Глобальная ошибка** — это итоговая, накопленная ошибка в конце вычислений. Она является результатом суммирования и распространения локальных ошибок на всех шагах. Порядок глобальной ошибки обычно на единицу ниже порядка локальной.

    - **Ошибка сокращения (truncation error)** — синоним локальной ошибки, возникает из-за "усечения" бесконечного ряда (например, ряда Тейлора) до конечного числа членов.

    - **Ошибка округления (round-off error)** возникает из-за ограниченной точности представления чисел в компьютере.

    - **Накопление ошибок**: В ходе вычислений локальные ошибки и ошибки округления с каждого шага переносятся на следующий, суммируются и могут расти, приводя к большой глобальной ошибке.

    - **Правило Симпсона** — это метод численного интегрирования, а не дифференцирования. Оно аппроксимирует функцию на отрезке параболой и имеет более высокий порядок точности (O(h⁴)), чем методы трапеций или прямоугольников.

    ---
    *Информация для ответа взята со слайдов 14, 15, 401.*
    """
    return answer

def t50():
    """
    Ответ на вопрос:
    - Сетка дифференцирования.
    """

    answer = """
    **Сетка** (или сеточная область) — это фундаментальное понятие в численных методах, которое позволяет перейти от непрерывной задачи к дискретной. Она представляет собой набор дискретных точек (узлов) `{x_i}`, в которых ищется приближенное решение.

    Основные характеристики сетки:
    - **Узлы**: Точки `x_i`, где вычисляются значения функции.
    - **Шаг сетки (h)**: Расстояние между соседними узлами.

    Типы сеток:
    1.  **Равномерная сетка**: Шаг `h` постоянен на всем интервале: `h_i = h = const`. Это наиболее простой и часто используемый тип сетки.
    2.  **Неравномерная сетка**: Шаг `h_i = x_{i+1} - x_i` может меняться. Такие сетки используются, когда поведение функции сильно различается на разных участках: сетку делают более густой там, где функция быстро меняется, и более редкой, где она меняется плавно.

    Выбор сетки — это компромисс между точностью (требуется мелкий шаг `h`) и вычислительными затратами (мелкий шаг увеличивает количество узлов и операций).

    ---
    *Информация для ответа взята со слайдов 371, 372, 374.*
    """
    return answer

def t51():
    """
    Ответ на вопрос:
    - Фазовые портреты, особые точки.
    """

    answer = """
    **Фазовый портрет** — это графический метод анализа систем дифференциальных уравнений, который позволяет визуализировать их поведение без нахождения явного решения. Он представляет собой набор **траекторий** в **фазовом пространстве**, где осями являются зависимые переменные системы (например, `x` и `y` для системы из двух ОДУ). Каждая траектория показывает, как состояние системы эволюционирует во времени из определенного начального условия.

    **Особые точки** (или точки равновесия) — это ключевые элементы фазового портрета. В этих точках производные всех переменных равны нулю (`x' = 0`, `y' = 0`), то есть система находится в состоянии покоя. Тип особой точки определяет локальное поведение траекторий вокруг нее:
    - **Узел**: Траектории либо сходятся к точке, либо расходятся от нее.
    - **Фокус**: Траектории закручиваются в спираль, сходясь к точке или расходясь.
    - **Седло**: Траектории приближаются к точке по одним направлениям и удаляются по другим.
    - **Центр**: Траектории образуют замкнутые кривые вокруг точки.

    Анализ фазового портрета и его особых точек позволяет определить устойчивость системы, наличие колебательных режимов и ее общее динамическое поведение.

    ---
    *Информация для ответа взята со слайдов 404, 405, 419-421.*
    """
    return answer

def t52():
    """
    Ответ на вопрос:
    - Неявные и явные методы численного дифференцирования.
    """

    answer = """
    При численном решении ОДУ вида `y' = f(x, y)` методы делятся на явные и неявные в зависимости от того, как вычисляется значение `y_{i+1}` на следующем шаге.

    **Явные методы** вычисляют `y_{i+1}` напрямую, используя только уже известные значения с предыдущих шагов (`y_i`, `y_{i-1}` и т.д.). Классический пример — **явный метод Эйлера**:
    `y_{i+1} = y_i + h * f(x_i, y_i)`
    Они просты в реализации, но часто имеют строгие ограничения на величину шага `h` для обеспечения устойчивости.

    **Неявные методы** для вычисления `y_{i+1}` используют уравнение, в которое входит само это неизвестное значение. Пример — **неявный метод Эйлера**:
    `y_{i+1} = y_i + h * f(x_{i+1}, y_{i+1})`
    На каждом шаге для нахождения `y_{i+1}` требуется решать (часто нелинейное) уравнение. Это делает их вычислительно более сложными, но они, как правило, обладают лучшими свойствами устойчивости и позволяют использовать значительно больший шаг `h`, что особенно важно для решения **жёстких систем ОДУ**.

    ---
    *Информация для ответа взята со слайдов 396, 397, 517, 525, 537.*
    """
    return answer

def t53():
    """
    Ответ на вопрос:
    - Многошаговые методы решения обыкновенных дифференциальных уравнений.
    """

    answer = """
    **Многошаговые методы**, в отличие от одношаговых (как Эйлер или Рунге-Кутта), используют для вычисления значения `y_{i+1}` информацию из нескольких предыдущих точек (`y_i`, `y_{i-1}`, `y_{i-2}` и т.д.). Идея этих методов заключается в аппроксимации интеграла от правой части ОДУ `f(x, y)` на отрезке `[x_i, x_{i+1}]` с помощью интерполяционного полинома, построенного по предыдущим значениям.

    Основное преимущество многошаговых методов — их вычислительная эффективность. После "разгона" (вычисления первых нескольких точек одношаговым методом) они требуют всего одно вычисление функции `f(x,y)` на шаге, в то время как, например, метод Рунге-Кутты 4-го порядка требует четыре.

    Многошаговые методы делятся на:
    - **Явные** (например, методы Адамса-Башфорта), где формула для `y_{i+1}` не содержит этого значения.
    - **Неявные** (например, методы Адамса-Мультона), которые более устойчивы, но требуют решения уравнения на каждом шаге.
    На практике часто используют их комбинацию в виде методов **предиктор-корректор**.

    ---
    *Информация для ответа взята со слайдов 533-542.*
    """
    return answer

def t54():
    """
    Ответ на вопрос:
    - Использование адаптивного шага.
    """

    answer = """
    Использование **адаптивного шага** — это продвинутая техника в численных методах решения ОДУ, позволяющая автоматически изменять размер шага `h` в процессе вычислений. Цель состоит в том, чтобы поддерживать локальную ошибку на заданном уровне, обеспечивая баланс между точностью и эффективностью.

    Основная идея:
    1.  На каждом шаге делается оценка локальной погрешности. Часто это делается путем сравнения результатов, полученных методами разного порядка точности, или путем расчета по одной и той же схеме с шагом `h` и двумя шагами `h/2` (метод Рунге).
    2.  Если оцененная ошибка больше заданной, шаг `h` уменьшается, и вычисление повторяется.
    3.  Если ошибка значительно меньше заданной, шаг `h` можно увеличить на следующей итерации для экономии вычислительных ресурсов.

    Адаптивный шаг особенно эффективен для уравнений, где решение меняется с разной скоростью: шаг уменьшается на участках с быстрым изменением функции и увеличивается на участках, где она меняется плавно. Одношаговые методы, такие как методы Рунге-Кутты, хорошо подходят для реализации с адаптивным шагом.

    ---
    *Информация для ответа взята со слайдов 532, 543-545.*
    """
    return answer

def t55():
    """
    Ответ на вопрос:
    - Понятия согласованности, устойчивости, сходимости алгоритмов.
    """

    answer = """
    Это три фундаментальных понятия, характеризующих качество численного метода для решения дифференциальных уравнений.

    1.  **Согласованность (Consistency)**: Метод называется согласованным, если его разностная схема правильно аппроксимирует исходное дифференциальное уравнение. Формально это означает, что локальная ошибка аппроксимации (невязка) стремится к нулю, когда шаг сетки `h` стремится к нулю.

    2.  **Устойчивость (Stability)**: Метод устойчив, если небольшие возмущения (например, ошибки округления или неточности в начальных данных) не приводят к неконтролируемому росту ошибок в решении. Устойчивость гарантирует, что ошибки остаются ограниченными в процессе вычислений.

    3.  **Сходимость (Convergence)**: Метод сходится, если его численное решение стремится к точному решению дифференциального уравнения при уменьшении шага сетки `h`.

    Эти три понятия связаны **Основной теоремой вычислительной математики (теоремой Лакса-Рябенького-Филипова)**, которая гласит: **Согласованность + Устойчивость = Сходимость**. То есть, для того чтобы численный метод давал правильный результат, он должен быть и согласованным, и устойчивым.

    ---
    *Информация для ответа взята со слайдов 387-392, 505, 511, 546-552.*
    """
    return answer

def t56():
    """
    Ответ на вопрос:
    - Строгая и нестрогая (слабая) устойчивость.
    """

    answer = """
    Понятия строгой и нестрогой устойчивости характеризуют поведение ошибок в численном методе на длительных интервалах времени. Они связаны с поведением собственных значений `λi` оператора перехода `R_h`.

    **Строгая устойчивость** требуется для моделирования долгосрочных процессов или при неопределённом количестве шагов. Метод строго устойчив, если ошибки гарантированно затухают со временем. Это соответствует условию, что модуль всех собственных значений оператора перехода строго меньше единицы: `|λi| < 1`. В этом случае любая ошибка со временем экспоненциально уменьшается.

    **Нестрогая (слабая) устойчивость** допустима для задач с конечным и известным числом шагов. Она требует, чтобы ошибки оставались ограниченными, но не обязательно затухали. Условием является `|λi| ≤ 1`, при этом для собственных значений, лежащих на единичной окружности (`|λi| = 1`), должны быть ограничены соответствующие корневые векторы. Это означает, что ошибка может расти, но не быстрее, чем полиномиально (например, линейно), что приемлемо на коротких интервалах.

    ---
    *Информация для ответа взята со слайдов 506, 509, 514, 515, 518.*
    """
    return answer

def t57():
    """
    Ответ на вопрос:
    - Детерминированный хаос, бифуркация, странные аттракторы.
    """

    answer = """
    Эти понятия описывают сложное, непредсказуемое поведение в детерминированных динамических системах.

    **Детерминированный хаос** — это поведение системы, которое, несмотря на полное отсутствие случайных факторов (система детерминирована), выглядит случайным и непредсказуемым. Ключевой его особенностью является **чувствительность к начальным условиям** ("эффект бабочки"): малейшие изменения в начальном состоянии приводят к экспоненциально расходящимся траекториям в будущем.

    **Бифуркация** — это качественное изменение поведения системы (например, смена устойчивого состояния на колебательное) при плавном изменении ее управляющего параметра. Точка, в которой происходит такое изменение, называется точкой бифуркации. Каскад бифуркаций удвоения периода является одним из классических сценариев перехода системы к хаосу.

    **Странный аттрактор** — это притягивающее множество в фазовом пространстве, к которому стремятся траектории хаотической системы. В отличие от простых аттракторов (точка, предельный цикл), странный аттрактор имеет фрактальную структуру. Траектория на странном аттракторе никогда не повторяется и не пересекает саму себя, бесконечно блуждая по этому сложному множеству. Примерами являются аттрактор Лоренца и аттрактор Рёсслера.

    ---
    *Информация для ответа взята со слайдов 562-582.*
    """
    return answer

def t58():
    """
    Ответ на вопрос:
    - Амплитуда, период, частота, длин волны, дискретизация, частота дискретизации, герц, угловая частота, фаза сигнала.
    """

    answer = """
    Эти термины являются основными характеристиками колебательных процессов и сигналов.

    - **Амплитуда**: Максимальное отклонение колеблющейся величины от положения равновесия.
    - **Период (T)**: Время одного полного колебания.
    - **Длина волны (λ)**: Пространственное расстояние, через которое повторяется форма волны.
    - **Частота (f)**: Число колебаний в единицу времени. Измеряется в **Герцах (Гц)**, где 1 Гц = 1 колебание в секунду. Связана с периодом как `f = 1/T`.
    - **Угловая (круговая) частота (ω)**: Скорость изменения фазы. Связана с обычной частотой как `ω = 2πf`. Измеряется в радианах в секунду.
    - **Фаза (φ)**: Определяет начальное состояние колебания в момент времени t=0. Показывает, на какую долю периода смещено колебание относительно начала отсчета.
    - **Дискретизация**: Процесс представления непрерывного сигнала в виде последовательности его значений (отсчетов), взятых в дискретные моменты времени.
    - **Частота дискретизации (Fs)**: Число отсчетов, взятых в секунду. Для корректного представления сигнала она должна быть как минимум в два раза больше максимальной частоты в спектре сигнала (теорема Котельникова).

    ---
    *Информация для ответа взята со слайдов 584, 586, 590, 601, 602, 659.*
    """
    return answer

def t59():
    """
    Ответ на вопрос:
    - Амплитудный спектр и частотный спектр.
    """

    answer = """
    **Частотный спектр** (или просто спектр) сигнала — это представление сигнала в частотной области, полученное с помощью преобразования Фурье. Он показывает, из каких гармонических колебаний (синусоид и косинусоид) и с какими параметрами состоит исходный сигнал. Для комплексного представления ряда Фурье `S(t) = Σ Ck * exp(j*ωk*t)`, спектр — это набор комплексных коэффициентов `Ck`.

    Поскольку коэффициенты `Ck` являются комплексными числами, их удобнее представлять в виде двух вещественных спектров:
    1.  **Амплитудный спектр**: Это зависимость амплитуд `|Ck|` гармоник от их частот `ωk`. Он показывает "силу" или "вклад" каждой частотной составляющей в общий сигнал.
    2.  **Фазовый спектр**: Это зависимость фаз `arg(Ck)` гармоник от их частот. Он показывает начальные сдвиги каждой гармоники относительно начала отсчета.

    Вместе амплитудный и фазовый спектры полностью описывают сигнал в частотной области и позволяют восстановить исходный сигнал без потерь информации. Анализ этих спектров является основой обработки сигналов.

    ---
    *Информация для ответа взята со слайдов 588, 601, 602.*
    """

    code = """
import numpy as np
import matplotlib.pyplot as plt

# Параметры сигнала
fs = 1000  # Частота дискретизации
t = np.linspace(0, 1, fs, endpoint=False)
# Сигнал = 10 Гц + 50 Гц
signal = np.sin(2 * np.pi * 10 * t) + 0.5 * np.sin(2 * np.pi * 50 * t + np.pi/4)

# Преобразование Фурье
fft_signal = np.fft.fft(signal)
freqs = np.fft.fftfreq(len(signal), 1/fs)

# Берем только положительную часть спектра
n = len(freqs)
positive_freqs = freqs[:n//2]
amplitude_spectrum = (2/n) * np.abs(fft_signal[:n//2])
phase_spectrum = np.angle(fft_signal[:n//2])

fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 7))
fig.suptitle('Амплитудный и фазовый спектры')

# Амплитудный спектр
ax1.plot(positive_freqs, amplitude_spectrum)
ax1.set_title('Амплитудный спектр')
ax1.set_xlabel('Частота (Гц)')
ax1.set_ylabel('Амплитуда')
ax1.grid(True)
ax1.set_xlim(0, 100)

# Фазовый спектр
ax2.plot(positive_freqs, phase_spectrum)
ax2.set_title('Фазовый спектр')
ax2.set_xlabel('Частота (Гц)')
ax2.set_ylabel('Фаза (радианы)')
ax2.grid(True)
ax2.set_xlim(0, 100)

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
# plt.show()
print("Код для построения амплитудного и фазового спектров:")
    """
    return answer + "\n\n" + code

def t60():
    """
    Ответ на вопрос:
    - Фильтрация сигналов.
    """

    answer = """
    **Фильтрация сигналов** — это процесс обработки сигнала с целью удаления или ослабления нежелательных компонентов (например, шума) и/или выделения полезных. В частотной области эта задача решается особенно эффективно.

    Процедура цифровой фильтрации с помощью преобразования Фурье состоит из трех основных шагов:
    1.  **Прямое преобразование Фурье**: Исходный зашумленный сигнал переводится из временной области в частотную, то есть вычисляется его спектр.
    2.  **Обработка спектра**: В полученном спектре "обнуляются" или ослабляются те частотные составляющие (гармоники), которые соответствуют шуму. Для этого спектр сигнала умножается на частотную характеристику фильтра (например, фильтра нижних частот, который пропускает низкие частоты и подавляет высокие).
    3.  **Обратное преобразование Фурье**: Модифицированный спектр преобразуется обратно во временную область. В результате получается отфильтрованный сигнал.

    Этот метод эффективен, когда полезный сигнал и шум занимают разные частотные диапазоны.

    ---
    *Информация для ответа взята со слайдов 603, 604, 605, 641.*
    """
    return answer

def t61():
    """
    Ответ на вопрос:
    - Применение преобразований Фурье с целью анализа сезонности во временных рядах.
    """

    answer = """
    Преобразование Фурье является мощным инструментом для анализа и удаления **сезонности** во временных рядах (например, в данных о потреблении электроэнергии). Сезонность проявляется в виде периодических колебаний (суточных, недельных, годовых), которые в частотном спектре соответствуют ярко выраженным пикам на определенных частотах.

    Процесс анализа и удаления сезонности ("десезонирования") включает следующие шаги:
    1.  **Вычисление спектра**: С помощью быстрого преобразования Фурье (БПФ) вычисляется спектр временного ряда.
    2.  **Идентификация сезонных частот**: На графике амплитудного спектра находятся пики, соответствующие периодическим компонентам (например, пик на частоте 1/24 часа для суточного цикла).
    3.  **Фильтрация**: Амплитуды этих сезонных гармоник в спектре обнуляются или значительно уменьшаются.
    4.  **Обратное преобразование Фурье**: Выполняется обратное БПФ отфильтрованного спектра. В результате получается "десезонированный" временной ряд, который показывает основной тренд и случайные колебания без периодических наслоений.

    Этот подход позволяет выделить и проанализировать долгосрочные тренды, которые были скрыты за сезонными колебаниями.

    ---
    *Информация для ответа взята из постановки домашнего задания #9 на слайдах 664-666.*
    """
    return answer

def t62():
    """
    Ответ на вопрос:
    - Алгоритмическая сложность, сходимость всех методов, которые могут встретиться в задачах.
    """

    answer = """
    Оценка сложности и сходимости является ключевой для выбора подходящего численного метода.

    **1. Решение нелинейных уравнений:**
    - **Метод половинного деления**: Гарантированно сходится (линейная сходимость, `p=1`), но медленно. Сложность шага O(1).
    - **Метод простой итерации**: Линейная сходимость (`p=1`), сходится при `|φ'(x)| < 1`. Сложность шага O(1).
    - **Метод Ньютона**: Очень быстрая квадратичная сходимость (`p=2`). Сходится при достаточно хорошем начальном приближении. Сложность шага O(1), но требует вычисления производной.
    - **Метод секущих (хорд)**: Сверхлинейная сходимость (`p ≈ 1.618`). Не требует производной.

    **2. Интерполяция:**
    - **Полином Лагранжа/Ньютона**: Сложность построения O(n²), вычисления в точке O(n). Неустойчив при большом `n`.
    - **Кубические сплайны**: Сложность построения O(n) (решение трёхдиагональной системы), вычисления O(log n). Устойчив.

    **3. Линейная алгебра:**
    - **Умножение матриц**: Наивный/блочный - O(n³), Штрассена - O(n^2.807).
    - **QR-алгоритм (поиск всех собственных значений)**: Общая сложность O(n³). Сходимость быстрая (квадратичная со сдвигами).
    - **Степенной метод (поиск одного λ_max)**: Сложность шага O(n²) для плотных матриц. Сходимость линейная, зависит от зазора `|λ₂/λ₁|`.

    **4. Решение ОДУ:**
    - **Метод Эйлера**: Первый порядок точности (сходимости), `p=1`. Сложность шага O(1). Условно устойчив.
    - **Метод Рунге-Кутты 4-го порядка**: Четвёртый порядок точности, `p=4`. Сложность шага O(1), но требует 4 вычисления правой части. Более устойчив, чем метод Эйлера.

    **5. Преобразование Фурье:**
    - **Дискретное преобразование Фурье (ДПФ)**: Сложность O(N²).
    - **Быстрое преобразование Фурье (БПФ)**: Значительно эффективнее, сложность O(N log N).

    ---
    *Ответ является обобщением информации со всех слайдов презентации.*
    """
    return answer

def info2():
    """
    Возвращает первый блок вопросов (1-21).
    """
    questions_block_1 = [
        ("Работа с числами с плавающей точкой.", t1),
        ("Архитектура памяти.", t2),
        ("Переполнение (overflow), потеря точности (underflow).", t3),
        ("Ошибка округления в арифметике с плавающей точкой, накопление ошибок округления, потеря значимости.", t4),
        ("Суммирование по Кахану.", t5),
        ("Абсолютная и относительная погрешности.", t6),
        ("Округление и значащие цифры в записи приближенного числа.", t7),
        ("Верные в строгом (узком) смысле цифры числа, верные в широком смысле цифры числа.", t8),
        ("Сложность алгоритмов и нотация big-O.", t9),
        ("Профилирование кода в Python.", t10),
        ("Представление чисел с плавающей точкой (стандарт IEEE 754), ошибки представления.", t11),
        ("Способы изолирования корней нелинейных функций.", t12),
        ("Сжимающие отображения.", t13),
        ("Погрешность и критерии сходимости, константа Липшица.", t14),
        ("Скорость сходимости итерационного алгоритма.", t15),
        ("Стабильность и распространение ошибки в методах численного решения нелинейных уравнений.", t16),
        ("Теория сжимающих отображений.", t17),
        ("Интерполяция, экстраполяция, аппроксимация.", t18),
        ("Глобальная и локальная интерполяция.", t19),
        ("Ступенчатая и линейная интерполяция.", t20),
        ("Интерполяционные полиномы.", t21)
    ]
    return questions_block_1

def info3():
    """
    Возвращает второй блок вопросов (22-42).
    """
    questions_block_2 = [
        ("Квадратичная интерполяция.", t22),
        ("Интерполяция сплайнами.", t23),
        ("Интерполяционный полином Лагранжа.", t24),
        ("Метод кубической сплайн-интерполяции.", t25),
        ("Основные операции в вычислительной линейной алгебре.", t26),
        ("Эффективная реализация алгоритмов вычисления произведения матриц.", t27),
        ("Алгоритм Штрассена, сложность метода Штрассена.", t28),
        ("Вычисление SVD.", t29),
        ("Собственные векторы, собственные значения.", t30),
        ("Разложение по собственным векторам.", t31),
        ("Задача Google PageRank.", t32),
        ("Вычисление собственых значений с помощью характеристического многочлена.", t33),
        ("Особенности степенного метода. Скорость сходимости.", t34),
        ("Круги Гершгорина, теорема Гершгорина.", t35),
        ("Теорема Шура.", t36),
        ("Нормальные матрицы, унитарно диагонализуемые матрицы, унитарные матрицы, эрмитовы матрицы.", t37),
        ("Верхне-гессенбергова форма матрицы.", t38),
        ("Приведение произвольной матрицы к верхне-гессенберговой форме", t39),
        ("Отношение Релея.", t40),
        ("Зазор между собственными значениями в матрице, алгоритмы со сдвигами.", t41),
        ("Отражения Хаусхолдера.", t42)
    ]
    return questions_block_2

def info4():
    """
    Возвращает третий блок вопросов (43-62).
    """
    questions_block_3 = [
        ("Сходимость и сложность QR алгоритма.", t43),
        ("Метод главных компонент и поиск сингулярных значений, прикладные аспекты.", t44),
        ("Сингулярное разложение (SVD).", t45),
        ("Плотные и разреженные матрицы, способы хранения разреженных матриц.", t46),
        ("Обыкновенные дифференциальные уравнения, численное дифференцирование. Типы ОДУ.", t47),
        ("Метод прямой разности, метод обратной разности, метод центральной разности.", t48),
        ("Локальная и глобальная ошибки, правило Симпсона, ошибка сокращения и ошибка округления, накопление ошибок.", t49),
        ("Сетка дифференцирования.", t50),
        ("Фазовые портреты, особые точки.", t51),
        ("Неявные и явные методы численного дифференцирования.", t52),
        ("Многошаговые методы решения обыкновенных дифференциальных уравнений.", t53),
        ("Использование адаптивного шага.", t54),
        ("Понятия согласованности, устойчивости, сходимости алгоритмов.", t55),
        ("Строгая и нестрогая (слабая) устойчивость.", t56),
        ("Детерминированный хаос, бифуркация, странные аттракторы.", t57),
        ("Амплитуда, период, частота, длин волны, дискретизация, частота дискретизации, герц, угловая частота, фаза сигнала.", t58),
        ("Амплитудный спектр и частотный спектр.", t59),
        ("Фильтрация сигналов.", t60),
        ("Применение преобразований Фурье с целью анализа сезонности во временных рядах.", t61),
        ("Алгоритмическая сложность, сходимость всех методов, которые могут встретиться в задачах.", t62)
    ]
    return questions_block_3

info2()