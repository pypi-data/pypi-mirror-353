# Copyright International Business Machines Corp, 2025
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import asyncio
import time

class BadminCollector:
    """
    Collection module for badmin commands
    """
    def __init__(self, metrics_generator) -> None:

        # Command used to collect data
        self.command = "badmin"

        # Class used to create and manage metrics
        self.metrics_generator = metrics_generator

        # 1. PERFMON VIEW
        # Tuple's List of: metric name, metric description, metric type (Gauge, Info, Counter), metric labels list to create metric, metric label to fetch data
        self.metrics_list_lsf_perf = [("ibm_lsf_performance_gauge", "LSF Scheduler Performance", "Gauge", ['type', 'cluster_name', 'metric'], "")]

        # Metrics list generated by metrics_generator class
        self.metrics_lsf_perf = self.metrics_generator.create_metrics(self.metrics_list_lsf_perf)

        # 2. RC VIEW
        # Tuple's List of: metric name, metric description, metric type (Gauge, Info, Counter), metric labels list to create metric, metric label to fetch data
        self.metrics_list_lsf_rc_view = [("ibm_lsf_rc_targets", "LSF Resource Connector VM Targets", "Gauge", ['provider', 'template_id', 'rc_account', 'cluster_name'], ""), ("ibm_lsf_rc_requests", "LSF Resource Connector VM Requests", "Gauge", ['provider', 'template_id', 'rc_account', 'state', 'cluster_name'], "")]

        # Metrics list generated by metrics_generator class
        self.metrics_lsf_rc_view = self.metrics_generator.create_metrics(self.metrics_list_lsf_rc_view)

        # 3. SHOW STATUS
        # Tuple's List of: metric name, metric description, metric type (Gauge, Info, Counter), metric labels list to create metric, metric label to fetch data
        self.metrics_list_showstatus = [("ibm_lsf_servers", "LSF Servers", "Gauge", ['status', 'cluster_name'], ""), ("ibm_lsf_jobs", "LSf Jobs", "Gauge",['state', 'cluster_name'], ""),
                                        ("ibm_lsf_users", "LSF Users", "Gauge", ['state', 'cluster_name'], ""), ("ibm_lsf_available_hosts", "LSF Hosts", "Gauge", ['resource', 'cluster_name'], "")]

        # Metrics list generated by metrics_generator class
        self.metrics_showstatus = self.metrics_generator.create_metrics(self.metrics_list_showstatus)

    def fetch_perfmon_view(self, cluster_info):
        """
        Collects info from badmin perfmon view
        """
        asyncio.run(self.fetch_perfmon_view_async(cluster_info), debug=self.metrics_generator.debug)

    async def fetch_perfmon_view_async(self, cluster_info):
        """
        Collects info from badmin perfmon view
        """
        cluster_name = cluster_info["cluster_name"]
        cmd = [self.command, 'perfmon', 'view', '-json']
        perfmon, _, return_bool = await self.metrics_generator.collect_data_async(cmd, "", "ibm_lsf_performance", "perfmon unvailable")

        if return_bool:
            labels_couple = [("Processed requests: mbatchd","processed-requests"), ("Job information queries", "job-queries"),
                             ("Host information queries", "host-queries"), ("Queue information queries", "queue-queries"),
                             ("Jobs submitted", "jobs-submitted"), ("Jobs dispatched", "jobs-dispatched"),
                             ("Jobs completed", "jobs-completed"), ("Jobs sent to remote cluster", "jobs-forwarded"),
                             ("Jobs accepted from remote cluster", "jobs-received"), ("Scheduling interval in second(s)", "scheduling-interval"),
                             ("Matching host criteria", "match-criteria"), ("Job buckets", "job-buckets")]

            for (metric, name, _) in self.metrics_lsf_perf:
                # First of all clean up the metric
                try:
                    metric.clear()
                except:
                    pass
                for record in perfmon['record'] or []:
                    for (to_check, to_set) in labels_couple:
                        if name == "ibm_lsf_performance_gauge" and record['name'] == to_check:
                            if self.metrics_generator.debug:
                                print(f"ibm_lsf_performance cluster_name={cluster_name} {record['name']}[current={record['current']} max={record['max']} min={record['min']} avg={record['avg']} total={record['total']}]")
                            metric.labels(to_set, cluster_name, "current").set(record['current'])
                            metric.labels(to_set, cluster_name, "max").set(record['max'])
                            metric.labels(to_set, cluster_name, "min").set(record['min'])
                            metric.labels(to_set, cluster_name, "avg").set(record['avg'])
                            metric.labels(to_set, cluster_name, "total").set(record['total'])

                if perfmon['fd'] and perfmon['fd']['name'] == "mbatchd file descriptor usage":
                    if self.metrics_generator.debug:
                        print(f"ibm_lsf_performance cluster_name={cluster_name} {perfmon['fd']['name']} free={perfmon['fd']['free']} used={perfmon['fd']['used']}")
                    if name == "ibm_lsf_performance_gauge":
                        metric.labels("file-descriptors-free", cluster_name, "current").set(perfmon['fd']['free'])
                        metric.labels("file-descriptors-used", cluster_name, "current").set(perfmon['fd']['used'])

    def fetch_rc_view(self, cluster_info):
        """
        Collects info from badmin rc view
        """
        asyncio.run(self.fetch_rc_view_async(cluster_info), debug=self.metrics_generator.debug)

    async def fetch_rc_view_async(self, cluster_info):
        """
        Collects info from badmin rc view
        """
        cluster_name = cluster_info["cluster_name"]
        cmd = [self.command,'rc','view','-json']
        rc_view, _, return_bool = await self.metrics_generator.collect_data_async(cmd, "", "ibm_lsf_rc", "rc view unvailable")
        if return_bool:
            for (metric, name, _) in self.metrics_lsf_rc_view:
                # First of all clean up the metric
                metric.clear()
                if rc_view['numProviders'] > 0:
                    for provider in rc_view['providers'] or []:
                        for demand in provider['demand'] or []:
                            if self.metrics_generator.debug:
                                print(f"ibm_lsf_rc cluster_name={cluster_name} provider={provider['name']} template-id={demand['templName']} rc-account={demand['account']}",
                                      f"target={demand['demand']} processing={demand['processing']} waiting={demand['waiting']} joined={demand['joined']}")
                            if name == "ibm_lsf_rc_targets":
                                for label in ["demand"]:
                                    metric.labels(provider['name'], demand['templName'], demand['account'], cluster_name).set(demand[label])
                            if name == "ibm_lsf_rc_requests":
                                for (label1, label2) in [("processing", "processing"), ("available", "waiting"), ("unavailable", "joined")]:
                                    metric.labels(provider['name'], demand['templName'], demand['account'], label1, cluster_name).set(demand[label2])

    def fetch_showstatus(self, cluster_info) -> None:
        """
        Collects info from badmin showstatus
        """
        asyncio.run(self.fetch_showstatus_async(cluster_info), debug=self.metrics_generator.debug)

    async def fetch_showstatus_async(self, cluster_info) -> None:
        """
        Collects info from badmin showstatus
        """
        cluster_name = cluster_info["cluster_name"]
        time_nanosec = time.time_ns()
        cmd = [self.command, 'showstatus']
        _, stdout, return_bool = await self.metrics_generator.collect_data_async(cmd, "", "ibm_lsf_showstatus", "showstatus unvailable")

        if return_bool:
            showstatus = stdout.split()
            for (metric, name, _) in self.metrics_showstatus:
                # First of all clean up the metric
                metric.clear()
                #
                # LSF server status
                #
                if name == "ibm_lsf_servers":
                    count = 21
                    for label in ["total", "ok", "closed", "unreachable", "unavailable"]:
                        if self.metrics_generator.debug:
                            print("lsf_servers,", f"cluster_name={cluster_name}", f"state={label}"," value=",showstatus[count],"i ",time_nanosec,sep='')
                        metric.labels(label, cluster_name).set(showstatus[count])
                        count += 2
                #
                # LSF job status
                #
                if name == "ibm_lsf_jobs":
                    count = 33
                    for label in ["total", "running", "suspended", "pending", "finished"]:
                        if self.metrics_generator.debug:
                            print("ibm_lsf_jobs,", f"cluster_name={cluster_name}", f"state={label}"," value=",showstatus[count],"i ",time_nanosec,sep='')
                        metric.labels(label, cluster_name).set(showstatus[count])
                        count += 2
                #
                # LSF user stats
                #
                if name == "ibm_lsf_users":
                    count = 50
                    for label in ["numgroups", "numactive"]:
                        if self.metrics_generator.debug:
                            print("ibm_lsf_users,", f"cluster_name={cluster_name}", f"state={label}"," value=",showstatus[count],"i ",time_nanosec,sep='')
                        metric.labels(label, cluster_name).set(showstatus[count])
                        count += 5
                #
                # LSF hosts stats
                # First we split out the current and peak values for clients, servers, cpus, cores, and slots.
                # The current and peak values are separated by the "/" delimiter.
                #
                if name == "ibm_lsf_available_hosts":
                    clientssplit = showstatus[9].split("/")
                    serverssplit = showstatus[11].split("/")
                    cpussplit = showstatus[13].split("/")
                    coressplit = showstatus[15].split("/")
                    slotssplit = showstatus[17].split("/")
                    for (label, data) in [("clients", clientssplit),("servers", serverssplit),("cpus", cpussplit),("cores", coressplit),("slots", slotssplit)]:
                        metric.labels(label, cluster_name).set(data[0])
                        if self.metrics_generator.debug:
                            print("ibm_lsf_available_hosts,", f"cluster_name={cluster_name}", f"state={label}"," current=",data[0],"i,","peak=",data[1],"i ",time_nanosec,sep='')
