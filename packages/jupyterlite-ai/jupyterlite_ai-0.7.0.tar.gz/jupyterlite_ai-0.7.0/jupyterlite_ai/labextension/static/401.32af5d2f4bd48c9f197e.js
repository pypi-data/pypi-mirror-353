"use strict";(self.webpackChunk_jupyterlite_ai=self.webpackChunk_jupyterlite_ai||[]).push([[401],{70618:(e,t,n)=>{n.d(t,{l:()=>i});const i=(e,t)=>e.reduce(((e,n,i)=>{const r=Math.floor(i/t),a=e[r]||[];return e[r]=a.concat([n]),e}),[])},33030:(e,t,n)=>{n.d(t,{J:()=>r});var i=n(71563);class r{constructor(e){Object.defineProperty(this,"caller",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),this.caller=new i.g(e??{})}}},16169:(e,t,n)=>{n.d(t,{dv:()=>m,sD:()=>l,cT:()=>c,u$:()=>s});var i=n(70041),r=n(52514),a=n(66286),o=n(13792);function s(e,t){if(void 0===e.function)return;let n;if(t?.partial)try{n=(0,r.d1)(e.function.arguments??"{}")}catch(e){return}else try{n=JSON.parse(e.function.arguments)}catch(t){throw new i.CC([`Function "${e.function.name}" arguments:`,"",e.function.arguments,"","are not valid JSON.",`Error: ${t.message}`].join("\n"))}const a={name:e.function.name,args:n,type:"tool_call"};return t?.returnId&&(a.id=e.id),a}function l(e){if(void 0===e.id)throw new Error('All OpenAI tool calls must have an "id" field.');return{id:e.id,type:"function",function:{name:e.name,arguments:JSON.stringify(e.args)}}}function c(e,t){return{name:e.function?.name,args:e.function?.arguments,id:e.id,error:t,type:"invalid_tool_call"}}class u extends a.T{static lc_name(){return"JsonOutputToolsParser"}constructor(e){super(e),Object.defineProperty(this,"returnId",{enumerable:!0,configurable:!0,writable:!0,value:!1}),Object.defineProperty(this,"lc_namespace",{enumerable:!0,configurable:!0,writable:!0,value:["langchain","output_parsers","openai_tools"]}),Object.defineProperty(this,"lc_serializable",{enumerable:!0,configurable:!0,writable:!0,value:!0}),this.returnId=e?.returnId??this.returnId}_diff(){throw new Error("Not supported.")}async parse(){throw new Error("Not implemented.")}async parseResult(e){return await this.parsePartialResult(e,!1)}async parsePartialResult(e,t=!0){const n=e[0].message;let i;if((0,o.KX)(n)&&n.tool_calls?.length?i=n.tool_calls.map((e=>{const{id:t,...n}=e;return this.returnId?{id:t,...n}:n})):void 0!==n.additional_kwargs.tool_calls&&(i=JSON.parse(JSON.stringify(n.additional_kwargs.tool_calls)).map((e=>s(e,{returnId:this.returnId,partial:t})))),!i)return[];const r=[];for(const e of i)if(void 0!==e){const t={type:e.name,args:e.args,id:e.id};r.push(t)}return r}}class m extends u{static lc_name(){return"JsonOutputKeyToolsParser"}constructor(e){super(e),Object.defineProperty(this,"lc_namespace",{enumerable:!0,configurable:!0,writable:!0,value:["langchain","output_parsers","openai_tools"]}),Object.defineProperty(this,"lc_serializable",{enumerable:!0,configurable:!0,writable:!0,value:!0}),Object.defineProperty(this,"returnId",{enumerable:!0,configurable:!0,writable:!0,value:!1}),Object.defineProperty(this,"keyName",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"returnSingle",{enumerable:!0,configurable:!0,writable:!0,value:!1}),Object.defineProperty(this,"zodSchema",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),this.keyName=e.keyName,this.returnSingle=e.returnSingle??this.returnSingle,this.zodSchema=e.zodSchema}async _validateResult(e){if(void 0===this.zodSchema)return e;const t=await this.zodSchema.safeParseAsync(e);if(t.success)return t.data;throw new i.CC(`Failed to parse. Text: "${JSON.stringify(e,null,2)}". Error: ${JSON.stringify(t.error.errors)}`,JSON.stringify(e,null,2))}async parsePartialResult(e){const t=(await super.parsePartialResult(e)).filter((e=>e.type===this.keyName));let n=t;if(t.length)return this.returnId||(n=t.map((e=>e.args))),this.returnSingle?n[0]:n}async parseResult(e){const t=(await super.parsePartialResult(e,!1)).filter((e=>e.type===this.keyName));let n=t;if(t.length)return this.returnId||(n=t.map((e=>e.args))),this.returnSingle?this._validateResult(n[0]):await Promise.all(n.map((e=>this._validateResult(e))))}}},6723:(e,t,n)=>{n.d(t,{Az:()=>i.Az,_$:()=>i._$});var i=n(76058)},85401:(e,t,n)=>{n.r(t),n.d(t,{ChatMistralAI:()=>y,MistralAI:()=>j,MistralAIEmbeddings:()=>P});var i=n(31620),r=n(5292),a=n(146),o=n(44112),s=n(6723),l=n(36172),c=n(16169),u=n(44356),m=n(6968);const d=/^[a-zA-Z0-9]{9}$/;function h(e){if(function(e){return d.test(e)}(e))return e;{const t=function(e){let t=e;const n="0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ";if(0===t)return n[0];const i=[];for(;t;)i.push(n[t%62]),t=Math.floor(t/62);return i.reverse().join("")}(function(e){let t=0;for(let n=0;n<e.length;n+=1)t=(t<<5)-t+e.charCodeAt(n),t&=t;return Math.abs(t)}(e));return t.length>=9?t.slice(0,9):t.padStart(9,"0")}}function p(e){const t=e=>{switch(e){case"human":return"user";case"ai":case"function":return"assistant";case"system":return"system";case"tool":return"tool";default:throw new Error(`Unknown message type: ${e}`)}};return e.map((e=>{const n=(e=>{if((0,r.KX)(e)&&e.tool_calls?.length)return e.tool_calls.map((e=>({...e,id:h(e.id??"")}))).map(c.sD);if(!e.additional_kwargs.tool_calls?.length)return;const t=e.additional_kwargs.tool_calls;return t?.map((e=>({id:h(e.id),type:"function",function:e.function})))})(e),i=void 0===n?(e=>{if("string"==typeof e)return e;throw new Error(`ChatMistralAI does not support non text message content. Received: ${JSON.stringify(e,null,2)}`)})(e.content):"";return"tool_call_id"in e&&"string"==typeof e.tool_call_id?{role:t(e._getType()),content:i,name:e.name,tool_call_id:h(e.tool_call_id)}:{role:t(e._getType()),content:i,tool_calls:n}}))}function f(e,t){const{message:n}=e;let a=[];if("tool_calls"in n&&Array.isArray(n.tool_calls)&&(a=n.tool_calls),"assistant"===n.role){const e=[],o=[];for(const t of a)try{const n=(0,c.u$)(t,{returnId:!0});e.push({...n,id:n.id??(0,i.A)().replace(/-/g,"")})}catch(e){o.push((0,c.cT)(t,e.message))}return new r.Od({content:n.content??"",tool_calls:e,invalid_tool_calls:o,additional_kwargs:{tool_calls:a.length?a.map((e=>({...e,type:"function"}))):void 0},usage_metadata:t?{input_tokens:t.prompt_tokens,output_tokens:t.completion_tokens,total_tokens:t.total_tokens}:void 0})}return new r.xc(n.content??"")}function b(e,t){if(!e.content&&!e.tool_calls)return t?new r.H({content:"",usage_metadata:t?{input_tokens:t.prompt_tokens,output_tokens:t.completion_tokens,total_tokens:t.total_tokens}:void 0}):null;const n=e.tool_calls?.length?e.tool_calls?.map(((e,t)=>({...e,index:t,id:e.id??(0,i.A)().replace(/-/g,""),type:"function"}))):void 0;let a="assistant";e.role&&(a=e.role);const o=e.content??"";let s;const l=[];if(void 0!==n){s={tool_calls:n};for(const e of n)l.push({name:e.function?.name,args:e.function?.arguments,id:e.id,index:e.index,type:"tool_call_chunk"})}else s={};return"user"===a?new r.a7({content:o}):"assistant"===a?new r.H({content:o,tool_call_chunks:l,additional_kwargs:s,usage_metadata:t?{input_tokens:t.prompt_tokens,output_tokens:t.completion_tokens,total_tokens:t.total_tokens}:void 0}):"tool"===a?new r.dr({content:o,additional_kwargs:s,tool_call_id:n?.[0].id??""}):"function"===a?new r.FK({content:o,additional_kwargs:s}):new r.XU({content:o,role:a})}function g(e){return e.map((e=>{if("function"in e)return e;const t=e.description??`Tool: ${e.name}`;return{type:"function",function:{name:e.name,description:t,parameters:(0,m.Ik)(e.schema)}}}))}class y extends a.xV{static lc_name(){return"ChatMistralAI"}constructor(e){super(e??{}),Object.defineProperty(this,"lc_namespace",{enumerable:!0,configurable:!0,writable:!0,value:["langchain","chat_models","mistralai"]}),Object.defineProperty(this,"modelName",{enumerable:!0,configurable:!0,writable:!0,value:"mistral-small-latest"}),Object.defineProperty(this,"model",{enumerable:!0,configurable:!0,writable:!0,value:"mistral-small-latest"}),Object.defineProperty(this,"apiKey",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"endpoint",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"temperature",{enumerable:!0,configurable:!0,writable:!0,value:.7}),Object.defineProperty(this,"streaming",{enumerable:!0,configurable:!0,writable:!0,value:!1}),Object.defineProperty(this,"topP",{enumerable:!0,configurable:!0,writable:!0,value:1}),Object.defineProperty(this,"maxTokens",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"safeMode",{enumerable:!0,configurable:!0,writable:!0,value:!1}),Object.defineProperty(this,"safePrompt",{enumerable:!0,configurable:!0,writable:!0,value:!1}),Object.defineProperty(this,"randomSeed",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"seed",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"lc_serializable",{enumerable:!0,configurable:!0,writable:!0,value:!0}),Object.defineProperty(this,"streamUsage",{enumerable:!0,configurable:!0,writable:!0,value:!0});const t=e?.apiKey??(0,s.Az)("MISTRAL_API_KEY");if(!t)throw new Error("API key MISTRAL_API_KEY is missing for MistralAI, but it is required.");this.apiKey=t,this.streaming=e?.streaming??this.streaming,this.endpoint=e?.endpoint,this.temperature=e?.temperature??this.temperature,this.topP=e?.topP??this.topP,this.maxTokens=e?.maxTokens??this.maxTokens,this.safeMode=e?.safeMode??this.safeMode,this.safePrompt=e?.safePrompt??this.safePrompt,this.randomSeed=e?.seed??e?.randomSeed??this.seed,this.seed=this.randomSeed,this.modelName=e?.model??e?.modelName??this.model,this.model=this.modelName,this.streamUsage=e?.streamUsage??this.streamUsage}get lc_secrets(){return{apiKey:"MISTRAL_API_KEY"}}get lc_aliases(){return{apiKey:"mistral_api_key"}}getLsParams(e){const t=this.invocationParams(e);return{ls_provider:"mistral",ls_model_name:this.model,ls_model_type:"chat",ls_temperature:t.temperature??void 0,ls_max_tokens:t.maxTokens??void 0}}_llmType(){return"mistral_ai"}invocationParams(e){const{response_format:t,tools:n,tool_choice:i}=e??{},r=n?.length?g(n):void 0;return{model:this.model,tools:r,temperature:this.temperature,maxTokens:this.maxTokens,topP:this.topP,randomSeed:this.seed,safeMode:this.safeMode,safePrompt:this.safePrompt,toolChoice:i,responseFormat:t}}bindTools(e,t){return this.bind({tools:g(e),...t})}async completionWithRetry(e,t){const{MistralClient:n}=await this.imports(),i=new n(this.apiKey,this.endpoint);return this.caller.call((async()=>{try{let n;return n=t?i.chatStream(e):await i.chat(e),n}catch(e){throw e.message?.includes("status: 400")&&(e.status=400),e}}))}async _generate(e,t,n){const i={},r={...this.invocationParams(t),messages:p(e)},a=!!t.signal??!!t.timeout;if(this.streaming||a){const r=this._streamResponseChunks(e,t,n),a={};for await(const e of r){const t=e.generationInfo?.completion??0;void 0===a[t]?a[t]=e:a[t]=a[t].concat(e)}return{generations:Object.entries(a).sort((([e],[t])=>parseInt(e,10)-parseInt(t,10))).map((([e,t])=>t)),llmOutput:{estimatedTokenUsage:i}}}const o=await this.completionWithRetry(r,!1),{completion_tokens:s,prompt_tokens:l,total_tokens:c}=o?.usage??{};s&&(i.completionTokens=(i.completionTokens??0)+s),l&&(i.promptTokens=(i.promptTokens??0)+l),c&&(i.totalTokens=(i.totalTokens??0)+c);const u=[];for(const e of o?.choices??[]){if("delta"in e)throw new Error("Delta not supported in non-streaming mode.");if(!("message"in e))throw new Error("No message found in the choice.");const t={text:e.message?.content??"",message:f(e,o?.usage)};e.finish_reason&&(t.generationInfo={finish_reason:e.finish_reason}),u.push(t)}return{generations:u,llmOutput:{tokenUsage:i}}}async*_streamResponseChunks(e,t,n){const i=p(e),r={...this.invocationParams(t),messages:i},a=await this.completionWithRetry(r,!0);for await(const e of a){if(t.signal?.aborted)throw new Error("AbortError");const i=e?.choices[0];if(!i||!("delta"in i))continue;const{delta:r}=i;if(!r)continue;const a={prompt:0,completion:i.index??0},s=b(r,this.streamUsage||t.streamUsage?e.usage:null);if(null===s)continue;const l=new o.Cf({message:s,text:r.content??"",generationInfo:a});yield l,n?.handleLLMNewToken(l.text??"",a,void 0,void 0,void 0,{chunk:l})}}_combineLLMOutput(){return[]}withStructuredOutput(e,t){let n,i,r,a;var o;let s,d;if(void 0!==(o=e)&&"object"==typeof o.schema?(n=e.schema,i=e.name,r=e.method,a=e.includeRaw):(n=e,i=t?.name,r=t?.method,a=t?.includeRaw),"jsonMode"===r)s=this.bind({response_format:{type:"json_object"}}),d=_(n)?l._6.fromZodSchema(n):new l.hU;else{let e=i??"extract";if(_(n)){const t=(0,m.Ik)(n);s=this.bind({tools:[{type:"function",function:{name:e,description:t.description,parameters:t}}],tool_choice:"any"}),d=new c.dv({returnSingle:!0,keyName:e,zodSchema:n})}else{let t;"string"==typeof n.name&&"object"==typeof n.parameters&&null!=n.parameters?(t=n,e=n.name):t={name:e,description:n.description??"",parameters:n},s=this.bind({tools:[{type:"function",function:t}],tool_choice:"any"}),d=new c.dv({returnSingle:!0,keyName:e})}}if(!a)return s.pipe(d);const h=u.kI.assign({parsed:(e,t)=>d.invoke(e.raw,t)}),p=u.kI.assign({parsed:()=>null}),f=h.withFallbacks({fallbacks:[p]});return u.zZ.from([{raw:s},f])}async imports(){const{default:e}=await n.e(436).then(n.bind(n,67436));return{MistralClient:e}}}function _(e){return"function"==typeof e?.parse}var w=n(33030),v=n(70618);class P extends w.J{constructor(e){super(e??{}),Object.defineProperty(this,"modelName",{enumerable:!0,configurable:!0,writable:!0,value:"mistral-embed"}),Object.defineProperty(this,"model",{enumerable:!0,configurable:!0,writable:!0,value:"mistral-embed"}),Object.defineProperty(this,"encodingFormat",{enumerable:!0,configurable:!0,writable:!0,value:"float"}),Object.defineProperty(this,"batchSize",{enumerable:!0,configurable:!0,writable:!0,value:512}),Object.defineProperty(this,"stripNewLines",{enumerable:!0,configurable:!0,writable:!0,value:!0}),Object.defineProperty(this,"apiKey",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"endpoint",{enumerable:!0,configurable:!0,writable:!0,value:void 0});const t=e?.apiKey??(0,s.Az)("MISTRAL_API_KEY");if(!t)throw new Error("API key missing for MistralAI, but it is required.");this.apiKey=t,this.endpoint=e?.endpoint,this.modelName=e?.model??e?.modelName??this.model,this.model=this.modelName,this.encodingFormat=e?.encodingFormat??this.encodingFormat,this.batchSize=e?.batchSize??this.batchSize,this.stripNewLines=e?.stripNewLines??this.stripNewLines}async embedDocuments(e){const t=(0,v.l)(this.stripNewLines?e.map((e=>e.replace(/\n/g," "))):e,this.batchSize),n=t.map((e=>this.embeddingWithRetry(e))),i=await Promise.all(n),r=[];for(let e=0;e<i.length;e+=1){const n=t[e],{data:a}=i[e];for(let e=0;e<n.length;e+=1)r.push(a[e].embedding)}return r}async embedQuery(e){const{data:t}=await this.embeddingWithRetry(this.stripNewLines?e.replace(/\n/g," "):e);return t[0].embedding}async embeddingWithRetry(e){const{MistralClient:t}=await this.imports(),n=new t(this.apiKey,this.endpoint);return this.caller.call((async()=>await n.embeddings({model:this.model,input:e})))}async imports(){const{default:e}=await n.e(436).then(n.bind(n,67436));return{MistralClient:e}}}var k=n(59470),O=n(71563);class j extends k.I{static lc_name(){return"MistralAI"}constructor(e){super(e??{}),Object.defineProperty(this,"lc_namespace",{enumerable:!0,configurable:!0,writable:!0,value:["langchain","llms","mistralai"]}),Object.defineProperty(this,"lc_serializable",{enumerable:!0,configurable:!0,writable:!0,value:!0}),Object.defineProperty(this,"model",{enumerable:!0,configurable:!0,writable:!0,value:"codestral-latest"}),Object.defineProperty(this,"temperature",{enumerable:!0,configurable:!0,writable:!0,value:0}),Object.defineProperty(this,"topP",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"maxTokens",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"randomSeed",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"streaming",{enumerable:!0,configurable:!0,writable:!0,value:!1}),Object.defineProperty(this,"batchSize",{enumerable:!0,configurable:!0,writable:!0,value:20}),Object.defineProperty(this,"apiKey",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"endpoint",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"maxRetries",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"maxConcurrency",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),this.model=e?.model??this.model,this.temperature=e?.temperature??this.temperature,this.topP=e?.topP??this.topP,this.maxTokens=e?.maxTokens??this.maxTokens,this.randomSeed=e?.randomSeed??this.randomSeed,this.batchSize=e?.batchSize??this.batchSize,this.streaming=e?.streaming??this.streaming,this.endpoint=e?.endpoint,this.maxRetries=e?.maxRetries,this.maxConcurrency=e?.maxConcurrency;const t=e?.apiKey??(0,s.Az)("MISTRAL_API_KEY");if(!t)throw new Error('MistralAI requires an API key to be set.\nEither provide one via the "apiKey" field in the constructor, or set the "MISTRAL_API_KEY" environment variable.');this.apiKey=t}get lc_secrets(){return{apiKey:"MISTRAL_API_KEY"}}get lc_aliases(){return{apiKey:"mistral_api_key"}}_llmType(){return"mistralai"}invocationParams(e){return{model:this.model,suffix:e.suffix,temperature:this.temperature,maxTokens:this.maxTokens,topP:this.topP,randomSeed:this.randomSeed,stop:e.stop}}async _call(e,t){const n={...this.invocationParams(t),prompt:e};return(await this.completionWithRetry(n,t,!1)).choices[0].message.content??""}async _generate(e,t,n){const i=(0,v.l)(e,this.batchSize),r=[],a=this.invocationParams(t);for(let e=0;e<i.length;e+=1){const o=await(async()=>{if(this.streaming){const r=[];for(let o=0;o<i[e].length;o+=1){const s=[];let l;const c=await this.completionWithRetry({...a,prompt:i[e][o]},t,!0);for await(const e of c){l||(l={id:e.id,object:"chat.completion",created:e.created,model:e.model});for(const t of e.choices){if(s[t.index]){const e=s[t.index];e.message.content+=t.delta.content??"",e.finish_reason=t.finish_reason}else s[t.index]={index:t.index,message:{role:t.delta.role??"assistant",content:t.delta.content??"",tool_calls:null},finish_reason:t.finish_reason};n?.handleLLMNewToken(t.delta.content??"",{prompt:t.index,completion:t.index})}}if(t.signal?.aborted)throw new Error("AbortError");r.push({...l,choices:s})}return r}{const n=[];for(let r=0;r<i[e].length;r+=1){const o=await this.completionWithRetry({...a,prompt:i[e][r]},t,!1);n.push(o)}return n}})();r.push(...o.map((e=>e.choices)))}return{generations:r.map((e=>e.map((e=>({text:e.message.content??"",generationInfo:{finishReason:e.finish_reason}})))))}}async completionWithRetry(e,t,n){const{MistralClient:i}=await this.imports(),r=new O.g({maxConcurrency:t.maxConcurrency||this.maxConcurrency,maxRetries:this.maxRetries}),a=new i(this.apiKey,this.endpoint,this.maxRetries,t.timeout);return r.callWithOptions({signal:t.signal},(async()=>n?a.completionStream(e):a.completion(e)))}async*_streamResponseChunks(e,t,n){const i={...this.invocationParams(t),prompt:e},r=await this.completionWithRetry(i,t,!0);for await(const e of r){const t=e?.choices[0];if(!t)continue;const i=new o.mu({text:t.delta.content??"",generationInfo:{finishReason:t.finish_reason,tokenUsage:e.usage}});yield i,n?.handleLLMNewToken(i.text??"")}if(t.signal?.aborted)throw new Error("AbortError")}async imports(){const{default:e}=await n.e(436).then(n.bind(n,67436));return{MistralClient:e}}}}}]);