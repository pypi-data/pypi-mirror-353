# ğŸ™ï¸ TransMeet â€” AI-Powered Meeting Summarizer

> **Turn your meeting recordings into clear, structured minutes using LLMs like Groq Whisper and Google Speech Recognition.**

---

## ğŸš€ Features

* âœ… **Audio Transcription** â€” Automatically convert `.wav` or `.mp3` files into text
* ğŸ§  **LLM-Powered Summarization** â€” Generate concise and structured meeting minutes
* ğŸ” **Groq & Google Support** â€” Choose between Groq Whisper models or Google Speech API
* ğŸª“ **Automatic Chunking** â€” Splits large files intelligently for smoother transcription
* âš™ï¸ **Fully Customizable** â€” Pick your preferred transcription and summarization models
* ğŸ§¾ **CLI & Python API** â€” Use it from the terminal or integrate in your Python workflows
* ğŸ“ **Clean Output** â€” Saves transcripts and summaries neatly in your desired folder

---

## ğŸ“¦ Installation

```bash
pip install transmeet
```

## Dependencies

```bash
sudo apt-get update && sudo apt-get install -y ffmpeg gcc && sudo apt-get clean && sudo rm -rf /var/lib/apt/lists/*
```

## ğŸ” Setup

Set your **GROQ API Key**/**OPENAI API Key** in your environment variables.

```bash
export GROQ_API_KEY=your_groq_api_key
```

To make this permanent:

```bash
echo 'export GROQ_API_KEY=your_groq_api_key' >> ~/.bashrc
```

> If using OPENAI, set the `OPENAI_API_KEY` similarly.
> For Google Speech, no API key is needed; it uses the default model.

---

## ğŸ§‘â€ğŸ’» How to Use

### âœ… Option 1: Import as a Python Module

```python
from transmeet import generate_meeting_transcript_and_minutes

generate_meeting_transcript_and_minutes(
    meeting_audio_file="/path/to/audio.wav",
    output_dir="complete_path_to_output_dir/",
    transcription_client="groq",  # or "openai"
    transcription_model="whisper-large-v3-turbo", # change as per your need
    llm_client="groq",  # or "openai"
    llm_model="llama-3.3-70b-versatile", # change as per your need
)
```

This will save two files in your output directory:

* `transcription_<timestamp>.txt`
* `meeting_minutes_<timestamp>.md`

---

### ğŸ”§ Option 2: Use the CLI

#### ğŸ”¹ Basic Usage (Default: GROQ)

```bash
transmeet -i /path/to/audio.wav -o output/
```

#### ğŸ”¸ Advanced Usage

```bash
transmeet \
  -i /path/to/audio.wav \
  -o output/ \
  --transcription-client groq \
  --transcription-model whisper-large-v3-turbo \
  --llm-client groq \
  --llm-model llama-3.3-70b-versatile \
```

---

## ğŸ—‚ï¸ Output Structure

```
output/
â”œâ”€â”€ transcriptions/
â”‚   â””â”€â”€ transcription_20250510_213038.txt
â”œâ”€â”€ meeting_minutes/
â”‚   â””â”€â”€ meeting_minutes_20250510_213041.md
```

---

## ğŸ§ª Supported Formats

* `.wav`
* `.mp3`

---

## âš™ï¸ CLI Options

| Argument                 | Description                                   |
| ------------------------ | --------------------------------------------- |
| `-i`, `--audio-path`     | Path to the input audio file                  |
| `-o`, `--output-dir`     | Output directory (default: `output/`)         |
| `--transcription-client` | `groq` or `google` (default: `groq`)          |
| `--transcription-model`  | e.g., `whisper-large-v3-turbo`                |
| `--llm-client`           | `groq` or `openai` (default: `groq`)          |
| `--llm-model`            | e.g., `llama-3.3-70b-versatile`               |

---

## ğŸ¤– LLM Models

* **Groq Whisper**: `whisper-large`, `whisper-large-v3-turbo`, etc.
* **Google Speech**: Model defaults to their API standard
* **LLMs for minutes**: `llama-3`, `mixtral`, `gpt-4`, etc. (Groq/OpenAI)

---

## ğŸ“‹ Roadmap

* [ ] Add support for multi-language meetings
* [ ] Speaker diarization support
* [ ] Upload directly to Notion or Google Docs
* [ ] Slack/Discord bots

---

## ğŸ§‘â€ğŸ“ Author

**Deepak Raj**
ğŸ‘¨â€ğŸ’» [GitHub](https://github.com/coderperfectplus) â€¢ ğŸŒ [LinkedIN](https://www.linkedin.com/in/deepak-raj-35887386/s)

---

## ğŸ¤ Contributing

Pull requests are welcome! Found a bug or need a feature? Open an issue or submit a PR.

---

## âš–ï¸ License

[MIT License](LICENSE)
