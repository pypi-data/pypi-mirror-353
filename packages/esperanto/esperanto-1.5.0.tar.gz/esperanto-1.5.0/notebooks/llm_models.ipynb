{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esperanto import OpenAILanguageModel, XAILanguageModel, OpenRouterLanguageModel, AnthropicLanguageModel, OllamaLanguageModel, GoogleLanguageModel\n",
    "\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What's the capital of France?\"},\n",
    "    ]\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"max_tokens\": 850,\n",
    "    \"temperature\": 1.0,\n",
    "    \"streaming\": False,\n",
    "    \"top_p\": 0.9,\n",
    "    \"structured\": None\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"openrouter\": {\"class\": OpenRouterLanguageModel, \"model\": \"openai/o1-mini-2024-09-12\"},\n",
    "    \"openai\": {\"class\": OpenAILanguageModel, \"model\": \"gpt-4o\"},\n",
    "    \"xai\": {\"class\": XAILanguageModel, \"model\": \"grok-beta\"},\n",
    "    \"anthropic\": {\"class\": AnthropicLanguageModel, \"model\": \"claude-3-5-sonnet-latest\"},\n",
    "    \"ollama\": {\"class\": OllamaLanguageModel, \"model\": \"qwen2.5:32b\"},\n",
    "    \"google\": {\"class\": GoogleLanguageModel, \"model\": \"gemini-2.0-flash-exp\"},\n",
    "\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, config in models.items():\n",
    "    try:\n",
    "        # Create an instance of the provider class\n",
    "        provider = config[\"class\"]()\n",
    "        print(f\"\\n=== {name.upper()} Models ===\")\n",
    "        print(provider.models)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get models for {name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, config in models.items():\n",
    "    try:\n",
    "        llm = config[\"class\"](model_name=config[\"model\"])\n",
    "        print(f\"Results for {llm.provider}:\")\n",
    "        result = llm.chat_complete(messages)\n",
    "        print(result.choices[0].message.content)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get models for {name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, config in models.items():\n",
    "    try:\n",
    "        llm = config[\"class\"](model_name=config[\"model\"])\n",
    "        print(f\"Results for {llm.provider}:\")\n",
    "        result = await llm.achat_complete(messages)\n",
    "        print(result.choices[0].message.content)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get models for {name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for openrouter:\n",
      "Error decoding JSON\n",
      "```json\n",
      "{\n",
      "  \"top_cities\": [\n",
      "    {\n",
      "      \"name\": \"São Paulo\",\n",
      "      \"state\": \"São Paulo\",\n",
      "      \"population\": 12325232\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Rio de Janeiro\",\n",
      "      \"state\": \"Rio de Janeiro\",\n",
      "      \"population\": 6747815\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Brasília\",\n",
      "      \"state\": \"Distrito Federal\",\n",
      "      \"population\": 3055149\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "*Please note that population figures are approximate and may have changed since the last update in 2023.*\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for openai:\n",
      "{'cities': [{'name': 'São Paulo', 'state': 'São Paulo', 'population': 12325232}, {'name': 'Rio de Janeiro', 'state': 'Rio de Janeiro', 'population': 6747815}, {'name': 'Brasília', 'state': 'Distrito Federal', 'population': 3055149}]}\n",
      "\n",
      "==================================================\n",
      "\n",
      "2025-01-10 13:57:37 | esperanto | WARNING | Structured output not supported for X.AI.\n",
      "Results for xai:\n",
      "Error decoding JSON\n",
      "Here is a JSON representation of the top 3 most populous cities in Brazil:\n",
      "\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"name\": \"São Paulo\",\n",
      "        \"state\": \"São Paulo\",\n",
      "        \"population\": \"11,253,503\",\n",
      "        \"rank\": 1\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Rio de Janeiro\",\n",
      "        \"state\": \"Rio de Janeiro\",\n",
      "        \"population\": \"6,320,446\",\n",
      "        \"rank\": 2\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Brasília\",\n",
      "        \"state\": \"Federal District\",\n",
      "        \"population\": \"2,817,068\",\n",
      "        \"rank\": 3\n",
      "    }\n",
      "]\n",
      "```\n",
      "\n",
      "Please note that the population figures are estimates and might change over time. The data here represents one of the more recent estimates available.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for anthropic:\n",
      "2025-01-10 13:57:41 | esperanto | WARNING | Structured output not supported for Anthropic.\n",
      "{'top_3_brazilian_cities': [{'name': 'São Paulo', 'population': 12.33, 'state': 'SP'}, {'name': 'Rio de Janeiro', 'population': 6.75, 'state': 'RJ'}, {'name': 'Brasília', 'population': 3.09, 'state': 'DF'}]}\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for ollama:\n",
      "{'format': 'json', 'options': {'temperature': 1.0, 'top_p': 0.9, 'num_predict': 850}}\n",
      "Error decoding JSON\n",
      "Sure! Here is the information about the top three Brazilian cities in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"cities\": [\n",
      "    {\n",
      "      \"name\": \"São Paulo\",\n",
      "      \"state\": \"São Paulo\",\n",
      "      \"population\": 12_345_068,\n",
      "      \"area_km2\": 1521.11\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Rio de Janeiro\",\n",
      "      \"state\": \"Rio de Janeiro\",\n",
      "      \"population\": 6_719_751,\n",
      "      \"area_km2\": 1255.80\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Belo Horizonte\",\n",
      "      \"state\": \"Minas Gerais\",\n",
      "      \"population\": 2_543_014,\n",
      "      \"area_km2\": 330.73\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "This JSON includes the name, state, population (approximate), and area (in square kilometers) for each city.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Results for google:\n",
      "[{'rank': '1', 'city': 'Sao Paulo'}, {'rank': '2', 'city': 'Rio de Janeiro'}, {'rank': '3', 'city': 'Brasilia'}]\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Please return the top 3 brazilian cities in JSON format\"},\n",
    "    ]\n",
    "\n",
    "\n",
    "for name, config in models.items():\n",
    "    try:\n",
    "        llm = config[\"class\"](model_name=config[\"model\"], structured={\"type\": \"json\"})\n",
    "        print(f\"Results for {llm.provider}:\")\n",
    "        result = llm.chat_complete(json_messages)\n",
    "        try:\n",
    "            json_data = json.loads(result.choices[0].message.content)\n",
    "            print(json_data)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error decoding JSON\")\n",
    "            print(result.choices[0].message.content)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get models for {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pydantic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from pydantic import BaseModel\n",
    "# from typing import List\n",
    "\n",
    "# class Country(BaseModel):\n",
    "#     name: str\n",
    "#     population: int\n",
    "\n",
    "# class Response(BaseModel):\n",
    "#     countries: List[Country]\n",
    "\n",
    "# json_messages = [\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#         {\"role\": \"user\", \"content\": \"Please return the top 3 countries in terms of population. Responda no formato JSON.\"},\n",
    "#     ]\n",
    "\n",
    "\n",
    "# for name, config in models.items():\n",
    "#     try:\n",
    "#         llm = config[\"class\"](model_name=config[\"model\"], structured={\"type\": \"json\", \"model\": Response})\n",
    "#         print(f\"Results for {llm.provider}:\")\n",
    "#         result = llm.chat_complete(json_messages)\n",
    "#         try:\n",
    "#             json_data = json.loads(result.choices[0].message.content)\n",
    "#             print(json_data)\n",
    "#         except json.JSONDecodeError:\n",
    "#             print(\"Error decoding JSON\")\n",
    "        \n",
    "#         print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to get models for {name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for name, config in models.items():\n",
    "    try:\n",
    "        llm = config[\"class\"](model_name=config[\"model\"])\n",
    "        print(f\"Results for {llm.provider}:\")\n",
    "        result = llm.chat_complete(\n",
    "            messages, stream=True\n",
    "        )\n",
    "\n",
    "        for chunk in result:\n",
    "            print(chunk)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "            print(f\"Failed to process for {name}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for name, config in models.items():\n",
    "    try:\n",
    "        llm = config[\"class\"](model_name=config[\"model\"])\n",
    "        print(f\"Results for {llm.provider}:\")\n",
    "        result = await llm.achat_complete(\n",
    "            messages, stream=True\n",
    "        )\n",
    "\n",
    "        async for chunk in result:\n",
    "            print(chunk)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "                print(f\"Failed to process for {name}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, config in models.items():\n",
    "    try:\n",
    "        llm = config[\"class\"](model_name=config[\"model\"])\n",
    "        print(f\"Results for {llm.provider}:\")\n",
    "        model = llm.to_langchain()\n",
    "        response = model.invoke(messages)\n",
    "        print(response.content)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process for {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, config in models.items():\n",
    "    try:\n",
    "        llm = config[\"class\"](model_name=config[\"model\"])\n",
    "        print(f\"Results for {llm.provider}:\")\n",
    "        model = llm.to_langchain()\n",
    "        response = await model.ainvoke(messages)\n",
    "        print(response.content)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process for {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, config in models.items():\n",
    "    try:\n",
    "        llm = config[\"class\"](model_name=config[\"model\"], streaming=True)\n",
    "        print(f\"Results for {llm.provider}:\")\n",
    "        model = llm.to_langchain()\n",
    "        response = model.stream(messages)\n",
    "        for chunk in response:\n",
    "            print(chunk)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process for {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, config in models.items():\n",
    "    try:\n",
    "        llm = config[\"class\"](model_name=config[\"model\"], streaming=True)\n",
    "        print(f\"Results for {llm.provider}:\")\n",
    "        model = llm.to_langchain()\n",
    "        response = model.astream(messages)\n",
    "        async for chunk in response:\n",
    "            print(chunk)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process for {name}: {e}\")\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
