Metadata-Version: 2.4
Name: prefix_grouper
Version: 0.0.1rc1
Summary: An efficient GRPO training util.
Author-email: Zikang Liu <liuzikang0625@gmail.com>
License: MIT
Project-URL: Homepage, https://github.com/johncaged/PrefixGrouper
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Dynamic: license-file

<h3 align="center">
    <img src="https://raw.githubusercontent.com/johncaged/PrefixGrouper/main/assets/images/logo.png" width="352" style="max-width: 100%;">
</h3>

<h4 align="center">
    <p>
        <b>English</b> |
        <a href="https://github.com/johncaged/PrefixGrouper/blob/main/i18n/README_zh-hans.md">简体中文</a>
    </p>
</h4>

<h3 align="center">
    <p>Efficient GRPO training through shared-prefix forward</p>
</h3>

``PrefixGrouper`` is a plug-and-play efficient GRPO training tool that requires minimal modifications to existing codebases to achieve reduced computation, lower VRAM consumption, and accelerated training. Additionally, this tool can be applied to other scenarios requiring shared-prefix training/inference beyond GRPO.

In current mainstream GRPO training pipelines, policy model training primarily involves copying prefixes (typically questions, multimodal inputs, etc.) `G` times. Consequently, when training data prefixes are sufficiently long (e.g., long-context reasoning, image/long-video inference), redundant computation during training becomes non-negligible, leading to increased VRAM usage, higher computation costs, and slower training speeds. To address this, we propose ``PrefixGrouper``, a plug-and-play GRPO training tool that enables efficient training through shared-prefix forward passes. Reduced VRAM consumption conversely allows more GPUs to support larger group sizes—critical for GRPO algorithms.

## News

**[2025/6/3]** We release ``PrefixGrouper``. Tech report is coming, please stay tuned.

## Method Overview

The core of ``PrefixGrouper`` lies in its attention operation design:

<h3 align="center">
    <img src="https://raw.githubusercontent.com/johncaged/PrefixGrouper/main/assets/images/method.jpg">
</h3>

By decomposing the original redundant self-attention operation into prefix self-attention + suffix concat-attention, ``PrefixGrouper`` enables efficient GRPO training and is theoretically compatible with various attention implementations (``EagerAttention``, ``FlashAttention``, ``SDPA``, etc.) as well as hardware devices (GPU, NPU, etc.).

Comparison of FLOPs and memory usage between ``PrefixGrouper`` and baseline is as follows, which display results at fixed prefix lengths (4096, 8192, and 16384) across different ratios (prefix length / suffix length):

<h3 align="center">
    <img src="https://raw.githubusercontent.com/johncaged/PrefixGrouper/main/assets/images/flops.png" width="90%" style="max-width: 90%">
</h3>

<h3 align="center">
    <img src="https://raw.githubusercontent.com/johncaged/PrefixGrouper/main/assets/images/mem.png" width="90%" style="max-width: 90%">
</h3>

``PrefixGrouper`` demonstrates significant advantages in long-context scenarios, further highlighting its efficiency.

## Installation

```py
pip install prefix_grouper
```

## Quick Start

To make ``PrefixGrouper`` simpler and easier to use, we provide modification examples for some models.

- Model file modification examples can be found in ``examples``. For clarity, we wrap key modifications with "PrefixGrouper Start" and "PrefixGrouper End" comments.
- For examples simulating the full training workflow, see ``tests/equivalence``. We provide an almost complete flow for one training step.

If you happen to use one of these models, you can directly integrate the example code into your codebase. However, we recommend briefly reviewing the tutorial below to better understand the tool's workflow.

## Tutorial

Briefly, ``PrefixGrouper`` requires modifications in three areas: data input/output, attention mechanisms, and position encoding. Throughout this document, we refer to data corresponding to a query (prefix) as a **sample**, and each model-generated output based on the prefix as a **response**.

### Data Input/Output

To minimize redundant prefix forward passes and maximize parallel acceleration, ``PrefixGrouper`` first concatenates each sample in the batch with its corresponding responses (pseudocode example):

```py
# Assume we have prefix input_ids (padded torch.Tensor, shape [b, seq_len1]) and corresponding mask
prompt_ids = ...
prompt_mask = ...

# Assume model-generated responses are obtained (method irrelevant; output as str or input_ids)
# responses: List[List[str]] - outer list: batch_size (b), inner list: number of responses per sample.
responses: List[List[str]] = ...
# Flatten responses, tokenize, and pad into suffix_ids (shape [b * group_size, seq_len2])
suffix_ids = ...
suffix_mask = ...

# ====== Input processing complete ======

# Create PrefixGrouper instance
# group_info: List[List[int]] - outer list: sample count (b), inner list: [prefix_len, suffix1_len, suffix2_len,...]
group_info: List[List[int]] = ...
prefix_grouper = PrefixGrouper(
    group_info=group_info,
    padding_mode="right",
    device=device,
)
# Concatenate inputs into optimized input_ids (shape [b, seq_len])
input_ids = prefix_grouper.concat_input(prompt_ids, prompt_mask, suffix_ids, suffix_mask)
# Model forward with prefix_grouper
res = model(*args, **kwargs, prefix_grouper=prefix_grouper)
# ====== Forward pass complete ======

# Loss calculation and backward pass
# Split outputs (handles autoregressive boundary tokens)
prefix_output, prefix_mask, suffix_output, suffix_mask = (
    prefix_grouper.split_output(res.logits, include_prefix_last=1)
)
# ====== Output processing complete ======

# Standard GRPO loss calculation and backpropagation
suffix_output = suffix_output[:, :-1]
suffix_mask = suffix_mask[:, 1:]
loss = (suffix_output.gather(-1, suffix_ids.unsqueeze(-1)).squeeze(-1) - suffix_output.logsumexp(-1)).exp()
loss = loss * suffix_mask
loss = (loss.sum(-1) / suffix_mask.sum(-1)).mean()
(-loss).backward()
```

Key points for data processing: input concatenation, `group_info` statistics, and output splitting. Customize based on your project needs while maintaining interface consistency (see docs).

### Attention Mechanism

Minor model modifications suffice for attention adaptation. For transformers supporting ``AttentionInterface``, simple registration is possible (experimental). Below describes the generic approach:

```py
if prefix_grouper is None:
    # Original attention (baseline)
    attn_output = _flash_attention_forward(...)
else:
    # ===== PrefixGrouper Start =====
    def attn_func(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, attn_mask: torch.Tensor, *args, **kwargs):
        # Adapter function for dimension/parameter alignment
        return _flash_attention_forward(...)
    
    attn_output = prefix_grouper.forward(...)
    # ====== PrefixGrouper End ======
```

Propagate `prefix_grouper` parameter through model forward passes.

### Position Encoding

Position IDs for concatenated responses should share the same starting ID pattern: ``[0, 1, ..., prefix_len, prefix_len+1, ..., suffix1_end, prefix_len+1, ..., suffix2_end, ...]``.

Position encoding is pre-adapted for models in the ``Quick Start`` section (see examples).

### Start Training

Complete GRPO training simulations are provided in ``tests`` for reference.

## Documentation

Core API documentation:

### PrefixGrouper

#### PrefixGrouper(Optional[List[List[int]]] = None, device=None, padding_mode: Union[str, torch.Tensor] = "right")

`group_info`: Outer list: sample count (b). Inner lists: ``[prefix_len, suffix1_len, suffix2_len,...]``. This parameter can be ``None``, in which case you need to manually call ``init`` (same signature as ``PrefixGrouper.__init__``) to implement delay initialization.

`device`: Device for initializing PrefixGrouper (actual ops use input tensor's device).

`padding_mode`: `"left"`/`"right"` (dense padding) or `torch.Tensor` (custom padding mask, shape ``[b, seq_len]``).

Usage examples:
- With `concat_input` (recommended):
```py
prefix_grouper = PrefixGrouper(group_info, padding_mode="right")
```
- Custom input handling:
```py
prefix_grouper = PrefixGrouper(group_info, padding_mode=custom_padding_mask)
```

#### PrefixGrouper.concat_input(self, prefix: torch.Tensor, prefix_mask: torch.Tensor, suffix: torch.Tensor, suffix_mask: torch.Tensor)

Concatenates `prefix` (``[b, seq_len]`` or ``[b, seq_len, dim]``) and `suffix` (``[b * group_size, seq_len]`` or ``[b * group_size, seq_len, dim]``) using `group_info`. Requires `prefix_mask`/`suffix_mask` (shape ``[b, seq_len]``).

#### PrefixGrouper.forward(self, __attn_func: AttnFuncType, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, *args, **kwargs)

Performs attention using `__attn_func`. Function signature: `attn_func(q, k, v, attn_mask, *args, **kwargs)`. Input `q/k/v` shape: ``[b, num_heads, seq_len, head_dim]``. Output shape: ``[b, seq_len, num_heads, head_dim]``. **Do not manually pass attention masks.**

#### PrefixGrouper.split_output(self, output: torch.Tensor, include_prefix_last: int = 0)

`output`: Shape ``[b, seq_len, dim]``

`include_prefix_last`: Controls prefix boundary handling (0: no conversion; 1: attach last prefix token to suffixes).

## Future Plans

- [ ] Hugging Face Transformers ``AttentionInterface`` Integration (This feature is currently in testing)
- [ ] Additional Training Device Support (``NPU`` under testing - no compatibility issues found so far)
- [ ] Test Cases for More Models (We plan to release plain-text test cases for ``Qwen2.5`` and ``Qwen3`` models)
- [ ] Support for other attention implementations (``EagerAttention``, ``SDPA``)

## Data Usage Statement

Test data in this project is strictly for **academic research purposes** with the following limitations:

1. **Commercial use is prohibited**  
2. **Data redistribution is prohibited**  
3. **De-anonymization attempts are prohibited**  

## Citation

[TODO]
