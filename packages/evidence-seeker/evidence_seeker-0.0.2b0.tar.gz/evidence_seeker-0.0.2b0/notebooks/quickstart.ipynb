{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to the EvidenceSeeker Pipeline\n",
    "\n",
    "This notebook serves as an introductory example for configuring and running the pipeline using [dedicated endpoints on HuggingFace](https://huggingface.co/docs/inference-endpoints/index)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0. For debugging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "# %pip pip install arize-phoenix\n",
    "# %pip install llama-index-callbacks-arize-phoenix\n",
    "# observability\n",
    "import llama_index.core  # type: ignore\n",
    "import phoenix as px  # type: ignore\n",
    "\n",
    "px.launch_app()\n",
    "\n",
    "llama_index.core.set_global_handler(\"arize_phoenix\", endpoint=\"http://localhost:6006/v1/traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Ensuring access to the models\n",
    "We use HuggingFace's [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) for model access. \n",
    "We use the following models:\n",
    "- [Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct)\n",
    "- [Paraphrase-multilingual-mpnet-base-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2)\n",
    "Accessing the models used by the pipeline requires creating a dedicated endpoint and an access token.\n",
    "\n",
    "For using your personal token, set up a file named `.env` with the following content: \n",
    "> `HF_TOKEN_EVIDENCE_SEEKER = <Your HuggingFace Token for Endpoint API Access>`.\n",
    "\n",
    "Change the value of the variable `ENV_PATH` to the path of your `.env` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_PATH = \"../.env\" #Change this value to the path to your .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And set the following variables to the url of your endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change this value to your endpoint url for Llama-3.1-70B-Instruct:\n",
    "LLAMA_ENDPOINT_URL = \"https://ev6086dt6s7nn1b5.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "LLAMA_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "#Change this value to your endpoint url for Paraphrase-multilingual-mpnet-base-v2:\n",
    "EMBED_ENDPOINT_URL = \"https://ibpp4xgm0kspxkjb.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "EMBED_MODEL = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "\n",
    "# HF Inference API, does not work:\n",
    "# LLAMA_BASE_URL = \"https://router.huggingface.co/hf-inference/v1/\"\n",
    "# EMBED_BASE_URL = \"https://router.huggingface.co/hf-inference/v1/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN_EVIDENCE_SEEKER environment variable found at ../.env!\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "import os\n",
    "\n",
    "if os.path.exists(ENV_PATH) and os.path.basename(ENV_PATH) == \".env\":\n",
    "    dotenv.load_dotenv(ENV_PATH)\n",
    "    if \"HF_TOKEN_EVIDENCE_SEEKER\" in os.environ:\n",
    "        print(f\"HF_TOKEN_EVIDENCE_SEEKER environment variable found at {ENV_PATH}!\")\n",
    "    else: \n",
    "        raise Exception(f\"Please set the HF_TOKEN_EVIDENCE_SEEKER environment variable in .env file at {ENV_PATH}\")\n",
    "else:\n",
    "    raise Exception(f\"Please set 'ENV_PATH' to a valid path to an '.env' file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Ensuring access to the database\n",
    "\n",
    "For simplicity, we will use the already built embedding included in this repository. It is built using articles from the journal \"Aus Politik und Zeitgeschichte\" (APuZ) provided by the Federal Agency for Civic Education (Bundeszentrale f√ºr politische Bildung/bpb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_PATH = \"../TMP/APUZ/storage/index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidence_seeker.retrieval import RetrievalConfig\n",
    "from evidence_seeker.preprocessing import ClaimPreprocessingConfig\n",
    "from evidence_seeker.confirmation_analysis import ConfirmationAnalyzerConfig\n",
    "\n",
    "retrieval_config = RetrievalConfig(\n",
    "    index_persist_path=EMBEDDING_PATH,\n",
    "    embed_base_url=EMBED_ENDPOINT_URL,\n",
    "    embed_model_name=EMBED_MODEL\n",
    ")\n",
    "\n",
    "model = {\"model_5\" : {\"name\": \"Llama-3.1-70B-Instruct\",\n",
    "                \"description\": \"HF dedicated endpoint (debatelab)\",\n",
    "                \"base_url\": LLAMA_ENDPOINT_URL,\n",
    "                \"model\": LLAMA_MODEL,\n",
    "                \"api_key_name\": \"HF_TOKEN_EVIDENCE_SEEKER\",\n",
    "                \"backend_type\": \"tgi\",\n",
    "                \"max_tokens\": 2048,\n",
    "                \"temperature\": 0.2,\n",
    "            }\n",
    "        }\n",
    "\n",
    "preprocessing_config = ClaimPreprocessingConfig(used_model_key=\"model_5\", models=model)\n",
    "\n",
    "confirmation_analysis_config = ConfirmationAnalyzerConfig(used_model_key=\"model_5\", models=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Running the pipeline on a set of example inputs\n",
    "We'll use the following example inputs for testing the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_inputs = [\n",
    "    \"Die Anzahl hybrider Kriege hat zugenommen.\",\n",
    "    \"Die Osterweiterung hat die EU-Institutionen nachhaltig geschw√§cht.\",\n",
    "    \"In der Bundeswehr gibt es keinen politischen Extremismus.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following lines to instantiate an EvidenceSeeker. Note that it may take a few minutes to load the pre-built index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-10 15:33:56.019\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.backend\u001b[0m:\u001b[36mget_openai_llm\u001b[0m:\u001b[36m109\u001b[0m - \u001b[34m\u001b[1mFetching api key via env var: HF_TOKEN_EVIDENCE_SEEKER\u001b[0m\n",
      "\u001b[32m2025-03-10 15:33:56.025\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.backend\u001b[0m:\u001b[36mget_openai_llm\u001b[0m:\u001b[36m117\u001b[0m - \u001b[34m\u001b[1mInstantiating OpenAILike model (model: meta-llama/Llama-3.1-70B-Instruct,base_url: https://ev6086dt6s7nn1b5.us-east-1.aws.endpoints.huggingface.cloud).\u001b[0m\n",
      "\u001b[32m2025-03-10 15:33:56.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevidence_seeker.retrieval.base\u001b[0m:\u001b[36mload_index\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mLoading index from disk at ../TMP/APUZ/storage/index\u001b[0m\n",
      "\u001b[32m2025-03-10 15:35:56.336\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.backend\u001b[0m:\u001b[36mget_openai_llm\u001b[0m:\u001b[36m109\u001b[0m - \u001b[34m\u001b[1mFetching api key via env var: HF_TOKEN_EVIDENCE_SEEKER\u001b[0m\n",
      "\u001b[32m2025-03-10 15:35:56.340\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.backend\u001b[0m:\u001b[36mget_openai_llm\u001b[0m:\u001b[36m117\u001b[0m - \u001b[34m\u001b[1mInstantiating OpenAILike model (model: meta-llama/Llama-3.1-70B-Instruct,base_url: https://ev6086dt6s7nn1b5.us-east-1.aws.endpoints.huggingface.cloud).\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from evidence_seeker import EvidenceSeeker\n",
    "\n",
    "evidence_seeker = EvidenceSeeker(retrieval_config=retrieval_config, preprocessing_config=preprocessing_config, confirmation_analysis_config=confirmation_analysis_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines will execute the pipeline on the given example inputs. Before running this cell, make sure the endpoint is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidence_seeker import describe_result\n",
    "\n",
    "results = []\n",
    "for input in example_inputs:\n",
    "    result = await evidence_seeker(input)\n",
    "    results.append(result)\n",
    "    md = describe_result(input, result)\n",
    "    display(Markdown(md))\n",
    "    display(Markdown(\"------\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For displaying all results at once\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "for (input, result) in zip(example_inputs, results):\n",
    "    md = describe_result(input, result)\n",
    "    display(Markdown(md))\n",
    "    display(Markdown(\"------\\n\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
