# Migration Guide: v0.1.0 to v0.1.1

## Overview

AgentiCraft v0.1.1 introduces several new features while maintaining backward compatibility. This guide helps you upgrade smoothly and take advantage of new capabilities.

## What's New in v0.1.1

### Major Features
- **Provider Switching**: Dynamically switch between OpenAI, Anthropic, and Ollama
- **New Providers**: Anthropic (Claude) and Ollama (local models) support
- **Advanced Agents**: ReasoningAgent and WorkflowAgent implementations
- **PyPI Package**: Install via `pip install agenticraft`

### Improvements
- Better error handling and provider fallback
- Enhanced documentation with more examples
- Performance optimizations
- Improved test coverage (>95%)

## Installation

### Upgrading from v0.1.0

```bash
# If installed from source
cd agenticraft
git pull origin main
pip install -e .

# Once published to PyPI (after June 7, 2025)
pip install --upgrade agenticraft
```

### New Installation

```bash
# From PyPI (after June 7, 2025)
pip install agenticraft

# From source
git clone https://github.com/agenticraft/agenticraft.git
cd agenticraft
pip install -e .
```

## Breaking Changes

**Good news!** There are no breaking changes in v0.1.1. All existing code will continue to work.

## New Features Usage

### 1. Provider Switching

**New in v0.1.1**: Switch providers at runtime

```python
# Existing code still works
agent = Agent(name="MyAgent", model="gpt-4")

# NEW: Switch providers dynamically
agent.set_provider("anthropic", model="claude-3-opus-20240229")
agent.set_provider("ollama", model="llama2")
```

### 2. Anthropic Provider

**New in v0.1.1**: Use Claude models

```python
# Before (v0.1.0) - Only OpenAI
agent = Agent(model="gpt-4")

# After (v0.1.1) - Multiple providers
agent = Agent(model="claude-3-opus-20240229")  # Auto-detects Anthropic
# or explicitly
agent = Agent(provider="anthropic", model="claude-3-opus-20240229")
```

Required setup:
```bash
export ANTHROPIC_API_KEY="your-anthropic-api-key"
```

### 3. Ollama Provider (Local Models)

**New in v0.1.1**: Run models locally

```python
# Use local models with Ollama
agent = Agent(
    model="llama2",  # Auto-detects Ollama
    base_url="http://localhost:11434"  # Optional, this is default
)

# Or use ollama/ prefix
agent = Agent(model="ollama/mistral")
```

Required setup:
```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Start Ollama
ollama serve

# Pull a model
ollama pull llama2
```

### 4. ReasoningAgent

**New in v0.1.1**: Agent with transparent reasoning

```python
from agenticraft.agents import ReasoningAgent

# Before (v0.1.0)
agent = Agent(name="Assistant")
response = agent.run("Complex question")
# Limited visibility into reasoning

# After (v0.1.1)
agent = ReasoningAgent(name="Assistant")
response = await agent.think_and_act("Complex question")

# Access detailed reasoning
print(f"Goal: {response.reasoning.goal_analysis}")
for step in response.reasoning.steps:
    print(f"Step {step.number}: {step.description}")
    print(f"Confidence: {step.confidence}")
```

### 5. WorkflowAgent

**New in v0.1.1**: Agent optimized for workflows

```python
from agenticraft.agents import WorkflowAgent
from agenticraft.workflows import Workflow, Step

# Create workflow
workflow = Workflow("data_pipeline")
workflow.add_step("fetch", fetch_data_agent)
workflow.add_step("process", process_agent, depends_on=["fetch"])
workflow.add_step("analyze", analyze_agent, depends_on=["process"])

# Before (v0.1.0) - Manual orchestration
results = []
results.append(await fetch_data_agent.run())
results.append(await process_agent.run(results[0]))
results.append(await analyze_agent.run(results[1]))

# After (v0.1.1) - WorkflowAgent
agent = WorkflowAgent(name="Pipeline")
agent.add_workflow(workflow)
result = await agent.run("Execute data pipeline")
```

## Configuration Changes

### Environment Variables

v0.1.1 recognizes additional environment variables:

```bash
# Existing (v0.1.0)
OPENAI_API_KEY=sk-...

# New in v0.1.1
ANTHROPIC_API_KEY=sk-ant-...
OLLAMA_BASE_URL=http://localhost:11434  # Optional

# Provider defaults (optional)
AGENTICRAFT_DEFAULT_PROVIDER=anthropic
AGENTICRAFT_DEFAULT_MODEL=claude-3-opus-20240229
```

### Settings Updates

```python
# agenticraft/core/config.py additions
class Settings:
    # Existing settings preserved
    
    # New in v0.1.1
    anthropic_api_key: Optional[str] = None
    ollama_base_url: str = "http://localhost:11434"
    default_provider: str = "openai"  # Can be "anthropic", "ollama"
```

## Common Migration Scenarios

### Scenario 1: Add Anthropic as Fallback

```python
# Before (v0.1.0)
try:
    response = agent.run(prompt)
except Exception as e:
    # Manual retry with same provider
    response = agent.run(prompt)

# After (v0.1.1)
try:
    response = agent.run(prompt)
except Exception as e:
    # Fallback to different provider
    agent.set_provider("anthropic")
    response = agent.run(prompt)
```

### Scenario 2: Use Local Model for Privacy

```python
# Before (v0.1.0) - Always uses cloud API
agent = Agent(name="PrivateAgent")
response = agent.run(sensitive_data)  # Sent to OpenAI

# After (v0.1.1) - Keep data local
agent = Agent(name="PrivateAgent", model="ollama/llama2")
response = agent.run(sensitive_data)  # Processed locally
```

### Scenario 3: Cost Optimization

```python
# Before (v0.1.0) - Fixed model
agent = Agent(model="gpt-4")  # Expensive for all tasks

# After (v0.1.1) - Dynamic model selection
class SmartAgent:
    def __init__(self):
        self.agent = Agent()
    
    def run(self, prompt):
        if self.is_simple(prompt):
            self.agent.set_provider("openai", model="gpt-3.5-turbo")
        else:
            self.agent.set_provider("openai", model="gpt-4")
        return self.agent.run(prompt)
```

## Testing Your Code

### Running Tests with Multiple Providers

```python
# test_my_agent.py
import pytest
from agenticraft import Agent

@pytest.mark.parametrize("provider,model", [
    ("openai", "gpt-3.5-turbo"),
    ("anthropic", "claude-3-sonnet-20240229"),
    ("ollama", "llama2"),
])
def test_agent_with_providers(provider, model):
    """Test agent works with different providers."""
    agent = Agent(name="TestAgent")
    
    try:
        agent.set_provider(provider, model=model)
        response = agent.run("Hello")
        assert response.content
    except Exception as e:
        pytest.skip(f"{provider} not available: {e}")
```

## Performance Considerations

### Provider Latency Comparison

| Provider | Model | Avg Latency | Cost/1K tokens |
|----------|-------|-------------|----------------|
| OpenAI | GPT-3.5 | 0.5-2s | $0.002 |
| OpenAI | GPT-4 | 2-10s | $0.03 |
| Anthropic | Claude-3-Opus | 2-5s | $0.015 |
| Anthropic | Claude-3-Sonnet | 1-3s | $0.003 |
| Ollama | Llama2 | 0.1-5s | Free |

### Memory Usage

- **v0.1.0**: ~100MB base
- **v0.1.1**: ~110MB base (+10MB for multi-provider support)
- **With Ollama**: Depends on model (1-8GB)

## Troubleshooting

### Issue: "Unknown provider" error
```python
# Error
ProviderError: Unknown provider: claude

# Fix - Use correct provider name
agent.set_provider("anthropic", model="claude-3-opus-20240229")
```

### Issue: Anthropic API key not found
```python
# Error
ProviderAuthError: Missing API key for anthropic

# Fix
export ANTHROPIC_API_KEY="your-key"
# or
agent = Agent(api_key="your-key", model="claude-3-opus-20240229")
```

### Issue: Ollama connection failed
```python
# Error
ProviderError: Cannot connect to Ollama at localhost:11434

# Fix
# 1. Start Ollama
ollama serve

# 2. Or specify custom URL
agent.set_provider("ollama", base_url="http://your-server:11434")
```

## Best Practices for v0.1.1

1. **Use Environment Variables** for API keys
   ```bash
   # .env file
   OPENAI_API_KEY=sk-...
   ANTHROPIC_API_KEY=sk-ant-...
   ```

2. **Implement Provider Fallback** for reliability
   ```python
   providers = [
       ("openai", "gpt-4"),
       ("anthropic", "claude-3-sonnet-20240229"),
       ("ollama", "llama2")
   ]
   
   for provider, model in providers:
       try:
           agent.set_provider(provider, model=model)
           break
       except ProviderError:
           continue
   ```

3. **Choose Models Wisely** for cost/performance
   - Simple tasks: `gpt-3.5-turbo`, `claude-3-haiku`
   - Complex tasks: `gpt-4`, `claude-3-opus`
   - Private data: `ollama/llama2`

4. **Test with Multiple Providers** to ensure compatibility

## Getting Help

- **Documentation**: https://docs.agenticraft.ai
- **Examples**: See `examples/provider_switching/`
- **Issues**: https://github.com/agenticraft/agenticraft/issues
- **Discord**: https://discord.gg/agenticraft

## Summary

v0.1.1 is a backward-compatible release that adds powerful new features:

- ✅ No breaking changes - existing code continues to work
- ✅ Easy provider switching with `set_provider()`
- ✅ Support for Anthropic (Claude) and Ollama (local)
- ✅ New specialized agents (ReasoningAgent, WorkflowAgent)
- ✅ Available on PyPI for easy installation

Upgrade today to access multiple LLM providers and build more resilient, cost-effective agents!
