"""Read X for llm context"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_read.ipynb.

# %% auto 0
__all__ = ['read_url', 'read_gist', 'read_gh_file', 'read_file', 'is_unicode', 'read_dir', 'read_pdf', 'read_yt_transcript',
           'read_google_sheet', 'read_gdoc', 'read_arxiv', 'read_gh_repo']

# %% ../nbs/00_read.ipynb 5
import httpx 
import html2text
from fastcore.all import delegates, ifnone

import re, os, glob, string
import requests
import fnmatch, mimetypes

from pypdf import PdfReader
from toolslm.download import html2md, read_html

import tempfile, subprocess, os, re, shutil
from pathlib import Path

# %% ../nbs/00_read.ipynb 8
def read_url(url,           # URL to read
             heavy=False,   # Use headless browser
             sel=None,      # Css selector to pull content from
             useJina=False, # Use Jina for the markdown conversion
             ignore_links=False, # Whether to keep links or not
             **kwargs): 
    "Reads a url and converts to markdown"
    if not heavy and not useJina: return read_html(url,sel=sel, ignore_links=ignore_links, **kwargs)
    elif not heavy and useJina:   return httpx.get(f"https://r.jina.ai/{url}").text
    elif heavy and not useJina: 
        import playwrightnb
        return playwrightnb.url2md(url,sel=ifnone(sel,'body'), **kwargs)
    elif heavy and useJina: raise NotImplementedError("Unsupported. No benefit to using Jina with playwrightnb")


# %% ../nbs/00_read.ipynb 14
def read_gist(url):
    "Returns raw gist content, or None"
    pattern = r'https://gist\.github\.com/([^/]+)/([^/]+)'
    match = re.match(pattern, url)
    if match:
        user, gist_id = match.groups()
        raw_url = f'https://gist.githubusercontent.com/{user}/{gist_id}/raw'
        return httpx.get(raw_url).text
    else:
        return None

# %% ../nbs/00_read.ipynb 18
def read_gh_file(url):
    "Reads the contents of a file from its GitHub URL"
    pattern = r'https://github\.com/([^/]+)/([^/]+)/blob/([^/]+)/(.+)'
    replacement = r'https://raw.githubusercontent.com/\1/\2/refs/heads/\3/\4'
    raw_url = re.sub(pattern, replacement, url)
    return httpx.get(raw_url).text

# %% ../nbs/00_read.ipynb 22
def read_file(path): return open(path,'r').read()

# %% ../nbs/00_read.ipynb 23
def is_unicode(filepath, sample_size=1024):
    try:
        with open(filepath, 'r') as file: sample = file.read(sample_size)
        return True
    except UnicodeDecodeError:
        return False

# %% ../nbs/00_read.ipynb 26
def read_dir(path,                          # path to read
             unicode_only=True,             # ignore non-unicode files
             included_patterns=["*"],       # glob pattern of files to include
             excluded_patterns=[".git/**"], # glob pattern of files to exclude
             verbose=True,                  # log paths of files being read
             as_dict=False                  # returns dict of {path,content}
            ) -> str|dict:                  # returns string with contents of files read
    pattern = '**/*'
    result = {}
    for file_path in glob.glob(os.path.join(path, pattern), recursive=True):
        if any(fnmatch.fnmatch(file_path, pat) for pat in excluded_patterns):
            continue
        if not any(fnmatch.fnmatch(file_path, pat) for pat in included_patterns):
            continue
        if os.path.isfile(file_path):
            if unicode_only and not is_unicode(file_path):
                continue
            if verbose:
                print(f"Including {file_path}")
            with open(file_path, 'r', errors='ignore') as f:
                result[file_path] = f.read()
    if not as_dict:
        return '\n'.join([f"--- File: {file_path} ---\n{v}\n--- End of {file_path} ---" for file_path,v in result.items()])
    else:
        return result

# %% ../nbs/00_read.ipynb 29
def read_pdf(file_path: str) -> str:
    with open(file_path, 'rb') as file:
        reader = PdfReader(file)
        return ' '.join(page.extract_text() for page in reader.pages)

# %% ../nbs/00_read.ipynb 32
def read_yt_transcript(yt_url):
    from pytube import YouTube
    from youtube_transcript_api import YouTubeTranscriptApi
    try:
        yt = YouTube(yt_url)
        video_id = yt.video_id
    except Exception as e:
        print(f"An error occurred parsing yt urul: {e}")
        return None
    transcript = YouTubeTranscriptApi.get_transcript(video_id)
    return ' '.join(entry['text'] for entry in transcript) 

# %% ../nbs/00_read.ipynb 35
def read_google_sheet(url):
    sheet_id = url.split('/d/')[1].split('/')[0]
    csv_url = f'https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv&id={sheet_id}&gid=0'
    res = requests.get(url=csv_url)
    return res.content

# %% ../nbs/00_read.ipynb 40
def read_gdoc(url):
    import html2text
    doc_url = url
    doc_id = doc_url.split('/d/')[1].split('/')[0]
    export_url = f'https://docs.google.com/document/d/{doc_id}/export?format=html'
    html_doc_content = requests.get(export_url).text
    doc_content = html2text.html2text(html_doc_content)
    return doc_content

# %% ../nbs/00_read.ipynb 43
def read_arxiv(url, save_pdf=False, save_dir='.'):
    "Get paper information from arxiv URL or ID, optionally saving PDF to disk"
    import re, httpx, tarfile, io, os
    import xml.etree.ElementTree as ET
    
    # Create save directory if needed
    if save_pdf:
        os.makedirs(save_dir, exist_ok=True)
    
    # Extract arxiv ID from URL or use directly if it's an ID
    arxiv_id = url.split('/')[-1] if '/' in url else url
    
    # Remove version number if present but save it for downloads
    version = re.search(r'v(\d+)$', arxiv_id)
    version_num = version.group(1) if version else None
    arxiv_id = re.sub(r'v\d+$', '', arxiv_id)
    
    # Construct API query URL
    api_url = f'http://export.arxiv.org/api/query?id_list={arxiv_id}'
    
    # Get response
    response = httpx.get(api_url)
    
    if response.status_code != 200:
        raise Exception(f"Failed to fetch arxiv data: {response.status_code}")
    
    # Parse XML response
    root = ET.fromstring(response.text)
    ns = {'arxiv': 'http://www.w3.org/2005/Atom'}
    entry = root.find('arxiv:entry', ns)
    if entry is None:
        raise Exception("No paper found")
    
    # Extract links including PDF
    links = entry.findall('arxiv:link', ns)
    pdf_url = next((l.get('href') for l in links if l.get('title') == 'pdf'), None)
    
    result = {
        'title': entry.find('arxiv:title', ns).text.strip(),
        'authors': [author.find('arxiv:name', ns).text for author in entry.findall('arxiv:author', ns)],
        'summary': entry.find('arxiv:summary', ns).text.strip(),
        'published': entry.find('arxiv:published', ns).text,
        'link': entry.find('arxiv:id', ns).text,
        'pdf_url': pdf_url
    }
    
    # Save PDF if requested
    if save_pdf and pdf_url:
        pdf_response = httpx.get(pdf_url)
        if pdf_response.status_code == 200:
            pdf_filename = f"{arxiv_id}{'v'+version_num if version_num else ''}.pdf"
            pdf_path = os.path.join(save_dir, pdf_filename)
            with open(pdf_path, 'wb') as f:
                f.write(pdf_response.content)
            result['pdf_path'] = pdf_path
    
    # Always try to get source files
    source_url = f'https://arxiv.org/e-print/{arxiv_id}{"v"+version_num if version_num else ""}'
    try:
        source_response = httpx.get(source_url)
        if source_response.status_code == 200:
            # Try to extract main tex file from tar archive
            tar_content = io.BytesIO(source_response.content)
            with tarfile.open(fileobj=tar_content, mode='r:*') as tar:
                # Look for main tex file
                tex_files = [f for f in tar.getnames() if f.endswith('.tex')]
                if tex_files:
                    main_tex = tar.extractfile(tex_files[0])
                    result['source'] = main_tex.read().decode('utf-8', errors='ignore')
    except Exception as e:
        result['source_error'] = str(e)
    
    return result

# %% ../nbs/00_read.ipynb 45
def _gh_ssh_from_gh_url(gh_repo_address):
    "Given a GH URL or SSH remote address, returns a GH URL or None"
    pattern = r'https://github\.com/([^/]+)/([^/]+)(?:/.*)?'
    if gh_repo_address.startswith("git@github.com:"):
        return gh_repo_address
    elif match := re.match(pattern, gh_repo_address):
        user, repo = match.groups()
        return f'git@github.com:{user}/{repo}.git'
    else:
        # Not a GitHub URL or a GitHub SSH remote address
        return None

def _get_default_branch(repo_path):
    "master or main"
    try:
        result = subprocess.run(['git', 'symbolic-ref', 'refs/remotes/origin/HEAD'], 
                                cwd=repo_path, capture_output=True, text=True, check=True)
        return result.stdout.strip().split('/')[-1]
    except subprocess.CalledProcessError:
        return 'main'  # Default to 'main' if we can't determine the branch

def _get_git_repo(gh_ssh):
    "Fetchs from a GH SSH address, returns a path"
    repo_name = gh_ssh.split('/')[-1].replace('.git', '')
    cache_dir = Path(os.environ.get('XDG_CACHE_HOME', Path.home() / '.cache')) / 'contextkit_git_clones'
    cache_dir.mkdir(parents=True, exist_ok=True)
    repo_dir = cache_dir / repo_name

    if repo_dir.exists():
        try:
            subprocess.run(['git', 'fetch'], cwd=repo_dir, check=True, capture_output=True)
            default_branch = _get_default_branch(repo_dir)
            subprocess.run(['git', 'reset', '--hard', f'origin/{default_branch}'], 
                           cwd=repo_dir, check=True, capture_output=True)
            return str(repo_dir)
        except subprocess.CalledProcessError:
            shutil.rmtree(repo_dir)  # Remove the cached directory if update fails

    with tempfile.TemporaryDirectory() as temp_dir:
        try:
            print("Cloning repo.")
            subprocess.run(['git', 'clone', gh_ssh], cwd=temp_dir, check=True, capture_output=False)
            cloned_dir = Path(temp_dir) / repo_name
            shutil.move(str(cloned_dir), str(repo_dir))
            return str(repo_dir)
        except subprocess.CalledProcessError as e:
            print(f"Error cloning repo from cwd {temp_dir} with error {e}")
            return None


# %% ../nbs/00_read.ipynb 46
def read_gh_repo(path_or_url, as_dict=True, verbose=True):
    "Repo contents from path, GH URL, or GH SSH address"
    gh_ssh = _gh_ssh_from_gh_url(path_or_url)
    path = path_or_url if not gh_ssh else _get_git_repo(gh_ssh)
    return read_dir(path,verbose=verbose,as_dict=as_dict)
