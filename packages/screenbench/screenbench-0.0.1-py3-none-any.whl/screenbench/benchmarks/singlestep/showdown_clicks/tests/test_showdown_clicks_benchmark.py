"""Tests for the VisualWebBench benchmark."""

import os
import tempfile
from unittest.mock import patch

import pytest
from PIL import Image
from smolagents import Model
from smolagents.models import ChatMessage

from screensuite.basebenchmark import EvaluationConfig
from screensuite.benchmarks.singlestep.showdown_clicks.benchmark import (
    ShowdownClicksBenchmark,
)
from screensuite.benchmarks.singlestep.showdown_clicks.config import (
    ShowdownClicksConfig,
)


class MockModel(Model):
    def __init__(self):
        super().__init__()
        self.call_count = 0

    def generate(self, message, max_tokens, temperature):
        self.call_count += 1
        return ChatMessage(role="assistant", content="click(0.576, 0.827)")


@pytest.fixture
def mock_dataset():
    """Create a mock dataset for testing."""
    return [
        {
            "screenshot": Image.new("RGB", (1, 1), color=(255, 255, 255)),
            "answer": "Test caption 1",
            "x1": 0,
            "y1": 0,
            "x2": 1,
            "y2": 1,
            "width": 2,
            "height": 1,
            "instruction": "open the app",
        },
        {
            "screenshot": Image.new("RGB", (1, 1), color=(255, 255, 255)),
            "answer": "Test caption 2",
            "x1": 1,
            "y1": 0,
            "x2": 2,
            "y2": 1,
            "width": 2,
            "height": 1,
            "instruction": "open the other app",
        },
    ]


@patch("screensuite.benchmarks.singlestep.showdown_clicks.benchmark.ShowdownClicksBenchmark.load")
def test_evaluate(mock_load, mock_dataset):
    """Test the evaluate method."""
    # Set up the benchmark with mock data
    mock_load.return_value = None

    # Create a temporary directory for testing
    with tempfile.TemporaryDirectory() as temp_dir:
        # Create output directory structure
        output_dir = os.path.join(temp_dir, "output", "showdown-clicks")
        os.makedirs(output_dir, exist_ok=True)

        # Change working directory to temp directory
        original_dir = os.getcwd()
        os.chdir(temp_dir)

        try:
            benchmark = ShowdownClicksBenchmark(
                name="showdown-clicks",
                config=ShowdownClicksConfig.create(),
                tags=["test", "showdown-clicks"],
            )
            benchmark.dataset = mock_dataset

            mock_model = MockModel()
            # Run the evaluation with a run_name
            run_name = "test_run"
            results = benchmark.evaluate(
                mock_model, EvaluationConfig(test_mode=False, parallel_workers=2, run_name=run_name)
            )

            # Check the results
            assert results["bounding_box_acc"] == 0.5

            # Check that the model was called for each sample
            expected_calls = len(mock_dataset)
            assert mock_model.call_count == expected_calls

            # Check that output files were created
            expected_output_dir = os.path.join(output_dir, run_name)

            assert os.path.exists(expected_output_dir), f"Output directory {expected_output_dir} was not created"

            expected_output_file = os.path.join(expected_output_dir, "answers.jsonl")
            assert os.path.exists(expected_output_file), f"Output file {expected_output_file} was not created"

            # Check that the output file contains the expected number of entries
            with open(expected_output_file, "r") as f:
                lines = f.readlines()
                assert len(lines) == len(mock_dataset), (
                    f"Expected {len(mock_dataset)} entries in output file, got {len(lines)}"
                )

        finally:
            # Restore original working directory
            os.chdir(original_dir)
