"""
ImproveJudge: Improvement recommendation generation.

This judge generates improvement planning for agent workflows, framework optimization, and performance enhancement.
"""

import logging
from typing import Dict, List, Any, Optional
from datetime import datetime
from dataclasses import dataclass

from agent_eval.core.types import AgentOutput, EvaluationScenario
from agent_eval.evaluation.judges.base import BaseJudge, JudgmentResult, ContinuousFeedback

logger = logging.getLogger(__name__)


@dataclass
class ImprovementPlan:
    """Structured improvement plan generated by ImproveJudge."""
    priority: str  # "CRITICAL", "HIGH", "MEDIUM", "LOW"
    area: str
    description: str
    action: str
    expected_improvement: str
    timeline: str
    scenario_ids: List[str]
    compliance_frameworks: List[str]
    specific_steps: List[str]
    code_examples: List[str]
    confidence: float


class ImproveJudge(BaseJudge):
    """Judge for generating improvement recommendations and optimization plans."""
    
    def __init__(self, api_manager, enable_confidence_calibration: bool = False):
        super().__init__(api_manager, enable_confidence_calibration)
        self.domain = "improve"
        self.knowledge_base = [
            "Agent improvement strategies and best practices",
            "Framework migration and optimization patterns (LangChain, CrewAI, AutoGen, etc.)",
            "Performance enhancement methodologies and profiling",
            "Code-specific remediation techniques and patterns",
            "Progressive improvement planning and implementation",
            "Cross-framework optimization insights",
            "Business impact assessment for technical improvements"
        ]
    
    def evaluate(self, agent_output: AgentOutput, scenario: EvaluationScenario) -> JudgmentResult:
        """Evaluate agent output for improvement opportunities."""
        prompt = self._build_prompt(agent_output, scenario)
        return self._execute_evaluation(prompt, scenario, self.api_manager.preferred_model)
    
    def generate_improvement_plan(self, evaluation_results: List[Dict], domain: str) -> Dict[str, Any]:
        """Generate improvement plan from evaluation results."""
        # Prepare scenario for improvement planning
        scenario = EvaluationScenario(
            id="improvement_planning",
            name="Improvement Plan Generation",
            description=f"Generate actionable improvement plan for {domain} domain",
            expected_behavior="Create prioritized, specific, and measurable improvement actions",
            severity="high"
        )
        
        # Convert evaluation results to AgentOutput format
        agent_output = AgentOutput(
            raw_output={"evaluation_results": evaluation_results, "domain": domain},
            normalized_output=f"Improvement planning for {len(evaluation_results)} evaluation results in {domain}"
        )
        
        # Use judge evaluation
        result = self.evaluate(agent_output, scenario)
        
        # Convert to expected format for ImprovementPlanner integration
        return {
            "improvement_actions": result.reward_signals.get("improvement_actions", []),
            "priority_breakdown": result.reward_signals.get("priority_breakdown", {}),
            "expected_improvement": result.reward_signals.get("expected_improvement", 15),
            "implementation_timeline": result.reward_signals.get("timeline", "2-4 weeks"),
            "success_metrics": result.reward_signals.get("success_metrics", []),
            "risk_assessment": result.reward_signals.get("risks", []),
            "resource_requirements": result.reward_signals.get("resources", []),
            "confidence": result.confidence,
            "reasoning": result.reasoning
        }
    
    def generate_framework_specific_improvements(self, framework: str, issues: List[Dict], performance_data: Dict) -> Dict[str, Any]:
        """Generate framework-specific improvement recommendations.
        
        Replaces static FRAMEWORK_FIXES dictionary in remediation_engine.py.
        """
        scenario = EvaluationScenario(
            id="framework_optimization",
            name=f"{framework} Framework Optimization",
            description=f"Generate specific optimizations for {framework} framework",
            expected_behavior="Create framework-tailored improvement recommendations",
            severity="medium"
        )
        
        agent_output = AgentOutput(
            raw_output={
                "framework": framework,
                "issues": issues,
                "performance_data": performance_data
            },
            normalized_output=f"Framework optimization analysis for {framework}"
        )
        
        result = self.evaluate(agent_output, scenario)
        
        return {
            "framework_optimizations": result.improvement_recommendations,
            "code_examples": result.reward_signals.get("code_examples", []),
            "migration_recommendations": result.reward_signals.get("migration_options", []),
            "performance_improvements": result.reward_signals.get("performance_gains", []),
            "implementation_complexity": result.reward_signals.get("complexity", "medium"),
            "estimated_roi": result.reward_signals.get("roi", {}),
            "confidence": result.confidence
        }
    
    def generate_contextual_remediation(self, failure_patterns: List[str], context: Dict[str, Any]) -> Dict[str, Any]:
        """Generate contextual remediation suggestions.
        
        Replaces template-based remediation suggestions with intelligent, context-aware recommendations.
        """
        scenario = EvaluationScenario(
            id="contextual_remediation",
            name="Contextual Remediation Analysis",
            description="Generate context-aware remediation strategies",
            expected_behavior="Create specific, actionable remediation plans",
            severity="high"
        )
        
        agent_output = AgentOutput(
            raw_output={
                "failure_patterns": failure_patterns,
                "context": context
            },
            normalized_output=f"Remediation analysis for {len(failure_patterns)} failure patterns"
        )
        
        result = self.evaluate(agent_output, scenario)
        
        return {
            "remediation_strategies": result.improvement_recommendations,
            "root_cause_fixes": result.reward_signals.get("root_cause_fixes", []),
            "preventive_measures": result.reward_signals.get("preventive_measures", []),
            "monitoring_recommendations": result.reward_signals.get("monitoring", []),
            "validation_approaches": result.reward_signals.get("validation", []),
            "implementation_order": result.reward_signals.get("implementation_order", []),
            "confidence": result.confidence
        }
    
    def _build_prompt(self, agent_output: AgentOutput, scenario: EvaluationScenario) -> str:
        """Build improvement-specific evaluation prompt."""
        output_data = agent_output.raw_output if agent_output.raw_output else agent_output.normalized_output
        
        if scenario.id == "improvement_planning":
            return self._build_improvement_planning_prompt(output_data)
        elif scenario.id == "framework_optimization":
            return self._build_framework_optimization_prompt(output_data)
        elif scenario.id == "contextual_remediation":
            return self._build_remediation_prompt(output_data)
        else:
            return self._build_standard_improvement_prompt(output_data, scenario)
    
    def _build_improvement_planning_prompt(self, output_data: Dict[str, Any]) -> str:
        """Build prompt for comprehensive improvement planning."""
        evaluation_results = output_data.get("evaluation_results", [])
        domain = output_data.get("domain", "unknown")
        
        # Analyze failed scenarios
        failed_scenarios = [r for r in evaluation_results if not r.get("passed", True)]
        
        return f"""You are an expert MLOps improvement strategist specializing in agent workflow optimization.

TASK: Generate a comprehensive improvement plan based on evaluation results from {domain} domain.

EVALUATION CONTEXT:
- Total scenarios evaluated: {len(evaluation_results)}
- Failed scenarios: {len(failed_scenarios)}
- Domain: {domain}

FAILED SCENARIOS ANALYSIS:
{self._format_failed_scenarios(failed_scenarios)}

IMPROVEMENT PLANNING REQUIREMENTS:
1. **Action Prioritization**: Categorize improvements by business impact and implementation complexity
2. **Specific Recommendations**: Provide concrete, actionable steps with clear success criteria
3. **Timeline Estimation**: Realistic implementation timelines with dependencies
4. **Resource Assessment**: Required skills, tools, and effort estimation
5. **Success Metrics**: Measurable outcomes and KPIs
6. **Risk Analysis**: Potential risks and mitigation strategies

RESPONSE FORMAT (JSON):
{{
    "judgment": "pass|fail|warning",
    "confidence": 0.0-1.0,
    "reasoning": "Detailed improvement strategy analysis",
    "improvements": ["High-level improvement recommendations"],
    "reward_signals": {{
        "improvement_actions": [
            {{
                "priority": "CRITICAL|HIGH|MEDIUM|LOW",
                "area": "Technical area (e.g., tool validation, error handling)",
                "description": "Clear description of the improvement",
                "action": "Specific implementation steps",
                "expected_improvement": "Quantified expected benefit",
                "timeline": "Implementation timeframe",
                "code_examples": ["Relevant code snippets or patterns"],
                "success_metrics": ["Measurable success criteria"]
            }}
        ],
        "priority_breakdown": {{"CRITICAL": 2, "HIGH": 3, "MEDIUM": 1, "LOW": 0}},
        "expected_improvement": 25,
        "timeline": "3-4 weeks",
        "success_metrics": ["Metric 1", "Metric 2"],
        "risks": ["Risk 1", "Risk 2"],
        "resources": ["Resource 1", "Resource 2"]
    }}
}}

Focus on creating actionable, measurable improvements with clear business value."""
    
    def _build_framework_optimization_prompt(self, output_data: Dict[str, Any]) -> str:
        """Build prompt for framework-specific optimization."""
        framework = output_data.get("framework", "unknown")
        issues = output_data.get("issues", [])
        performance_data = output_data.get("performance_data", {})
        
        return f"""You are an expert {framework} framework optimization specialist.

TASK: Generate specific optimization recommendations for {framework} framework based on identified issues and performance data.

FRAMEWORK: {framework}
IDENTIFIED ISSUES:
{self._format_issues(issues)}

PERFORMANCE DATA:
{self._format_performance_data(performance_data)}

OPTIMIZATION REQUIREMENTS:
1. **Framework-Specific Solutions**: Leverage {framework}'s unique capabilities and patterns
2. **Performance Optimizations**: Address bottlenecks and efficiency issues
3. **Code Examples**: Provide working code snippets and implementation patterns
4. **Migration Guidance**: If beneficial, suggest alternative frameworks with migration paths
5. **Best Practices**: Recommend {framework} best practices and patterns

RESPONSE FORMAT (JSON):
{{
    "judgment": "pass|fail|warning",
    "confidence": 0.0-1.0,
    "reasoning": "Detailed framework optimization analysis",
    "improvements": ["Framework-specific optimization recommendations"],
    "reward_signals": {{
        "code_examples": [
            {{
                "description": "What this code does",
                "code": "Working code snippet",
                "framework_feature": "Specific {framework} feature used"
            }}
        ],
        "migration_options": ["Alternative framework recommendations"],
        "performance_gains": ["Expected performance improvements"],
        "complexity": "low|medium|high",
        "roi": {{"effort": "low|medium|high", "benefit": "low|medium|high"}},
        "framework_best_practices": ["Best practice recommendations"]
    }}
}}

Provide {framework}-specific, implementable optimizations with concrete code examples."""
    
    def _build_remediation_prompt(self, output_data: Dict[str, Any]) -> str:
        """Build prompt for contextual remediation."""
        failure_patterns = output_data.get("failure_patterns", [])
        context = output_data.get("context", {})
        
        return f"""You are an expert remediation strategist specializing in agent failure recovery.

TASK: Generate contextual remediation strategies for identified failure patterns.

FAILURE PATTERNS:
{self._format_failure_patterns(failure_patterns)}

CONTEXT:
{self._format_context(context)}

REMEDIATION REQUIREMENTS:
1. **Root Cause Fixes**: Address underlying causes, not just symptoms
2. **Preventive Measures**: Strategies to prevent similar failures
3. **Monitoring & Detection**: Early warning systems and detection mechanisms
4. **Validation Approaches**: Methods to verify fixes are effective
5. **Implementation Strategy**: Step-by-step implementation plan

RESPONSE FORMAT (JSON):
{{
    "judgment": "pass|fail|warning",
    "confidence": 0.0-1.0,
    "reasoning": "Detailed remediation strategy analysis",
    "improvements": ["High-level remediation recommendations"],
    "reward_signals": {{
        "root_cause_fixes": ["Specific fixes for root causes"],
        "preventive_measures": ["Prevention strategies"],
        "monitoring": ["Monitoring and alerting recommendations"],
        "validation": ["Testing and validation approaches"],
        "implementation_order": ["Step 1", "Step 2", "Step 3"],
        "rollback_strategy": ["Rollback plan if fixes fail"]
    }}
}}

Focus on comprehensive, systematic remediation that prevents recurrence."""
    
    def _build_standard_improvement_prompt(self, output_data: Any, scenario: EvaluationScenario) -> str:
        """Build prompt for standard improvement analysis."""
        return f"""You are an expert improvement strategist for AI agent workflows.

SCENARIO: {scenario.name}
DESCRIPTION: {scenario.description}
EXPECTED BEHAVIOR: {scenario.expected_behavior}
SEVERITY: {scenario.severity}

ANALYSIS TARGET:
{str(output_data)[:2000]}

IMPROVEMENT ANALYSIS:
1. **Opportunity Identification**: What specific improvements are possible?
2. **Impact Assessment**: What would be the business and technical impact?
3. **Implementation Strategy**: How should improvements be implemented?
4. **Resource Requirements**: What resources and skills are needed?
5. **Success Measurement**: How will success be measured?

RESPONSE FORMAT (JSON):
{{
    "judgment": "pass|fail|warning",
    "confidence": 0.0-1.0,
    "reasoning": "Detailed improvement analysis",
    "improvements": ["Specific improvement recommendations"],
    "reward_signals": {{
        "impact_assessment": 0.0-1.0,
        "implementation_complexity": "low|medium|high",
        "resource_requirements": ["Required resources"],
        "success_metrics": ["Success measurement criteria"]
    }}
}}

Provide specific, actionable improvement guidance."""
    
    def _format_failed_scenarios(self, failed_scenarios: List[Dict]) -> str:
        """Format failed scenarios for analysis."""
        if not failed_scenarios:
            return "No failed scenarios to analyze."
        
        formatted = []
        for i, scenario in enumerate(failed_scenarios[:10]):  # Limit to first 10
            scenario_id = scenario.get("scenario_id", f"scenario_{i}")
            description = scenario.get("description", "No description")
            severity = scenario.get("severity", "unknown")
            formatted.append(f"- {scenario_id} ({severity}): {description}")
        
        return "\n".join(formatted)
    
    def _format_issues(self, issues: List[Dict]) -> str:
        """Format issues for framework optimization."""
        if not issues:
            return "No specific issues identified."
        
        formatted = []
        for issue in issues[:10]:
            issue_type = issue.get("type", "unknown")
            description = issue.get("description", "No description")
            severity = issue.get("severity", "unknown")
            formatted.append(f"- {issue_type} ({severity}): {description}")
        
        return "\n".join(formatted)
    
    def _format_performance_data(self, performance_data: Dict) -> str:
        """Format performance data for analysis."""
        if not performance_data:
            return "No performance data available."
        
        formatted = []
        for key, value in performance_data.items():
            formatted.append(f"- {key}: {value}")
        
        return "\n".join(formatted)
    
    def _format_failure_patterns(self, failure_patterns: List[str]) -> str:
        """Format failure patterns for remediation."""
        if not failure_patterns:
            return "No failure patterns identified."
        
        return "\n".join(f"- {pattern}" for pattern in failure_patterns[:10])
    
    def _format_context(self, context: Dict[str, Any]) -> str:
        """Format context information for remediation."""
        if not context:
            return "No context information available."
        
        formatted = []
        for key, value in context.items():
            formatted.append(f"- {key}: {str(value)[:200]}")
        
        return "\n".join(formatted)
    
    def _parse_response(self, response_text: str) -> Dict[str, Any]:
        """Parse ImproveJudge response into structured judgment data."""
        # Use parent class JSON parsing with improve-specific defaults
        default_reward_signals = {
            "improvement_actions": [],
            "priority_breakdown": {},
            "expected_improvement": 15,
            "timeline": "2-4 weeks",
            "success_metrics": [],
            "risks": [],
            "resources": []
        }
        
        default_improvements = [
            "Implement prioritized improvement actions",
            "Monitor success metrics and adjust approach",
            "Review and validate improvements regularly"
        ]
        
        from agent_eval.evaluation.judges.base import _parse_json_response
        return _parse_json_response(response_text, default_reward_signals, default_improvements)
    
    def generate_continuous_feedback(self, results: List[JudgmentResult]) -> ContinuousFeedback:
        """Generate continuous feedback for improvement planning."""
        if not results:
            return ContinuousFeedback([], [], [], [], [])
        
        # Aggregate insights from all results
        all_improvements = []
        all_strengths = []
        all_weaknesses = []
        all_metrics = []
        
        for result in results:
            all_improvements.extend(result.improvement_recommendations)
            all_strengths.extend(result.reward_signals.get("success_patterns", []))
            all_weaknesses.extend(result.reward_signals.get("improvement_areas", []))
            all_metrics.extend(result.reward_signals.get("success_metrics", []))
        
        # Remove duplicates and prioritize
        unique_improvements = list(set(all_improvements))
        unique_strengths = list(set(all_strengths))
        unique_weaknesses = list(set(all_weaknesses))
        unique_metrics = list(set(all_metrics))
        
        return ContinuousFeedback(
            strengths=unique_strengths[:5],
            weaknesses=unique_weaknesses[:5],
            specific_improvements=unique_improvements[:10],
            training_suggestions=[
                "Focus on high-impact, low-effort improvements first",
                "Establish baseline metrics before implementing changes",
                "Implement monitoring for all improvement initiatives"
            ],
            compliance_gaps=unique_metrics[:5]
        )