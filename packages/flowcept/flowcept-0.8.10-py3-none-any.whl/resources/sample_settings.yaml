flowcept_version: 0.8.10 # Version of the Flowcept package. This setting file is compatible with this version.

project:
  debug: true # Toggle debug mode. This will add a property `debug: true` to all saved data, making it easier to retrieve/delete them later.
  json_serializer: default # JSON serialization mode: default or complex. If "complex", Flowcept will deal with complex python dicts that may contain JSON unserializable values
  replace_non_json_serializable: true # Replace values that can't be JSON serialized
  performance_logging: false # Enable performance logging if true. Particularly useful for MQ flushes.
  enrich_messages: true # Add extra metadata to task messages, such as IP addresses and UTC timestamps.
  db_flush_mode: online # Mode for flushing DB entries: "online" or "offline". If online, flushes to the DB will happen before the workflow ends.

log:
  log_path: "default" # Path for log file output; "default" will write the log in the directory where the main executable is running from.
  log_file_level: error # Logging level (error, debug, info, critical) for file logs; use "disable" to turn off.
  log_stream_level: error # Logging level (error, debug, info, critical) for console/stream logs; use "disable" to turn off.

telemetry_capture: # This toggles each individual type of telemetry capture. GPU capture is treated different depending on the vendor (AMD or NVIDIA).
  gpu: ~ # ~ means None. This is a list with GPU metrics. AMD=[activity,used,power,temperature,others,id]; NVIDIA=[used,temperature,power,name,id]
  cpu: true
  per_cpu: true
  process_info: true
  mem: true
  disk: true
  network: true
  machine_info: true

instrumentation:
  enabled: true # This toggles data capture for instrumentation.
  torch:
    what: parent_and_children # Scope of instrumentation: "parent_only" -- will capture only at the main model level, "parent_and_children" -- will capture the inner layers, or ~ (disable).
    children_mode: telemetry_and_tensor_inspection   # What to capture if parent_and_children is chosen in the scope. Possible values: "tensor_inspection" (i.e., tensor metadata), "telemetry", "telemetry_and_tensor_inspection"
    epoch_loop: lightweight # lightweight, ~ (disable), or default (default will use the default telemetry capture method)
    batch_loop: lightweight # lightweight, ~ (disable), or default (default will use the default telemetry capture method)
    capture_epochs_at_every: 1 # Will capture data at every N epochs; please use a value that is multiple of the total number of #epochs.
    register_workflow: true # Will store the parent model forward as a workflow itself in the database.

experiment:
  user: root  # Optionally identify the user running the experiment. The logged username will be captured anyways.

mq:
  type: redis  # or kafka or mofka; Please adjust the port (kafka's default is 9092; redis is 6379). If mofka, adjust the group_file.
  host: localhost
  # instances: ["localhost:6379"] # We can have multiple redis instances being accessed by the consumers but each interceptor will currently access one single redis.
  port: 6379
  # group_file: mofka.json
  channel: interception
  buffer_size: 50
  insertion_buffer_time_secs: 5
  timing: false
  chunk_size: -1  # use 0 or -1 to disable this. Or simply omit this from the config file.

kv_db:  # You can optionally use KV == MQ if MQ is Redis. Otherwise, these will be the Redis instance credentials.
  host: localhost
  port: 6379
  # uri: use Redis connection uri here

web_server:
  host: 0.0.0.0
  port: 5000

sys_metadata:
  environment_id: "laptop"   # We use this to keep track of the environment used to run an experiment. Typical values include the cluster name, but it can be anything that you think will help identify your experimentation environment.

extra_metadata: # We use this to store any extra metadata you want to keep track of during an experiment.
  place_holder: ""

analytics:
  sort_orders:
    generated.loss: minimum_first
    generated.accuracy: maximum_first

db_buffer:
  insertion_buffer_time_secs: 5   # Time interval (in seconds) to buffer incoming records before flushing to the database
  buffer_size: 50    # Maximum number of records to hold in the buffer before forcing a flush
  remove_empty_fields: false    # If true, fields with null/empty values will be removed before insertion
  stop_max_trials: 240    # Maximum number of trials before giving up when waiting for a fully safe stop (i.e., all records have been inserted as expected).
  stop_trials_sleep: 0.01   # Sleep duration (in seconds) between trials when waiting for a fully safe stop.

databases:

  lmdb:
    enabled: true
    path: flowcept_lmdb

  mongodb:
    enabled: true
    host: localhost
    port: 27017
    db: flowcept
    create_collection_index: true  # Whether flowcept should create collection indices if they haven't been created yet. This is done only at the Flowcept start up.

adapters:
  # For each key below, you can have multiple instances. Like mlflow1, mlflow2; zambeze1, zambeze2. Use an empty dict, {}, if you won't use any adapter.
  zambeze:
    kind: zambeze
    host: localhost
    port: 5672
    queue_names:
      - hello
      - hello2
#    key_values_to_filter:
#      - key: activity_status
#        value: CREATED

  mlflow:
    kind: mlflow
    file_path: mlflow.db
    log_params: ['*']
    log_metrics: ['*']
    watch_interval_sec: 2

  tensorboard:
    kind: tensorboard
    file_path: tensorboard_events
    log_tags: ['scalars', 'hparams', 'tensors']
    log_metrics: ['accuracy']
    watch_interval_sec: 5

  dask:
    kind: dask
    worker_should_get_input: true
    scheduler_should_get_input: true
    worker_should_get_output: true
    scheduler_create_timestamps: true
    worker_create_timestamps: false
